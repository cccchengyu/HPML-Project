{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Import\n",
        "import kagglehub\n",
        "\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "import itertools\n",
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    BertConfig\n",
        ")\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    f1_score\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "33LC2G1TbiwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Time and memory\n",
        "def get_gpu_memory_mb():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.memory_allocated() / 1024**2\n",
        "    return 0\n",
        "\n",
        "def get_peak_gpu_memory_mb():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.max_memory_allocated() / 1024**2\n",
        "    return 0\n",
        "\n",
        "def reset_peak_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.reset_peak_memory_stats()"
      ],
      "metadata": {
        "id": "3KpkE94sTxmd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load dataset\n",
        "# Set cache directory (just the directory, not including specific files)\n",
        "# Don't include the filename or 'versions/1/train.tsv' in the path\n",
        "os.environ['KAGGLE_HUB_CACHE'] = '/root/.cache/kagglehub'\n",
        "\n",
        "# Download the dataset\n",
        "path = kagglehub.dataset_download(\"doanquanvietnamca/liar-dataset\")\n",
        "\n",
        "# CORRECT way to join paths - don't start with '/'\n",
        "train_path = os.path.join(path, 'train.tsv')\n",
        "valid_path = os.path.join(path, 'valid.tsv')\n",
        "test_path = os.path.join(path, 'test.tsv')\n",
        "\n",
        "print(\"Path to dataset directory:\", path)\n",
        "print(\"Path to train.tsv file:\", train_path)\n",
        "print(\"Path to valid.tsv file:\", valid_path)\n",
        "print(\"Path to test.tsv file:\", test_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgowySh4JPQU",
        "outputId": "45aff3f7-8e69-4d93-ed22-ec718607bc78",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'liar-dataset' dataset.\n",
            "Path to dataset directory: /kaggle/input/liar-dataset\n",
            "Path to train.tsv file: /kaggle/input/liar-dataset/train.tsv\n",
            "Path to valid.tsv file: /kaggle/input/liar-dataset/valid.tsv\n",
            "Path to test.tsv file: /kaggle/input/liar-dataset/test.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Profiler function\n",
        "\n",
        "def train_epoch_with_profiler(model, dataloader, optimizer, scheduler, criterion, device, prof):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Profile the first 5 batches\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        if batch_idx < 5:\n",
        "            with record_function(\"data_loading\"):\n",
        "                input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "                attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
        "                labels = batch['label'].to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with record_function(\"forward_pass\"):\n",
        "                with torch.amp.autocast(device_type='cuda', dtype=dtype):\n",
        "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                    loss = criterion(outputs.logits, labels)\n",
        "\n",
        "            with record_function(\"backward_pass\"):\n",
        "                loss.backward()\n",
        "\n",
        "            with record_function(\"optimizer_step\"):\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            prof.step()\n",
        "        else:\n",
        "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
        "            labels = batch['label'].to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.amp.autocast(device_type='cuda', dtype=dtype):\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                loss = criterion(outputs.logits, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_time = end_time - start_time\n",
        "    peak_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
        "\n",
        "    return total_loss / len(dataloader), epoch_time, peak_memory"
      ],
      "metadata": {
        "id": "JTopLXFMupc2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Only baseline\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 128\n",
        "LR = 2e-5\n",
        "EPOCHS = 10\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "NUM_WORKERS = 8\n",
        "\n",
        "TRAIN_PATH = '/kaggle/input/liar-dataset/train.tsv'\n",
        "VALID_PATH = '/kaggle/input/liar-dataset/valid.tsv'\n",
        "TEST_PATH  = '/kaggle/input/liar-dataset/test.tsv'\n",
        "\n",
        "#1. Dataset Class\n",
        "class TextualizedLIARDataset(Dataset):\n",
        "    def __init__(self, tsv_path, tokenizer, max_len=128):\n",
        "        self.df = pd.read_csv(tsv_path, sep='\\t', header=None, names=[\n",
        "            \"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job\",\n",
        "            \"state\", \"party\", \"barely_true_counts\", \"false_counts\",\n",
        "            \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\",\n",
        "            \"context\"\n",
        "        ])\n",
        "\n",
        "        self.df.dropna(subset=['statement'], inplace=True)\n",
        "\n",
        "        # Label logic: False/Pants-fire/Barely-true = 0 (Fake)\n",
        "        self.label_map = {\n",
        "            \"pants-fire\": 0, \"false\": 0, \"barely-true\": 0,\n",
        "            \"half-true\": 1, \"mostly-true\": 1, \"true\": 1\n",
        "        }\n",
        "\n",
        "        self.df['label'] = self.df['label'].map(self.label_map)\n",
        "        self.df.dropna(subset=['label'], inplace=True)\n",
        "        self.df['label'] = self.df['label'].astype(int)\n",
        "\n",
        "        text_cols = ['statement', 'subject', 'speaker', 'party', 'state', 'speaker_job', 'context']\n",
        "        for col in text_cols:\n",
        "            self.df[col] = self.df[col].fillna(\"Unknown\")\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        metadata_str = (\n",
        "            f\"Speaker: {row['speaker']} | \"\n",
        "            f\"Job: {row['speaker_job']} | \"\n",
        "            f\"Party: {row['party']} | \"\n",
        "            f\"State: {row['state']} | \"\n",
        "            f\"Context: {row['context']} | \"\n",
        "            f\"Subject: {row['subject']}\"\n",
        "        )\n",
        "        final_text = f\"{metadata_str} [SEP] Statement: {row['statement']}\"\n",
        "\n",
        "        encoded = self.tokenizer(\n",
        "            final_text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(row['label'], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "#2. Training and Evaluation Functions\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    #dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "    reset_peak_memory()\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
        "        labels = batch['label'].to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_time = end_time - start_time\n",
        "\n",
        "    peak_memory = get_peak_gpu_memory_mb()\n",
        "\n",
        "    return total_loss / len(dataloader), epoch_time, peak_memory\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    reset_peak_memory()\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    eval_time = time.time() - start_time\n",
        "    peak_memory = get_peak_gpu_memory_mb()\n",
        "\n",
        "    return total_loss / len(dataloader), accuracy_score(all_labels, all_preds), all_labels, all_preds, eval_time, peak_memory\n",
        "\n",
        "#Main training\n",
        "def run_official_split_training():\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    print(f\"Using device: {DEVICE} | Model: {MODEL_NAME}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    print(\"Loading Official Datasets (Train / Valid / Test)...\")\n",
        "    if not os.path.exists(TRAIN_PATH):\n",
        "        print(f\"Error: Path {TRAIN_PATH} not found.\")\n",
        "        return\n",
        "\n",
        "    train_dataset = TextualizedLIARDataset(TRAIN_PATH, tokenizer, max_len=MAX_LEN)\n",
        "    valid_dataset = TextualizedLIARDataset(VALID_PATH, tokenizer, max_len=MAX_LEN)\n",
        "    test_dataset  = TextualizedLIARDataset(TEST_PATH, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "    # Build DataLoader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "    test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "    print(f\"Stats: Train={len(train_dataset)}, Valid={len(valid_dataset)}, Test={len(test_dataset)}\")\n",
        "\n",
        "    # Get Class Weights (directly from train_dataset)\n",
        "    print(\"Calculating class weights from Training set...\")\n",
        "    train_labels = train_dataset.df['label'].values\n",
        "\n",
        "    # Automatically compute class weights\n",
        "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
        "\n",
        "    print(f\"Class Weights: {class_weights} (Index 0 is Fake, Index 1 is True)\")\n",
        "\n",
        "    # Define the weighted loss\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "    # Model Loading and Configuration (Dropout 0.3)\n",
        "    print(\"Loading model with increased dropout...\")\n",
        "    config = BertConfig.from_pretrained(MODEL_NAME)\n",
        "    config.hidden_dropout_prob = 0.3\n",
        "    config.attention_probs_dropout_prob = 0.3\n",
        "    config.num_labels = 2\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
        "\n",
        "    # calculate the num of parameters\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"trainable params: {trainable:,} || all params: {total:,} || trainable%: {100*trainable/total:.4f}\")\n",
        "\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
        "\n",
        "    total_train_time = 0\n",
        "    epoch_times = []\n",
        "    epoch_memories = []\n",
        "\n",
        "\n",
        "    total_steps = len(train_loader) * EPOCHS\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)\n",
        "\n",
        "    best_val_f1 = 0\n",
        "\n",
        "    print(\"\\nStarting Training on Official Split...\")\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "      if epoch == 2:\n",
        "        print(\"Profiling enabled for epoch 2 ...\")\n",
        "\n",
        "        with profile(\n",
        "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "            record_shapes=True,\n",
        "            profile_memory=True,\n",
        "            with_stack=False\n",
        "        ) as prof:\n",
        "            train_loss, train_time, train_memory = train_epoch_with_profiler(\n",
        "                model, train_loader, optimizer, scheduler, criterion, DEVICE, prof\n",
        "            )\n",
        "\n",
        "\n",
        "        print(\"PROFILER SUMMARY (Epoch 2)\")\n",
        "\n",
        "\n",
        "        print(\"\\nTop Operations by CPU Time:\")\n",
        "        print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=15))\n",
        "\n",
        "        print(\"\\nTop Operations by CUDA Time:\")\n",
        "        try:\n",
        "            print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=15))\n",
        "        except:\n",
        "            print(prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=15))\n",
        "\n",
        "        print(\"\\nTop Operations by Memory:\")\n",
        "        print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=15))\n",
        "\n",
        "        trace_file = \"profiler_trace_baselineonly_epoch1.json\"\n",
        "        prof.export_chrome_trace(trace_file)\n",
        "        print(f\"\\n Chrome trace saved to: {trace_file}\")\n",
        "        print(\"  Download and open in chrome://tracing\")\n",
        "      else:\n",
        "        train_loss, train_time, train_memory = train_epoch(\n",
        "          model, train_loader, optimizer, scheduler, criterion, DEVICE\n",
        "        )\n",
        "\n",
        "      val_loss, val_acc, val_labels, val_preds, val_time, val_memory = evaluate(\n",
        "          model, valid_loader, criterion, DEVICE\n",
        "      )\n",
        "\n",
        "      total_train_time += train_time\n",
        "      epoch_times.append(train_time)\n",
        "      epoch_memories.append(train_memory)\n",
        "\n",
        "      report_dict = classification_report(val_labels, val_preds, output_dict=True)\n",
        "      macro_f1 = report_dict['macro avg']['f1-score']\n",
        "      fake_recall = report_dict['0']['recall']\n",
        "\n",
        "      print(f\"Epoch {epoch}/{EPOCHS} | Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Macro-F1: {macro_f1:.4f} | Fake Recall: {fake_recall:.4f}|  T_Time: {train_time:.1f}s | T_Mem: {train_memory:.0f}MB | V_Time: {val_time:.2f}s | V_Memory: {val_memory:.1f} MB\")\n",
        "\n",
        "      if macro_f1 > best_val_f1:\n",
        "          best_val_f1 = macro_f1\n",
        "          torch.save(model.state_dict(), 'best_baselineonly_model.pth')\n",
        "          print(\" -> Best baselineamp model updated!\")\n",
        "\n",
        "    print(f\"Total Training Time: {total_train_time:.1f}s\")\n",
        "    print(f\"Avg Epoch Time: {sum(epoch_times)/len(epoch_times):.1f}s\")\n",
        "    print(f\"Avg Peak Memory: {sum(epoch_memories)/len(epoch_memories):.0f}MB\")\n",
        "\n",
        "    # automatically run best model on Official Test Set after training\n",
        "    print(\"\\n========== FINAL TEST RESULT (Official Test Set) ==========\")\n",
        "\n",
        "    model.load_state_dict(torch.load('best_baselineonly_model.pth'))\n",
        "    test_loss, test_acc, test_labels, test_preds, test_time, test_memory = evaluate(model, test_loader, criterion, DEVICE)\n",
        "    print(classification_report(test_labels, test_preds, target_names=['Fake (0)', 'True (1)']))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_official_split_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTFQCjeNdIHy",
        "outputId": "741bce0a-c6a3-413f-d629-f2f0b1dc8750",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda | Model: bert-base-uncased\n",
            "Loading Official Datasets (Train / Valid / Test)...\n",
            "Stats: Train=10240, Valid=1284, Test=1267\n",
            "Calculating class weights from Training set...\n",
            "Class Weights: [1.14081996 0.89012517] (Index 0 is Fake, Index 1 is True)\n",
            "Loading model with increased dropout...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 109,483,778 || all params: 109,483,778 || trainable%: 100.0000\n",
            "\n",
            "Starting Training on Official Split...\n",
            "Epoch 1/10 | Val Loss: 0.6858 | Acc: 0.5927 | Macro-F1: 0.5680 | Fake Recall: 0.3685|  T_Time: 85.5s | T_Mem: 21885MB | V_Time: 4.93s | V_Memory: 3633.0 MB\n",
            " -> Best baselineamp model updated!\n",
            "Profiling enabled for epoch 2 ...\n",
            "PROFILER SUMMARY (Epoch 2)\n",
            "\n",
            "Top Operations by CPU Time:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                             aten::item         0.36%      47.821ms        36.80%        4.833s     149.534us       0.000us         0.00%     335.247us       0.010us           0 B           0 B           0 B           0 B         32322  \n",
            "                              aten::_local_scalar_dense         0.19%      25.099ms        36.44%        4.785s     148.054us     335.247us         0.00%     335.247us       0.010us           0 B           0 B           0 B           0 B         32322  \n",
            "                                  cudaStreamSynchronize        36.19%        4.754s        36.20%        4.754s      29.715ms       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B           160  \n",
            "                                           aten::linear         1.17%     153.594ms        14.83%        1.947s     164.469us       0.000us         0.00%        4.220s     356.392us           0 B           0 B    1004.78 GB           0 B         11840  \n",
            "                                               aten::to         0.76%      99.515ms        11.02%        1.447s      28.132us       0.000us         0.00%        1.171s      22.772us           0 B           0 B     688.84 GB           0 B         51440  \n",
            "    autograd::engine::evaluate_function: AddmmBackward0         1.36%     178.399ms        10.41%        1.367s     230.979us       0.000us         0.00%        3.874s     654.468us           0 B           0 B    -361.68 GB    -779.77 GB          5920  \n",
            "                                         aten::_to_copy         2.50%     328.874ms        10.26%        1.348s      38.547us       0.000us         0.00%        1.171s      33.507us           0 B           0 B     688.84 GB     -50.00 MB         34960  \n",
            "                                       cudaLaunchKernel         7.20%     945.997ms         7.30%     958.211ms      10.237us       0.000us         0.00%       4.693us       0.000us           0 B           0 B           0 B           0 B         93600  \n",
            "autograd::engine::evaluate_function: ToCopyBackward0...         1.43%     188.143ms         7.28%     955.751ms      60.338us       0.000us         0.00%        1.254s      79.183us           0 B           0 B    -167.25 GB    -553.08 GB         15840  \n",
            "                              Optimizer.step#AdamW.step         2.84%     373.588ms         7.01%     920.765ms      11.510ms       0.000us         0.00%     452.322ms       5.654ms           0 B        -320 B           0 B     -32.97 GB            80  \n",
            "                                         AddmmBackward0         0.85%     111.797ms         6.75%     887.114ms     149.850us       0.000us         0.00%        3.443s     581.610us           0 B           0 B     418.08 GB           0 B          5920  \n",
            "                                            aten::copy_         2.44%     320.261ms         5.37%     705.489ms      19.997us        1.176s         8.75%        1.176s      33.321us           0 B           0 B           0 B           0 B         35280  \n",
            "                                        ToCopyBackward0         0.43%      56.246ms         5.28%     693.655ms      43.791us       0.000us         0.00%     590.644ms      37.288us           0 B           0 B     385.82 GB           0 B         15840  \n",
            "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...         4.85%     637.512ms         4.88%     640.540ms       7.908ms       0.000us         0.00%       0.000us       0.000us           0 B     -80.00 KB           0 B           0 B            81  \n",
            "                                               aten::mm         3.04%     399.496ms         4.49%     589.666ms      49.803us        3.443s        25.62%        3.443s     290.805us           0 B           0 B     418.08 GB     418.08 GB         11840  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 13.134s\n",
            "Self CUDA time total: 13.440s\n",
            "\n",
            "\n",
            "Top Operations by CUDA Time:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::linear         1.17%     153.594ms        14.83%        1.947s     164.469us       0.000us         0.00%        4.220s     356.392us           0 B           0 B    1004.78 GB           0 B         11840  \n",
            "    autograd::engine::evaluate_function: AddmmBackward0         1.36%     178.399ms        10.41%        1.367s     230.979us       0.000us         0.00%        3.874s     654.468us           0 B           0 B    -361.68 GB    -779.77 GB          5920  \n",
            "                                         AddmmBackward0         0.85%     111.797ms         6.75%     887.114ms     149.850us       0.000us         0.00%        3.443s     581.610us           0 B           0 B     418.08 GB           0 B          5920  \n",
            "                                               aten::mm         3.04%     399.496ms         4.49%     589.666ms      49.803us        3.443s        25.62%        3.443s     290.805us           0 B           0 B     418.08 GB     418.08 GB         11840  \n",
            "autograd::engine::evaluate_function: ScaledDotProduc...         0.25%      32.958ms         1.91%     250.269ms     260.697us       0.000us         0.00%        1.996s       2.079ms     -15.00 KB     -15.00 KB    -106.58 GB    -241.58 GB           960  \n",
            "            ScaledDotProductEfficientAttentionBackward0         0.12%      16.076ms         1.65%     217.311ms     226.365us       0.000us         0.00%        1.996s       2.079ms           0 B           0 B     135.00 GB           0 B           960  \n",
            "aten::_scaled_dot_product_efficient_attention_backwa...         0.23%      30.034ms         1.53%     201.235ms     209.619us       0.000us         0.00%        1.996s       2.079ms           0 B           0 B     135.00 GB           0 B           960  \n",
            "                    aten::_efficient_attention_backward         0.33%      43.751ms         0.98%     128.317ms     133.663us        1.947s        14.49%        1.996s       2.079ms           0 B           0 B     135.00 GB     -91.49 GB           960  \n",
            "fmha_cutlassB_bf16_aligned_64x64_k64_dropout_sm80(Py...         0.00%       0.000us         0.00%       0.000us       0.000us        1.947s        14.49%        1.947s       2.029ms           0 B           0 B           0 B           0 B           960  \n",
            "                                            aten::addmm         2.28%     299.508ms         2.84%     373.645ms      63.116us        1.916s        14.25%        1.916s     323.604us           0 B           0 B     405.01 GB     405.01 GB          5920  \n",
            "                     aten::scaled_dot_product_attention         0.19%      24.481ms         2.53%     332.503ms     173.178us       0.000us         0.00%        1.509s     785.749us      30.00 KB           0 B     108.16 GB           0 B          1920  \n",
            "ampere_bf16_s16816gemm_bf16_128x128_ldg8_relu_f2f_st...         0.00%       0.000us         0.00%       0.000us       0.000us        1.299s         9.66%        1.299s     270.982us           0 B           0 B           0 B           0 B          4793  \n",
            "autograd::engine::evaluate_function: ToCopyBackward0...         1.43%     188.143ms         7.28%     955.751ms      60.338us       0.000us         0.00%        1.254s      79.183us           0 B           0 B    -167.25 GB    -553.08 GB         15840  \n",
            "ampere_bf16_s16816gemm_bf16_128x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us        1.239s         9.22%        1.239s     258.187us           0 B           0 B           0 B           0 B          4800  \n",
            "                                            aten::copy_         2.44%     320.261ms         5.37%     705.489ms      19.997us        1.176s         8.75%        1.176s      33.321us           0 B           0 B           0 B           0 B         35280  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 13.134s\n",
            "Self CUDA time total: 13.440s\n",
            "\n",
            "\n",
            "Top Operations by Memory:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                    aten::empty_strided         3.52%     462.164ms         3.58%     469.572ms       8.494us       0.000us         0.00%       0.000us       0.000us           0 B           0 B     866.21 GB     866.21 GB         55280  \n",
            "                                            aten::empty         1.40%     184.229ms         1.40%     184.229ms       7.858us       0.000us         0.00%       0.000us       0.000us     160.64 KB     160.64 KB     771.92 GB     771.92 GB         23444  \n",
            "                                               aten::mm         3.04%     399.496ms         4.49%     589.666ms      49.803us        3.443s        25.62%        3.443s     290.805us           0 B           0 B     418.08 GB     418.08 GB         11840  \n",
            "                                            aten::addmm         2.28%     299.508ms         2.84%     373.645ms      63.116us        1.916s        14.25%        1.916s     323.604us           0 B           0 B     405.01 GB     405.01 GB          5920  \n",
            "                                              aten::add         0.37%      47.993ms         0.56%      73.340ms      35.260us     366.429ms         2.73%     366.429ms     176.168us           0 B           0 B     187.50 GB     187.50 GB          2080  \n",
            "                                             aten::gelu         0.17%      22.891ms         0.25%      33.251ms      34.637us     223.144ms         1.66%     223.144ms     232.441us           0 B           0 B     180.00 GB     180.00 GB           960  \n",
            "                                    aten::gelu_backward         0.13%      17.442ms         0.22%      28.263ms      29.441us     347.573ms         2.59%     347.573ms     362.056us           0 B           0 B     180.00 GB     180.00 GB           960  \n",
            "                                          aten::resize_         0.03%       3.449ms         0.03%       3.449ms       7.127us       0.000us         0.00%       0.000us       0.000us           0 B           0 B      15.10 GB      15.10 GB           484  \n",
            "                                              aten::sub         0.02%       2.427ms         0.03%       3.512ms      43.898us       3.171ms         0.02%       3.171ms      39.642us           0 B           0 B       2.50 GB       2.50 GB            80  \n",
            "                                              aten::sum         1.39%     182.003ms         2.13%     279.849ms      46.641us     436.063ms         3.24%     436.063ms      72.677us           0 B           0 B      72.81 MB      72.81 MB          6000  \n",
            "                                             aten::tanh         0.02%       2.290ms         0.03%       3.290ms      41.126us     251.752us         0.00%     251.752us       3.147us           0 B           0 B      15.00 MB      15.00 MB            80  \n",
            "                                    aten::tanh_backward         0.01%       1.599ms         0.02%       2.582ms      32.276us     187.983us         0.00%     187.983us       2.350us           0 B           0 B      15.00 MB      15.00 MB            80  \n",
            "                                               aten::eq         0.02%       3.088ms         0.03%       4.281ms      53.515us     243.882us         0.00%     243.882us       3.049us           0 B           0 B       2.50 MB       2.50 MB            80  \n",
            "                                 aten::nll_loss_forward         0.02%       2.896ms         0.03%       3.950ms      49.372us     448.425us         0.00%     448.425us       5.605us           0 B           0 B      80.00 KB      80.00 KB            80  \n",
            "                                aten::nll_loss_backward         0.02%       3.004ms         0.06%       7.760ms      97.003us     263.607us         0.00%     378.096us       4.726us           0 B           0 B      80.00 KB      80.00 KB            80  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 13.134s\n",
            "Self CUDA time total: 13.440s\n",
            "\n",
            "\n",
            " Chrome trace saved to: profiler_trace_baselineonly_epoch1.json\n",
            "  Download and open in chrome://tracing\n",
            "Epoch 2/10 | Val Loss: 0.6818 | Acc: 0.6223 | Macro-F1: 0.6156 | Fake Recall: 0.5114|  T_Time: 15.5s | T_Mem: 15211MB | V_Time: 5.18s | V_Memory: 3210.2 MB\n",
            " -> Best baselineamp model updated!\n",
            "Epoch 3/10 | Val Loss: 0.6965 | Acc: 0.6160 | Macro-F1: 0.5815 | Fake Recall: 0.3425|  T_Time: 85.6s | T_Mem: 21461MB | V_Time: 4.95s | V_Memory: 3208.2 MB\n",
            "Epoch 4/10 | Val Loss: 0.7137 | Acc: 0.6075 | Macro-F1: 0.5437 | Fake Recall: 0.2435|  T_Time: 85.5s | T_Mem: 21461MB | V_Time: 4.89s | V_Memory: 3208.2 MB\n",
            "Epoch 5/10 | Val Loss: 0.7147 | Acc: 0.6114 | Macro-F1: 0.5556 | Fake Recall: 0.2679|  T_Time: 85.4s | T_Mem: 21461MB | V_Time: 4.94s | V_Memory: 3208.2 MB\n",
            "Epoch 6/10 | Val Loss: 0.6732 | Acc: 0.6550 | Macro-F1: 0.6320 | Fake Recall: 0.4221|  T_Time: 85.4s | T_Mem: 21461MB | V_Time: 4.89s | V_Memory: 3208.2 MB\n",
            " -> Best baselineamp model updated!\n",
            "Epoch 7/10 | Val Loss: 0.7307 | Acc: 0.6371 | Macro-F1: 0.5954 | Fake Recall: 0.3295|  T_Time: 85.5s | T_Mem: 21461MB | V_Time: 4.95s | V_Memory: 3208.2 MB\n",
            "Epoch 8/10 | Val Loss: 0.7288 | Acc: 0.6410 | Macro-F1: 0.6047 | Fake Recall: 0.3523|  T_Time: 85.5s | T_Mem: 21461MB | V_Time: 4.88s | V_Memory: 3208.2 MB\n",
            "Epoch 9/10 | Val Loss: 0.7159 | Acc: 0.6526 | Macro-F1: 0.6227 | Fake Recall: 0.3864|  T_Time: 85.5s | T_Mem: 21461MB | V_Time: 4.96s | V_Memory: 3208.2 MB\n",
            "Epoch 10/10 | Val Loss: 0.7278 | Acc: 0.6456 | Macro-F1: 0.6110 | Fake Recall: 0.3620|  T_Time: 85.5s | T_Mem: 21461MB | V_Time: 4.88s | V_Memory: 3208.2 MB\n",
            "Total Training Time: 785.0s\n",
            "Avg Epoch Time: 78.5s\n",
            "Avg Peak Memory: 20878MB\n",
            "\n",
            "========== FINAL TEST RESULT (Official Test Set) ==========\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Fake (0)       0.70      0.40      0.51       553\n",
            "    True (1)       0.65      0.87      0.75       714\n",
            "\n",
            "    accuracy                           0.66      1267\n",
            "   macro avg       0.68      0.63      0.63      1267\n",
            "weighted avg       0.67      0.66      0.64      1267\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title rebuild for baseline only\n",
        "\n",
        "# 1. Re-prepare environment and data\n",
        "\n",
        "# Redefine configuration\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "BATCH_SIZE = 128\n",
        "MAX_LEN = 256\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "VALID_PATH = '/kaggle/input/liar-dataset/valid.tsv'\n",
        "TEST_PATH =  '/kaggle/input/liar-dataset/test.tsv'\n",
        "\n",
        "print(\"Re-loading Tokenizer and Dataloaders...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Re-instantiate datasets (ensure TextualizedLIARDataset class has been run earlier)\n",
        "valid_dataset = TextualizedLIARDataset(VALID_PATH, tokenizer, max_len=MAX_LEN)\n",
        "test_dataset  = TextualizedLIARDataset(TEST_PATH, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "# Re-instantiate DataLoader\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "\n",
        "# 2. Define function to find optimal threshold\n",
        "\n",
        "def find_optimal_threshold(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(\"Running inference on Validation Set...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            # Softmax to obtain probabilities\n",
        "            probs = F.softmax(outputs.logits, dim=1)\n",
        "            # Extract probability for Label 1 (True)\n",
        "            true_probs = probs[:, 1].cpu().numpy()\n",
        "\n",
        "            all_probs.extend(true_probs)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_probs = np.array(all_probs)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Iterate to find best F1 score\n",
        "    best_threshold = 0.5\n",
        "    best_f1 = 0\n",
        "\n",
        "    thresholds = np.arange(0.1, 0.95, 0.05)\n",
        "\n",
        "    print(f\"\\n{'Threshold':<10} | {'Macro F1':<10} | {'Fake Recall':<12} | {'True Recall':<12}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for thresh in thresholds:\n",
        "        preds = (all_probs > thresh).astype(int)\n",
        "\n",
        "        report = classification_report(all_labels, preds, output_dict=True)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        fake_recall = report['0']['recall']\n",
        "        true_recall = report['1']['recall']\n",
        "\n",
        "        print(f\"{thresh:.2f}       | {macro_f1:.4f}     | {fake_recall:.4f}       | {true_recall:.4f}\")\n",
        "\n",
        "        if macro_f1 > best_f1:\n",
        "            best_f1 = macro_f1\n",
        "            best_threshold = thresh\n",
        "\n",
        "    print(f\"\\nBest Threshold found: {best_threshold:.2f}\")\n",
        "    return best_threshold\n",
        "\n",
        "\n",
        "# 3. Run optimization\n",
        "\n",
        "# Load model\n",
        "print(\"\\nLoading model weights from 'best_baselineonly_model.pth'...\")\n",
        "# Must reinitialize model structure before loading weights\n",
        "config = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).config\n",
        "config.num_labels = 2\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
        "model.load_state_dict(torch.load('best_baselineonly_model.pth'))\n",
        "model.to(DEVICE)\n",
        "\n",
        "# 1. Search for best threshold on Validation Set\n",
        "best_thresh = find_optimal_threshold(model, valid_loader, DEVICE)\n",
        "\n",
        "# 2. Apply to Test Set\n",
        "print(f\"\\nApplying Threshold {best_thresh:.2f} to Test Set...\")\n",
        "model.eval()\n",
        "test_probs = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(DEVICE)\n",
        "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "        labels = batch['label'].to(DEVICE)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        probs = F.softmax(outputs.logits, dim=1)\n",
        "        test_probs.extend(probs[:, 1].cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_probs = np.array(test_probs)\n",
        "final_preds = (test_probs > best_thresh).astype(int)\n",
        "\n",
        "print(\"\\n========== OPTIMIZED TEST RESULT ==========\")\n",
        "print(classification_report(test_labels, final_preds, target_names=['Fake (0)', 'True (1)']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsSdS_7v_4rW",
        "outputId": "52351189-c980-43b8-95d5-b5a6b5b00c35",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Re-loading Tokenizer and Dataloaders...\n",
            "\n",
            "Loading model weights from 'best_baselineonly_model.pth'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running inference on Validation Set...\n",
            "\n",
            "Threshold  | Macro F1   | Fake Recall  | True Recall \n",
            "------------------------------------------------------------\n",
            "0.10       | 0.3422     | 0.0000       | 1.0000\n",
            "0.15       | 0.3547     | 0.0114       | 1.0000\n",
            "0.20       | 0.3865     | 0.0422       | 0.9970\n",
            "0.25       | 0.4229     | 0.0795       | 0.9940\n",
            "0.30       | 0.4646     | 0.1315       | 0.9746\n",
            "0.35       | 0.5146     | 0.1997       | 0.9551\n",
            "0.40       | 0.5586     | 0.2695       | 0.9326\n",
            "0.45       | 0.6069     | 0.3506       | 0.9147\n",
            "0.50       | 0.6320     | 0.4221       | 0.8698\n",
            "0.55       | 0.6572     | 0.4984       | 0.8278\n",
            "0.60       | 0.6606     | 0.5633       | 0.7605\n",
            "0.65       | 0.6528     | 0.6364       | 0.6692\n",
            "0.70       | 0.6492     | 0.7354       | 0.5719\n",
            "0.75       | 0.6235     | 0.8117       | 0.4656\n",
            "0.80       | 0.5769     | 0.8718       | 0.3503\n",
            "0.85       | 0.4921     | 0.9464       | 0.1946\n",
            "0.90       | 0.4048     | 0.9870       | 0.0808\n",
            "\n",
            "Best Threshold found: 0.60\n",
            "\n",
            "Applying Threshold 0.60 to Test Set...\n",
            "\n",
            "========== OPTIMIZED TEST RESULT ==========\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Fake (0)       0.63      0.54      0.58       553\n",
            "    True (1)       0.68      0.76      0.72       714\n",
            "\n",
            "    accuracy                           0.66      1267\n",
            "   macro avg       0.66      0.65      0.65      1267\n",
            "weighted avg       0.66      0.66      0.66      1267\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title baseline+Amp\n",
        "\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 128\n",
        "LR = 2e-5\n",
        "EPOCHS = 10\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "NUM_WORKERS = 8\n",
        "\n",
        "TRAIN_PATH = '/kaggle/input/liar-dataset/train.tsv'\n",
        "VALID_PATH = '/kaggle/input/liar-dataset/valid.tsv'\n",
        "TEST_PATH  = '/kaggle/input/liar-dataset/test.tsv'\n",
        "\n",
        "#1. Dataset Class\n",
        "class TextualizedLIARDataset(Dataset):\n",
        "    def __init__(self, tsv_path, tokenizer, max_len=128):\n",
        "        self.df = pd.read_csv(tsv_path, sep='\\t', header=None, names=[\n",
        "            \"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job\",\n",
        "            \"state\", \"party\", \"barely_true_counts\", \"false_counts\",\n",
        "            \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\",\n",
        "            \"context\"\n",
        "        ])\n",
        "\n",
        "        self.df.dropna(subset=['statement'], inplace=True)\n",
        "\n",
        "        #Label logicFalse/Pants-fire/Barely-true = 0 (Fake)\n",
        "        self.label_map = {\n",
        "            \"pants-fire\": 0, \"false\": 0, \"barely-true\": 0,\n",
        "            \"half-true\": 1, \"mostly-true\": 1, \"true\": 1\n",
        "        }\n",
        "\n",
        "        self.df['label'] = self.df['label'].map(self.label_map)\n",
        "        self.df.dropna(subset=['label'], inplace=True)\n",
        "        self.df['label'] = self.df['label'].astype(int)\n",
        "\n",
        "        text_cols = ['statement', 'subject', 'speaker', 'party', 'state', 'speaker_job', 'context']\n",
        "        for col in text_cols:\n",
        "            self.df[col] = self.df[col].fillna(\"Unknown\")\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        metadata_str = (\n",
        "            f\"Speaker: {row['speaker']} | \"\n",
        "            f\"Job: {row['speaker_job']} | \"\n",
        "            f\"Party: {row['party']} | \"\n",
        "            f\"State: {row['state']} | \"\n",
        "            f\"Context: {row['context']} | \"\n",
        "            f\"Subject: {row['subject']}\"\n",
        "        )\n",
        "        final_text = f\"{metadata_str} [SEP] Statement: {row['statement']}\"\n",
        "\n",
        "        encoded = self.tokenizer(\n",
        "            final_text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(row['label'], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "#2. Training and Evaluation Functions\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "    reset_peak_memory()\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
        "        labels = batch['label'].to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', dtype=dtype):\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_time = end_time - start_time\n",
        "\n",
        "    peak_memory = get_peak_gpu_memory_mb()\n",
        "\n",
        "    return total_loss / len(dataloader), epoch_time, peak_memory\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    reset_peak_memory()\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    eval_time = time.time() - start_time\n",
        "    peak_memory = get_peak_gpu_memory_mb()\n",
        "\n",
        "    return total_loss / len(dataloader), accuracy_score(all_labels, all_preds), all_labels, all_preds, eval_time, peak_memory\n",
        "\n",
        "#Main training\n",
        "def run_official_split_training():\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    print(f\"Using device: {DEVICE} | Model: {MODEL_NAME}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    print(\"Loading Official Datasets (Train / Valid / Test)...\")\n",
        "    if not os.path.exists(TRAIN_PATH):\n",
        "        print(f\"Error: Path {TRAIN_PATH} not found.\")\n",
        "        return\n",
        "\n",
        "    train_dataset = TextualizedLIARDataset(TRAIN_PATH, tokenizer, max_len=MAX_LEN)\n",
        "    valid_dataset = TextualizedLIARDataset(VALID_PATH, tokenizer, max_len=MAX_LEN)\n",
        "    test_dataset  = TextualizedLIARDataset(TEST_PATH, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "    # Build DataLoader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "    test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "    print(f\"Stats: Train={len(train_dataset)}, Valid={len(valid_dataset)}, Test={len(test_dataset)}\")\n",
        "\n",
        "    # Get Class Weights (directly from train_dataset)\n",
        "    print(\"Calculating class weights from Training set...\")\n",
        "    train_labels = train_dataset.df['label'].values\n",
        "\n",
        "    # Automatically compute class weights\n",
        "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
        "\n",
        "    print(f\"Class Weights: {class_weights} (Index 0 is Fake, Index 1 is True)\")\n",
        "\n",
        "    # Define the weighted loss\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "    # Model Loading and Configuration (Dropout 0.3)\n",
        "    print(\"Loading model with increased dropout...\")\n",
        "    config = BertConfig.from_pretrained(MODEL_NAME)\n",
        "    config.hidden_dropout_prob = 0.3\n",
        "    config.attention_probs_dropout_prob = 0.3\n",
        "    config.num_labels = 2\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
        "\n",
        "    # calculate the num of parameters\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"trainable params: {trainable:,} || all params: {total:,} || trainable%: {100*trainable/total:.4f}\")\n",
        "\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
        "\n",
        "    total_train_time = 0\n",
        "    epoch_times = []\n",
        "    epoch_memories = []\n",
        "\n",
        "\n",
        "    total_steps = len(train_loader) * EPOCHS\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)\n",
        "\n",
        "    best_val_f1 = 0\n",
        "\n",
        "    print(\"\\nStarting Training on Official Split...\")\n",
        "\n",
        "    profiler_data = {\n",
        "    'cpu_time': [],\n",
        "    'cuda_time': [],\n",
        "    'memory': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "      if epoch == 2:\n",
        "        print(\"Profiling enabled for epoch 2 ...\")\n",
        "\n",
        "        with profile(\n",
        "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "            record_shapes=True,\n",
        "            profile_memory=True,\n",
        "            with_stack=False\n",
        "        ) as prof:\n",
        "            train_loss, train_time, train_memory = train_epoch_with_profiler(\n",
        "                model, train_loader, optimizer, scheduler, criterion, DEVICE, prof\n",
        "            )\n",
        "\n",
        "        print(\"PROFILER SUMMARY (Epoch 2)\")\n",
        "\n",
        "\n",
        "        print(\"\\nTop Operations by CPU Time:\")\n",
        "        print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=15))\n",
        "\n",
        "        print(\"\\nTop Operations by CUDA Time:\")\n",
        "        try:\n",
        "            print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=15))\n",
        "        except:\n",
        "            print(prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=15))\n",
        "\n",
        "        print(\"\\nTop Operations by Memory:\")\n",
        "        print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=15))\n",
        "\n",
        "        trace_file = \"profiler_trace_epoch1.json\"# profile records\n",
        "        prof.export_chrome_trace(trace_file)\n",
        "        print(f\"\\ Chrome trace saved to: {trace_file}\")\n",
        "        print(\"  Download and open in chrome://tracing\")\n",
        "      else:\n",
        "        train_loss, train_time, train_memory = train_epoch(\n",
        "          model, train_loader, optimizer, scheduler, criterion, DEVICE\n",
        "        )\n",
        "\n",
        "      val_loss, val_acc, val_labels, val_preds, val_time, val_memory = evaluate(\n",
        "          model, valid_loader, criterion, DEVICE\n",
        "      )\n",
        "\n",
        "      total_train_time += train_time\n",
        "      epoch_times.append(train_time)\n",
        "      epoch_memories.append(train_memory)\n",
        "\n",
        "      report_dict = classification_report(val_labels, val_preds, output_dict=True)\n",
        "      macro_f1 = report_dict['macro avg']['f1-score']\n",
        "      fake_recall = report_dict['0']['recall']\n",
        "\n",
        "      print(f\"Epoch {epoch}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Macro-F1: {macro_f1:.4f} | Fake Recall: {fake_recall:.4f}|  T_Time: {train_time:.1f}s | T_Mem: {train_memory:.0f}MB | V_Time: {val_time:.2f}s | V_Memory: {val_memory:.1f} MB\")\n",
        "\n",
        "      if macro_f1 > best_val_f1:\n",
        "          best_val_f1 = macro_f1\n",
        "          torch.save(model.state_dict(), 'best_baselineamp_model.pth')\n",
        "          print(\" -> Best baselineamp model updated!\")\n",
        "\n",
        "\n",
        "    print(f\"Total Training Time: {total_train_time:.1f}s\")\n",
        "    print(f\"Avg Epoch Time: {sum(epoch_times)/len(epoch_times):.1f}s\")\n",
        "    print(f\"Avg Peak Memory: {sum(epoch_memories)/len(epoch_memories):.0f}MB\")\n",
        "\n",
        "    # automatically run best model on Official Test Set after training\n",
        "    print(\"\\n========== FINAL TEST RESULT (Official Test Set) ==========\")\n",
        "    model.load_state_dict(torch.load('best_baselineamp_model.pth'))\n",
        "    test_loss, test_acc, test_labels, test_preds, test_time, test_memory = evaluate(model, test_loader, criterion, DEVICE)\n",
        "    print(classification_report(test_labels, test_preds, target_names=['Fake (0)', 'True (1)']))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_official_split_training()"
      ],
      "metadata": {
        "id": "CT1LYjFO2YxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d43e1cb1-afb8-4af1-a6ea-4a98be165e52",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda | Model: bert-base-uncased\n",
            "Loading Official Datasets (Train / Valid / Test)...\n",
            "Stats: Train=10240, Valid=1284, Test=1267\n",
            "Calculating class weights from Training set...\n",
            "Class Weights: [1.14081996 0.89012517] (Index 0 is Fake, Index 1 is True)\n",
            "Loading model with increased dropout...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 109,483,778 || all params: 109,483,778 || trainable%: 100.0000\n",
            "\n",
            "Starting Training on Official Split...\n",
            "Epoch 1/10 | Train Loss: 0.6994 | Val Loss: 0.6917 | Acc: 0.5576 | Macro-F1: 0.4643 | Fake Recall: 0.1461|  T_Time: 14.1s | T_Mem: 16057MB | V_Time: 3.81s | V_Memory: 4475.1 MB\n",
            " -> Best baselineamp model updated!\n",
            "Profiling enabled for epoch 2 ...\n",
            "PROFILER SUMMARY (Epoch 2)\n",
            "\n",
            "Top Operations by CPU Time:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                             aten::item         0.35%      44.766ms        42.45%        5.420s     167.676us       0.000us         0.00%       2.076ms       0.064us           0 B           0 B           0 B           0 B         32322  \n",
            "                              aten::_local_scalar_dense         0.19%      24.431ms        42.10%        5.375s     166.291us     338.850us         0.00%       2.076ms       0.064us           0 B           0 B           0 B           0 B         32322  \n",
            "                                  cudaStreamSynchronize        41.86%        5.344s        41.87%        5.344s      33.403ms       0.000us         0.00%       1.737ms      10.856us           0 B           0 B           0 B           0 B           160  \n",
            "                                           aten::linear         1.01%     128.950ms        13.83%        1.765s     149.084us       0.000us         0.00%        4.199s     354.642us           0 B           0 B    1005.04 GB     -96.00 MB         11840  \n",
            "                                               aten::to         0.72%      91.887ms        10.67%        1.362s      26.478us       0.000us         0.00%        1.172s      22.779us           0 B           0 B     689.18 GB           0 B         51440  \n",
            "    autograd::engine::evaluate_function: AddmmBackward0         1.32%     168.801ms        10.55%        1.347s     227.495us       0.000us         0.00%        3.867s     653.140us           0 B           0 B    -362.11 GB    -780.18 GB          5920  \n",
            "                                         aten::_to_copy         2.26%     289.123ms         9.95%        1.270s      36.331us       0.000us         0.00%        1.172s      33.517us           0 B           0 B     689.18 GB      -1.50 KB         34960  \n",
            "                                       cudaLaunchKernel         7.32%     933.980ms         7.35%     938.250ms      10.024us       0.000us         0.00%      69.184us       0.001us           0 B           0 B           0 B           0 B         93600  \n",
            "autograd::engine::evaluate_function: ToCopyBackward0...         1.41%     180.005ms         7.30%     931.679ms      58.818us       0.000us         0.00%        1.254s      79.145us           0 B           0 B    -167.16 GB    -553.13 GB         15840  \n",
            "                                         AddmmBackward0         0.81%     102.802ms         6.84%     873.008ms     147.468us       0.000us         0.00%        3.436s     580.333us           0 B           0 B     418.06 GB           0 B          5920  \n",
            "                              Optimizer.step#AdamW.step         2.20%     281.049ms         5.89%     751.868ms       9.398ms       0.000us         0.00%     454.313ms       5.679ms           0 B        -320 B           0 B     -33.08 GB            80  \n",
            "                                            aten::copy_         2.52%     321.588ms         5.39%     688.194ms      19.507us        1.176s         8.76%        1.176s      33.327us           0 B           0 B           0 B           0 B         35280  \n",
            "                                        ToCopyBackward0         0.41%      52.008ms         5.30%     675.975ms      42.675us       0.000us         0.00%     589.594ms      37.222us           0 B           0 B     385.97 GB           0 B         15840  \n",
            "                                               aten::mm         3.09%     393.916ms         4.63%     590.769ms      49.896us        3.436s        25.61%        3.436s     290.167us           0 B           0 B     418.06 GB     418.06 GB         11840  \n",
            "                                    aten::empty_strided         3.36%     428.902ms         3.41%     435.147ms       7.872us       0.000us         0.00%      90.047us       0.002us           0 B           0 B     866.67 GB     866.67 GB         55280  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 12.766s\n",
            "Self CUDA time total: 13.415s\n",
            "\n",
            "\n",
            "Top Operations by CUDA Time:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::linear         1.01%     128.950ms        13.83%        1.765s     149.084us       0.000us         0.00%        4.199s     354.642us           0 B           0 B    1005.04 GB     -96.00 MB         11840  \n",
            "    autograd::engine::evaluate_function: AddmmBackward0         1.32%     168.801ms        10.55%        1.347s     227.495us       0.000us         0.00%        3.867s     653.140us           0 B           0 B    -362.11 GB    -780.18 GB          5920  \n",
            "                                         AddmmBackward0         0.81%     102.802ms         6.84%     873.008ms     147.468us       0.000us         0.00%        3.436s     580.333us           0 B           0 B     418.06 GB           0 B          5920  \n",
            "                                               aten::mm         3.09%     393.916ms         4.63%     590.769ms      49.896us        3.436s        25.61%        3.436s     290.167us           0 B           0 B     418.06 GB     418.06 GB         11840  \n",
            "autograd::engine::evaluate_function: ScaledDotProduc...         0.24%      30.280ms         1.79%     228.758ms     238.290us       0.000us         0.00%        1.991s       2.074ms     -15.00 KB     -15.00 KB    -106.68 GB    -241.76 GB           960  \n",
            "            ScaledDotProductEfficientAttentionBackward0         0.11%      13.987ms         1.55%     198.478ms     206.748us       0.000us         0.00%        1.991s       2.074ms           0 B           0 B     135.08 GB           0 B           960  \n",
            "aten::_scaled_dot_product_efficient_attention_backwa...         0.21%      26.889ms         1.45%     184.492ms     192.179us       0.000us         0.00%        1.991s       2.074ms           0 B           0 B     135.08 GB           0 B           960  \n",
            "                    aten::_efficient_attention_backward         0.31%      39.829ms         0.94%     119.873ms     124.867us        1.942s        14.48%        1.991s       2.074ms           0 B           0 B     135.08 GB     -92.17 GB           960  \n",
            "fmha_cutlassB_bf16_aligned_64x64_k64_dropout_sm80(Py...         0.00%       0.000us         0.00%       0.000us       0.000us        1.942s        14.48%        1.942s       2.023ms           0 B           0 B           0 B           0 B           960  \n",
            "                                            aten::addmm         2.19%     279.040ms         2.76%     352.585ms      59.558us        1.905s        14.20%        1.905s     321.761us           0 B           0 B     405.01 GB     405.01 GB          5920  \n",
            "                     aten::scaled_dot_product_attention         0.17%      21.360ms         2.29%     292.021ms     152.094us       0.000us         0.00%        1.505s     783.722us      30.00 KB           0 B     108.48 GB           0 B          1920  \n",
            "ampere_bf16_s16816gemm_bf16_128x128_ldg8_relu_f2f_st...         0.00%       0.000us         0.00%       0.000us       0.000us        1.289s         9.61%        1.289s     268.646us           0 B           0 B           0 B           0 B          4800  \n",
            "autograd::engine::evaluate_function: ToCopyBackward0...         1.41%     180.005ms         7.30%     931.679ms      58.818us       0.000us         0.00%        1.254s      79.145us           0 B           0 B    -167.16 GB    -553.13 GB         15840  \n",
            "ampere_bf16_s16816gemm_bf16_128x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us        1.237s         9.22%        1.237s     257.671us           0 B           0 B           0 B           0 B          4800  \n",
            "                                            aten::copy_         2.52%     321.588ms         5.39%     688.194ms      19.507us        1.176s         8.76%        1.176s      33.327us           0 B           0 B           0 B           0 B         35280  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 12.766s\n",
            "Self CUDA time total: 13.415s\n",
            "\n",
            "\n",
            "Top Operations by Memory:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                    aten::empty_strided         3.36%     428.902ms         3.41%     435.147ms       7.872us       0.000us         0.00%      90.047us       0.002us           0 B           0 B     866.67 GB     866.67 GB         55280  \n",
            "                                            aten::empty         1.34%     171.405ms         1.34%     171.405ms       7.311us       0.000us         0.00%       0.000us       0.000us     160.64 KB     160.64 KB     772.96 GB     772.96 GB         23444  \n",
            "                                               aten::mm         3.09%     393.916ms         4.63%     590.769ms      49.896us        3.436s        25.61%        3.436s     290.167us           0 B           0 B     418.06 GB     418.06 GB         11840  \n",
            "                                            aten::addmm         2.19%     279.040ms         2.76%     352.585ms      59.558us        1.905s        14.20%        1.905s     321.761us           0 B           0 B     405.01 GB     405.01 GB          5920  \n",
            "                                              aten::add         0.36%      45.527ms         0.53%      67.468ms      32.437us     366.311ms         2.73%     366.315ms     176.113us           0 B           0 B     187.50 GB     187.50 GB          2080  \n",
            "                                             aten::gelu         0.17%      21.227ms         0.25%      31.321ms      32.626us     223.031ms         1.66%     223.031ms     232.324us           0 B           0 B     180.00 GB     180.00 GB           960  \n",
            "                                    aten::gelu_backward         0.14%      17.431ms         0.22%      28.359ms      29.540us     347.598ms         2.59%     347.598ms     362.081us           0 B           0 B     180.00 GB     180.00 GB           960  \n",
            "                                          aten::resize_         0.02%       3.144ms         0.02%       3.144ms       6.495us       0.000us         0.00%       0.000us       0.000us           0 B           0 B      15.10 GB      15.10 GB           484  \n",
            "                                              aten::sub         0.02%       2.137ms         0.02%       3.173ms      39.666us       3.177ms         0.02%       3.177ms      39.715us           0 B           0 B       2.50 GB       2.50 GB            80  \n",
            "                                           Buffer Flush         0.04%       5.213ms         0.05%       5.825ms     306.567us      85.376us         0.00%      85.376us       4.493us         -16 B         -16 B     165.33 MB      74.96 MB            19  \n",
            "                                              aten::sum         1.42%     181.654ms         2.21%     282.191ms      47.032us     435.788ms         3.25%     435.788ms      72.631us           0 B           0 B      72.81 MB      72.81 MB          6000  \n",
            "                                             aten::tanh         0.02%       2.175ms         0.02%       3.174ms      39.672us     250.620us         0.00%     250.620us       3.133us           0 B           0 B      15.00 MB      15.00 MB            80  \n",
            "                                    aten::tanh_backward         0.01%       1.813ms         0.02%       2.824ms      35.296us     187.551us         0.00%     187.551us       2.344us           0 B           0 B      15.00 MB      15.00 MB            80  \n",
            "                                               aten::eq         0.02%       2.952ms         0.03%       4.116ms      51.455us     245.671us         0.00%     245.671us       3.071us           0 B           0 B       2.50 MB       2.50 MB            80  \n",
            "                                 aten::nll_loss_forward         0.02%       2.678ms         0.03%       3.723ms      46.542us     402.663us         0.00%     402.663us       5.033us           0 B           0 B      80.00 KB      80.00 KB            80  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 12.766s\n",
            "Self CUDA time total: 13.415s\n",
            "\n",
            "\\ Chrome trace saved to: profiler_trace_epoch1.json\n",
            "  Download and open in chrome://tracing\n",
            "Epoch 2/10 | Train Loss: 0.6747 | Val Loss: 0.6803 | Acc: 0.6184 | Macro-F1: 0.6112 | Fake Recall: 0.5032|  T_Time: 14.3s | T_Mem: 16057MB | V_Time: 4.78s | V_Memory: 4475.1 MB\n",
            " -> Best baselineamp model updated!\n",
            "Epoch 3/10 | Train Loss: 0.6583 | Val Loss: 0.7090 | Acc: 0.6106 | Macro-F1: 0.5644 | Fake Recall: 0.2971|  T_Time: 14.9s | T_Mem: 16057MB | V_Time: 4.49s | V_Memory: 4475.1 MB\n",
            "Epoch 4/10 | Train Loss: 0.6399 | Val Loss: 0.6824 | Acc: 0.6355 | Macro-F1: 0.6070 | Fake Recall: 0.3815|  T_Time: 14.8s | T_Mem: 16057MB | V_Time: 4.56s | V_Memory: 4475.1 MB\n",
            "Epoch 5/10 | Train Loss: 0.6238 | Val Loss: 0.7072 | Acc: 0.6386 | Macro-F1: 0.6047 | Fake Recall: 0.3604|  T_Time: 14.9s | T_Mem: 16057MB | V_Time: 4.54s | V_Memory: 4475.1 MB\n",
            "Epoch 6/10 | Train Loss: 0.6103 | Val Loss: 0.6792 | Acc: 0.6511 | Macro-F1: 0.6261 | Fake Recall: 0.4091|  T_Time: 14.8s | T_Mem: 16057MB | V_Time: 4.55s | V_Memory: 4475.1 MB\n",
            " -> Best baselineamp model updated!\n",
            "Epoch 7/10 | Train Loss: 0.5947 | Val Loss: 0.7221 | Acc: 0.6480 | Macro-F1: 0.6122 | Fake Recall: 0.3588|  T_Time: 14.9s | T_Mem: 16057MB | V_Time: 4.56s | V_Memory: 4475.1 MB\n",
            "Epoch 8/10 | Train Loss: 0.5834 | Val Loss: 0.7157 | Acc: 0.6519 | Macro-F1: 0.6213 | Fake Recall: 0.3831|  T_Time: 15.0s | T_Mem: 16057MB | V_Time: 4.56s | V_Memory: 4475.1 MB\n",
            "Epoch 9/10 | Train Loss: 0.5759 | Val Loss: 0.7066 | Acc: 0.6565 | Macro-F1: 0.6333 | Fake Recall: 0.4221|  T_Time: 14.9s | T_Mem: 16057MB | V_Time: 4.56s | V_Memory: 4475.1 MB\n",
            " -> Best baselineamp model updated!\n",
            "Epoch 10/10 | Train Loss: 0.5697 | Val Loss: 0.7086 | Acc: 0.6488 | Macro-F1: 0.6244 | Fake Recall: 0.4107|  T_Time: 14.9s | T_Mem: 16057MB | V_Time: 4.49s | V_Memory: 4475.1 MB\n",
            "Total Training Time: 147.4s\n",
            "Avg Epoch Time: 14.7s\n",
            "Avg Peak Memory: 16057MB\n",
            "\n",
            "========== FINAL TEST RESULT (Official Test Set) ==========\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Fake (0)       0.68      0.41      0.51       553\n",
            "    True (1)       0.65      0.85      0.74       714\n",
            "\n",
            "    accuracy                           0.66      1267\n",
            "   macro avg       0.66      0.63      0.62      1267\n",
            "weighted avg       0.66      0.66      0.64      1267\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title rebuild for baseline + amp\n",
        "\n",
        "# 1. Re-prepare environment and data\n",
        "\n",
        "# Redefine configuration\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "BATCH_SIZE = 128\n",
        "MAX_LEN = 256\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "VALID_PATH = '/kaggle/input/liar-dataset/valid.tsv'\n",
        "TEST_PATH =  '/kaggle/input/liar-dataset/test.tsv'\n",
        "\n",
        "print(\"Re-loading Tokenizer and Dataloaders...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Re-instantiate datasets (make sure TextualizedLIARDataset class has been executed earlier)\n",
        "valid_dataset = TextualizedLIARDataset(VALID_PATH, tokenizer, max_len=MAX_LEN)\n",
        "test_dataset  = TextualizedLIARDataset(TEST_PATH, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "# Re-instantiate DataLoaders\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# 2. Define function to search for best threshold\n",
        "def find_optimal_threshold(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(\"Running inference on Validation Set...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Use softmax to obtain probabilities\n",
        "            probs = F.softmax(outputs.logits, dim=1)\n",
        "            # Extract probability of label 1 (True)\n",
        "            true_probs = probs[:, 1].cpu().numpy()\n",
        "\n",
        "            all_probs.extend(true_probs)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_probs = np.array(all_probs)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Iterate to find best F1 score\n",
        "    best_threshold = 0.5\n",
        "    best_f1 = 0\n",
        "\n",
        "    thresholds = np.arange(0.1, 0.95, 0.05)\n",
        "\n",
        "    print(f\"\\n{'Threshold':<10} | {'Macro F1':<10} | {'Fake Recall':<12} | {'True Recall':<12}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for thresh in thresholds:\n",
        "        preds = (all_probs > thresh).astype(int)\n",
        "\n",
        "        report = classification_report(all_labels, preds, output_dict=True)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        fake_recall = report['0']['recall']\n",
        "        true_recall = report['1']['recall']\n",
        "\n",
        "        print(f\"{thresh:.2f}       | {macro_f1:.4f}     | {fake_recall:.4f}       | {true_recall:.4f}\")\n",
        "\n",
        "        if macro_f1 > best_f1:\n",
        "            best_f1 = macro_f1\n",
        "            best_threshold = thresh\n",
        "\n",
        "    print(f\"\\nBest Threshold found: {best_threshold:.2f}\")\n",
        "    return best_threshold\n",
        "\n",
        "\n",
        "# 3. Run optimization\n",
        "\n",
        "# Load model\n",
        "print(\"\\nLoading model weights from 'best_baselineamp_model.pth'...\")\n",
        "# Must reinitialize model architecture before loading weights\n",
        "config = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).config\n",
        "config.num_labels = 2\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
        "model.load_state_dict(torch.load('best_baselineamp_model.pth'))\n",
        "model.to(DEVICE)\n",
        "\n",
        "# 1. Find best threshold on Validation Set\n",
        "best_thresh = find_optimal_threshold(model, valid_loader, DEVICE)\n",
        "\n",
        "# 2. Apply optimal threshold to Test Set\n",
        "print(f\"\\nApplying Threshold {best_thresh:.2f} to Test Set...\")\n",
        "model.eval()\n",
        "test_probs = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(DEVICE)\n",
        "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "        labels = batch['label'].to(DEVICE)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        probs = F.softmax(outputs.logits, dim=1)\n",
        "        test_probs.extend(probs[:, 1].cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_probs = np.array(test_probs)\n",
        "final_preds = (test_probs > best_thresh).astype(int)\n",
        "\n",
        "print(\"\\n========== OPTIMIZED TEST RESULT ==========\")\n",
        "print(classification_report(test_labels, final_preds, target_names=['Fake (0)', 'True (1)']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oW2a7YqQykK",
        "outputId": "6b893462-e164-4b82-ba22-376e2a245c83",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Re-loading Tokenizer and Dataloaders...\n",
            "\n",
            "Loading model weights from 'best_baselineamp_model.pth'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running inference on Validation Set...\n",
            "\n",
            "Threshold  | Macro F1   | Fake Recall  | True Recall \n",
            "------------------------------------------------------------\n",
            "0.10       | 0.3582     | 0.0146       | 1.0000\n",
            "0.15       | 0.3845     | 0.0406       | 0.9955\n",
            "0.20       | 0.4392     | 0.1006       | 0.9820\n",
            "0.25       | 0.4865     | 0.1575       | 0.9731\n",
            "0.30       | 0.5153     | 0.2013       | 0.9536\n",
            "0.35       | 0.5534     | 0.2549       | 0.9461\n",
            "0.40       | 0.5832     | 0.3052       | 0.9311\n",
            "0.45       | 0.6060     | 0.3523       | 0.9102\n",
            "0.50       | 0.6333     | 0.4221       | 0.8728\n",
            "0.55       | 0.6436     | 0.4805       | 0.8204\n",
            "0.60       | 0.6546     | 0.5373       | 0.7769\n",
            "0.65       | 0.6605     | 0.6055       | 0.7156\n",
            "0.70       | 0.6464     | 0.6623       | 0.6317\n",
            "0.75       | 0.6331     | 0.7597       | 0.5225\n",
            "0.80       | 0.6043     | 0.8409       | 0.4132\n",
            "0.85       | 0.5463     | 0.9058       | 0.2859\n",
            "0.90       | 0.4568     | 0.9643       | 0.1452\n",
            "\n",
            "Best Threshold found: 0.65\n",
            "\n",
            "Applying Threshold 0.65 to Test Set...\n",
            "\n",
            "========== OPTIMIZED TEST RESULT ==========\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Fake (0)       0.60      0.59      0.60       553\n",
            "    True (1)       0.69      0.70      0.69       714\n",
            "\n",
            "    accuracy                           0.65      1267\n",
            "   macro avg       0.65      0.64      0.64      1267\n",
            "weighted avg       0.65      0.65      0.65      1267\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title LORA\n",
        "!pip install peft --break-system-packages"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18hoIU4HL5BB",
        "outputId": "66bc30d4-2c32-4cc3-9f00-96788383b825",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.12.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.7.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title grid search outside function\n",
        "\n",
        "def grid_search_lora_bert_model(\n",
        "    model_name,\n",
        "    train_loader,\n",
        "    valid_loader,\n",
        "    param_grid,\n",
        "    tokenizer,\n",
        "    device,\n",
        "    class_weights_tensor,\n",
        "    epochs_per_config=2,\n",
        "    verbose=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Perform Grid Search for BERT + LoRA model\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate all parameter combinations\n",
        "    keys = param_grid.keys()\n",
        "    values = param_grid.values()\n",
        "    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"GRID SEARCH START\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Total configurations to search: {len(param_combinations)}\")\n",
        "    print(f\"Epochs per configuration: {epochs_per_config}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    results = []\n",
        "    best_f1 = 0\n",
        "    best_config = None\n",
        "    best_model_state = None\n",
        "    last_r = None\n",
        "\n",
        "    for idx, params in enumerate(param_combinations, 1):\n",
        "        # Print current configuration (compact format)\n",
        "        param_str = \" | \".join([f\"{k}={v}\" for k, v in params.items()])\n",
        "        print(f\"[{idx}/{len(param_combinations)}] {param_str}\")\n",
        "\n",
        "        # Re-create DataLoader if batch_size is part of parameters\n",
        "        if 'batch_size' in params:\n",
        "            current_train_loader = DataLoader(\n",
        "                train_loader.dataset,\n",
        "                batch_size=params['batch_size'],\n",
        "                shuffle=True,\n",
        "                num_workers=train_loader.num_workers,\n",
        "                pin_memory=True\n",
        "            )\n",
        "            current_valid_loader = DataLoader(\n",
        "                valid_loader.dataset,\n",
        "                batch_size=params['batch_size'],\n",
        "                shuffle=False,\n",
        "                num_workers=valid_loader.num_workers,\n",
        "                pin_memory=True\n",
        "            )\n",
        "        else:\n",
        "            current_train_loader = train_loader\n",
        "            current_valid_loader = valid_loader\n",
        "\n",
        "        # Create base BERT classification model\n",
        "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=2\n",
        "        )\n",
        "\n",
        "        # Configure LoRA\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            r=params.get('lora_r', 8),\n",
        "            lora_alpha=params.get('lora_alpha', 32),\n",
        "            lora_dropout=params.get('lora_dropout', 0.01),\n",
        "            target_modules=[\"query\", \"value\"],\n",
        "            bias=\"none\",\n",
        "            inference_mode=False\n",
        "        )\n",
        "\n",
        "        # Apply LoRA\n",
        "        model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "        # Print trainable parameters when LoRA rank changes\n",
        "        if verbose:\n",
        "            current_r = params.get('lora_r', 8)\n",
        "            if current_r != last_r:\n",
        "                model.print_trainable_parameters()\n",
        "                last_r = current_r\n",
        "\n",
        "        model.to(device)\n",
        "\n",
        "        # Create optimizer\n",
        "        optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=params.get('learning_rate', 2e-5),\n",
        "            weight_decay=params.get('weight_decay', 0.01)\n",
        "        )\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        total_steps = len(current_train_loader) * epochs_per_config\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=int(0.1 * total_steps),\n",
        "            num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "        # Loss function\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "        # Training loop (no detailed epoch-level printing)\n",
        "        best_epoch_f1 = 0\n",
        "        best_epoch_acc = 0\n",
        "        best_epoch_loss = float('inf')\n",
        "\n",
        "        for epoch in range(1, epochs_per_config + 1):\n",
        "            train_loss, train_time, train_memory = train_epoch(\n",
        "                model, current_train_loader, optimizer, scheduler, criterion, device\n",
        "            )\n",
        "            val_loss, val_acc, val_labels, val_preds, val_time, val_memory = evaluate(\n",
        "                model, current_valid_loader, criterion, device\n",
        "            )\n",
        "\n",
        "            report_dict = classification_report(\n",
        "                val_labels, val_preds, output_dict=True, zero_division=0\n",
        "            )\n",
        "            macro_f1 = report_dict['macro avg']['f1-score']\n",
        "\n",
        "            # Track best epoch result under this configuration\n",
        "            if macro_f1 > best_epoch_f1:\n",
        "                best_epoch_f1 = macro_f1\n",
        "                best_epoch_acc = val_acc\n",
        "                best_epoch_loss = val_loss\n",
        "\n",
        "        # Print results for this configuration (one-line summary)\n",
        "        print(\n",
        "            f\" Val Loss: {best_epoch_loss:.4f} | Val Acc: {best_epoch_acc:.4f} | Val F1: {best_epoch_f1:.4f}\",\n",
        "            end=\"\"\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        result = {\n",
        "            'params': params.copy(),\n",
        "            'best_val_f1': best_epoch_f1,\n",
        "            'best_val_acc': best_epoch_acc,\n",
        "            'best_val_loss': best_epoch_loss,\n",
        "            'final_train_time': train_time,\n",
        "            'final_memory': train_memory\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        # Update global best\n",
        "        if best_epoch_f1 > best_f1:\n",
        "            best_f1 = best_epoch_f1\n",
        "            best_config = params.copy()\n",
        "            best_model_state = deepcopy(model.state_dict())\n",
        "            print(\"NEW BEST!\")\n",
        "        else:\n",
        "            print()  # newline\n",
        "\n",
        "        # Memory cleanup\n",
        "        del model, base_model, optimizer, scheduler\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Print final summary\n",
        "    print(\"\\nGRID SEARCH COMPLETE\\n\")\n",
        "    print(\"Top 5 Configurations (sorted by F1 score):\")\n",
        "\n",
        "    results_sorted = sorted(results, key=lambda x: x['best_val_f1'], reverse=True)\n",
        "\n",
        "    for i, r in enumerate(results_sorted[:5], 1):\n",
        "        param_str = \" | \".join([f\"{k}={v}\" for k, v in r['params'].items()])\n",
        "        print(f\"{i}. F1: {r['best_val_f1']:.4f} | Acc: {r['best_val_acc']:.4f} | Loss: {r['best_val_loss']:.4f}\")\n",
        "        print(f\"   {param_str}\")\n",
        "\n",
        "    print(\"\\nBEST CONFIGURATION:\")\n",
        "    print(f\"Best Validation F1: {best_f1:.4f}\")\n",
        "    param_str = \" | \".join([f\"{k}={v}\" for k, v in best_config.items()])\n",
        "    print(f\"Parameters: {param_str}\\n\")\n",
        "\n",
        "    return {\n",
        "        'all_results': results,\n",
        "        'best_config': best_config,\n",
        "        'best_f1': best_f1,\n",
        "        'best_model_state': best_model_state\n",
        "    }\n"
      ],
      "metadata": {
        "id": "HX6o2Xc8dKmz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Lora grid search before final training\n",
        "\n",
        "def run_lora_training_with_grid_search():\n",
        "    torch.manual_seed(42)\n",
        "    np.random_seed(42)\n",
        "    print(f\"Using device: {DEVICE} | Model: {MODEL_NAME}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    print(\"Loading Official Datasets (Train / Valid / Test)...\")\n",
        "    if not os.path.exists(TRAIN_PATH):\n",
        "        print(f\"Error: Path {TRAIN_PATH} not found.\")\n",
        "        return\n",
        "\n",
        "    train_dataset = TextualizedLIARDataset(TRAIN_PATH, tokenizer, max_len=MAX_LEN)\n",
        "    valid_dataset = TextualizedLIARDataset(VALID_PATH, tokenizer, max_len=MAX_LEN)\n",
        "    test_dataset  = TextualizedLIARDataset(TEST_PATH, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    valid_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(f\"Stats: Train={len(train_dataset)}, Valid={len(valid_dataset)}, Test={len(test_dataset)}\")\n",
        "\n",
        "    # Calculate class weights\n",
        "    print(\"Calculating class weights from Training set...\")\n",
        "    train_labels = train_dataset.df['label'].values\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(train_labels),\n",
        "        y=train_labels\n",
        "    )\n",
        "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
        "    print(f\"Class Weights: {class_weights}\")\n",
        "\n",
        "    # Define LoRA Parameter Grid\n",
        "    param_grid = {\n",
        "        'learning_rate': [1e-5, 2e-5, 3e-5],\n",
        "        'lora_r': [4, 8, 16],\n",
        "        'lora_alpha': [16, 32],\n",
        "        'lora_dropout': [0.0, 0.1],\n",
        "        'weight_decay': [0.01]\n",
        "        # 'batch_size': [64, 128]  # enable this if you want to grid-search batch_size\n",
        "    }\n",
        "\n",
        "    # Run Grid Search\n",
        "    print(\"Starting Grid Search for LoRA Model...\")\n",
        "\n",
        "    search_results = grid_search_lora_bert_model(\n",
        "        model_name=MODEL_NAME,\n",
        "        train_loader=train_loader,\n",
        "        valid_loader=valid_loader,\n",
        "        param_grid=param_grid,\n",
        "        tokenizer=tokenizer,\n",
        "        device=DEVICE,\n",
        "        class_weights_tensor=class_weights_tensor,\n",
        "        epochs_per_config=2,  # train 2 epochs per configuration\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Retrain Full Model with Best Configuration\n",
        "    print(\"Training final model with best LoRA configuration...\")\n",
        "\n",
        "    best_params = search_results['best_config']\n",
        "\n",
        "    print(\"\\nBest Configuration Selected:\")\n",
        "    print(f\"  Learning Rate:  {best_params.get('learning_rate', 2e-5)}\")\n",
        "    print(f\"  LoRA Rank (r):  {best_params.get('lora_r', 8)}\")\n",
        "    print(f\"  LoRA Alpha:     {best_params.get('lora_alpha', 32)}\")\n",
        "    print(f\"  LoRA Dropout:   {best_params.get('lora_dropout', 0.01)}\")\n",
        "    print(f\"  Weight Decay:   {best_params.get('weight_decay', 0.01)}\")\n",
        "    print(f\"  Best Val F1:    {search_results['best_f1']:.4f}\")\n",
        "\n",
        "    return best_params\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "j7tcoxm3vyv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title get best params\n",
        "\n",
        "grid_search_output = run_lora_training_with_grid_search()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz90eGUkZnmX",
        "outputId": "32abf0ee-e4c3-4f63-edc3-e7c71f5508ed",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda | Model: bert-base-uncased\n",
            "Loading Official Datasets (Train / Valid / Test)...\n",
            "Stats: Train=10240, Valid=1284, Test=1267\n",
            "Calculating class weights from Training set...\n",
            "Class Weights: [1.14081996 0.89012517]\n",
            "Starting Grid Search for LoRA Model...\n",
            "\n",
            "======================================================================\n",
            "GRID SEARCH START\n",
            "======================================================================\n",
            "Total configurations to search: 36\n",
            "Epochs per configuration: 2\n",
            "======================================================================\n",
            "\n",
            "[1/36] learning_rate=1e-05 | lora_r=4 | lora_alpha=16 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 148,994 || all params: 109,632,772 || trainable%: 0.1359\n",
            " Val Loss: 0.6903 | Val Acc: 0.5350 | Val F1: 0.5329NEW BEST!\n",
            "[2/36] learning_rate=1e-05 | lora_r=4 | lora_alpha=16 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6973 | Val Acc: 0.4930 | Val F1: 0.4795\n",
            "[3/36] learning_rate=1e-05 | lora_r=4 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6946 | Val Acc: 0.4782 | Val F1: 0.4202\n",
            "[4/36] learning_rate=1e-05 | lora_r=4 | lora_alpha=32 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6947 | Val Acc: 0.4961 | Val F1: 0.4959\n",
            "[5/36] learning_rate=1e-05 | lora_r=8 | lora_alpha=16 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 296,450 || all params: 109,780,228 || trainable%: 0.2700\n",
            " Val Loss: 0.7017 | Val Acc: 0.4540 | Val F1: 0.4524\n",
            "[6/36] learning_rate=1e-05 | lora_r=8 | lora_alpha=16 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6926 | Val Acc: 0.5101 | Val F1: 0.5092\n",
            "[7/36] learning_rate=1e-05 | lora_r=8 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6939 | Val Acc: 0.5000 | Val F1: 0.4743\n",
            "[8/36] learning_rate=1e-05 | lora_r=8 | lora_alpha=32 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6922 | Val Acc: 0.5421 | Val F1: 0.5418NEW BEST!\n",
            "[9/36] learning_rate=1e-05 | lora_r=16 | lora_alpha=16 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 591,362 || all params: 110,075,140 || trainable%: 0.5372\n",
            " Val Loss: 0.6901 | Val Acc: 0.5343 | Val F1: 0.5058\n",
            "[10/36] learning_rate=1e-05 | lora_r=16 | lora_alpha=16 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6946 | Val Acc: 0.5467 | Val F1: 0.5419NEW BEST!\n",
            "[11/36] learning_rate=1e-05 | lora_r=16 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6966 | Val Acc: 0.4922 | Val F1: 0.4780\n",
            "[12/36] learning_rate=1e-05 | lora_r=16 | lora_alpha=32 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6920 | Val Acc: 0.5522 | Val F1: 0.5425NEW BEST!\n",
            "[13/36] learning_rate=2e-05 | lora_r=4 | lora_alpha=16 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 148,994 || all params: 109,632,772 || trainable%: 0.1359\n",
            " Val Loss: 0.6919 | Val Acc: 0.5600 | Val F1: 0.5463NEW BEST!\n",
            "[14/36] learning_rate=2e-05 | lora_r=4 | lora_alpha=16 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6898 | Val Acc: 0.5421 | Val F1: 0.5404\n",
            "[15/36] learning_rate=2e-05 | lora_r=4 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6907 | Val Acc: 0.5584 | Val F1: 0.5524NEW BEST!\n",
            "[16/36] learning_rate=2e-05 | lora_r=4 | lora_alpha=32 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6926 | Val Acc: 0.5483 | Val F1: 0.5264\n",
            "[17/36] learning_rate=2e-05 | lora_r=8 | lora_alpha=16 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 296,450 || all params: 109,780,228 || trainable%: 0.2700\n",
            " Val Loss: 0.6922 | Val Acc: 0.5296 | Val F1: 0.4980\n",
            "[18/36] learning_rate=2e-05 | lora_r=8 | lora_alpha=16 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6948 | Val Acc: 0.4626 | Val F1: 0.4189\n",
            "[19/36] learning_rate=2e-05 | lora_r=8 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6966 | Val Acc: 0.5374 | Val F1: 0.5277\n",
            "[20/36] learning_rate=2e-05 | lora_r=8 | lora_alpha=32 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6940 | Val Acc: 0.5117 | Val F1: 0.4903\n",
            "[21/36] learning_rate=2e-05 | lora_r=16 | lora_alpha=16 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 591,362 || all params: 110,075,140 || trainable%: 0.5372\n",
            " Val Loss: 0.6955 | Val Acc: 0.4938 | Val F1: 0.4936\n",
            "[22/36] learning_rate=2e-05 | lora_r=16 | lora_alpha=16 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6938 | Val Acc: 0.5101 | Val F1: 0.5059\n",
            "[23/36] learning_rate=2e-05 | lora_r=16 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6943 | Val Acc: 0.4782 | Val F1: 0.4751\n",
            "[24/36] learning_rate=2e-05 | lora_r=16 | lora_alpha=32 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6935 | Val Acc: 0.5140 | Val F1: 0.4792\n",
            "[25/36] learning_rate=3e-05 | lora_r=4 | lora_alpha=16 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 148,994 || all params: 109,632,772 || trainable%: 0.1359\n",
            " Val Loss: 0.6915 | Val Acc: 0.5132 | Val F1: 0.5108\n",
            "[26/36] learning_rate=3e-05 | lora_r=4 | lora_alpha=16 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6909 | Val Acc: 0.5055 | Val F1: 0.4858\n",
            "[27/36] learning_rate=3e-05 | lora_r=4 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6919 | Val Acc: 0.5413 | Val F1: 0.4960\n",
            "[28/36] learning_rate=3e-05 | lora_r=4 | lora_alpha=32 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6958 | Val Acc: 0.5187 | Val F1: 0.5167\n",
            "[29/36] learning_rate=3e-05 | lora_r=8 | lora_alpha=16 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 296,450 || all params: 109,780,228 || trainable%: 0.2700\n",
            " Val Loss: 0.6941 | Val Acc: 0.5055 | Val F1: 0.5032\n",
            "[30/36] learning_rate=3e-05 | lora_r=8 | lora_alpha=16 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6904 | Val Acc: 0.5600 | Val F1: 0.5562NEW BEST!\n",
            "[31/36] learning_rate=3e-05 | lora_r=8 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6904 | Val Acc: 0.5600 | Val F1: 0.5597NEW BEST!\n",
            "[32/36] learning_rate=3e-05 | lora_r=8 | lora_alpha=32 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6930 | Val Acc: 0.5179 | Val F1: 0.5100\n",
            "[33/36] learning_rate=3e-05 | lora_r=16 | lora_alpha=16 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 591,362 || all params: 110,075,140 || trainable%: 0.5372\n",
            " Val Loss: 0.6956 | Val Acc: 0.4540 | Val F1: 0.4512\n",
            "[34/36] learning_rate=3e-05 | lora_r=16 | lora_alpha=16 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6922 | Val Acc: 0.5358 | Val F1: 0.5313\n",
            "[35/36] learning_rate=3e-05 | lora_r=16 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6922 | Val Acc: 0.5148 | Val F1: 0.5115\n",
            "[36/36] learning_rate=3e-05 | lora_r=16 | lora_alpha=32 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6923 | Val Acc: 0.5506 | Val F1: 0.5391\n",
            "\n",
            "\n",
            "GRID SEARCH COMPLETE\n",
            "\n",
            "Top 5 Configurations (sorted by F1 score):\n",
            "1. F1: 0.5597 | Acc: 0.5600 | Loss: 0.6904\n",
            "   learning_rate=3e-05 | lora_r=8 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n",
            "2. F1: 0.5562 | Acc: 0.5600 | Loss: 0.6904\n",
            "   learning_rate=3e-05 | lora_r=8 | lora_alpha=16 | lora_dropout=0.1 | weight_decay=0.01\n",
            "3. F1: 0.5524 | Acc: 0.5584 | Loss: 0.6907\n",
            "   learning_rate=2e-05 | lora_r=4 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n",
            "4. F1: 0.5463 | Acc: 0.5600 | Loss: 0.6919\n",
            "   learning_rate=2e-05 | lora_r=4 | lora_alpha=16 | lora_dropout=0.0 | weight_decay=0.01\n",
            "5. F1: 0.5425 | Acc: 0.5522 | Loss: 0.6920\n",
            "   learning_rate=1e-05 | lora_r=16 | lora_alpha=32 | lora_dropout=0.1 | weight_decay=0.01\n",
            "\n",
            "\n",
            "BEST CONFIGURATION:\n",
            "Best Validation F1: 0.5597\n",
            "Parameters: learning_rate=3e-05 | lora_r=8 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n",
            "\n",
            "\n",
            "Training final model with best LoRA configuration...\n",
            "\n",
            "Best Configuration Selected:\n",
            "  Learning Rate:  3e-05\n",
            "  LoRA Rank (r):  8\n",
            "  LoRA Alpha:     32\n",
            "  LoRA Dropout:   0.0\n",
            "  Weight Decay:   0.01\n",
            "  Best Val F1:    0.5597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title model with lora\n",
        "\n",
        "#Configuration\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 128    # A100 has large VRAM\n",
        "LR = 2e-5\n",
        "EPOCHS = 10\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "NUM_WORKERS = 8\n",
        "\n",
        "#Modification Point 1: Define official dataset paths\n",
        "TRAIN_PATH = '/kaggle/input/liar-dataset/train.tsv'\n",
        "VALID_PATH = '/kaggle/input/liar-dataset/valid.tsv'\n",
        "TEST_PATH  = '/kaggle/input/liar-dataset/test.tsv'\n",
        "\n",
        "#1. Dataset Class (unchanged)\n",
        "class TextualizedLIARDataset(Dataset):\n",
        "    def __init__(self, tsv_path, tokenizer, max_len=128):\n",
        "        self.df = pd.read_csv(tsv_path, sep='\\t', header=None, names=[\n",
        "            \"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job\",\n",
        "            \"state\", \"party\", \"barely_true_counts\", \"false_counts\",\n",
        "            \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\",\n",
        "            \"context\"\n",
        "        ])\n",
        "\n",
        "        self.df.dropna(subset=['statement'], inplace=True)\n",
        "\n",
        "        # Label logic: False / Pants-fire / Barely-true = 0 (Fake)\n",
        "        self.label_map = {\n",
        "            \"pants-fire\": 0, \"false\": 0, \"barely-true\": 0,\n",
        "            \"half-true\": 1, \"mostly-true\": 1, \"true\": 1\n",
        "        }\n",
        "\n",
        "        self.df['label'] = self.df['label'].map(self.label_map)\n",
        "        self.df.dropna(subset=['label'], inplace=True)\n",
        "        self.df['label'] = self.df['label'].astype(int)\n",
        "\n",
        "        text_cols = ['statement', 'subject', 'speaker', 'party', 'state', 'speaker_job', 'context']\n",
        "        for col in text_cols:\n",
        "            self.df[col] = self.df[col].fillna(\"Unknown\")\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        metadata_str = (\n",
        "            f\"Speaker: {row['speaker']} | \"\n",
        "            f\"Job: {row['speaker_job']} | \"\n",
        "            f\"Party: {row['party']} | \"\n",
        "            f\"State: {row['state']} | \"\n",
        "            f\"Context: {row['context']} | \"\n",
        "            f\"Subject: {row['subject']}\"\n",
        "        )\n",
        "        final_text = f\"{metadata_str} [SEP] Statement: {row['statement']}\"\n",
        "\n",
        "        encoded = self.tokenizer(\n",
        "            final_text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(row['label'], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "#2. Training and Evaluation Functions\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "    reset_peak_memory()\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
        "        labels = batch['label'].to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', dtype=dtype):\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_time = end_time - start_time\n",
        "\n",
        "    peak_memory = get_peak_gpu_memory_mb()\n",
        "\n",
        "    return total_loss / len(dataloader), epoch_time, peak_memory\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    reset_peak_memory()\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    eval_time = time.time() - start_time\n",
        "    peak_memory = get_peak_gpu_memory_mb()\n",
        "\n",
        "    return total_loss / len(dataloader), accuracy_score(all_labels, all_preds), all_labels, all_preds, eval_time, peak_memory\n",
        "\n",
        "#Main training\n",
        "def run_official_split_training():\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    print(f\"Using device: {DEVICE} | Model: {MODEL_NAME}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    print(\"Loading Official Datasets (Train / Valid / Test)...\")\n",
        "    if not os.path.exists(TRAIN_PATH):\n",
        "        print(f\"Error: Path {TRAIN_PATH} not found.\")\n",
        "        return\n",
        "\n",
        "    best_params = grid_search_output\n",
        "\n",
        "    train_dataset = TextualizedLIARDataset(TRAIN_PATH, tokenizer, max_len=MAX_LEN)\n",
        "    valid_dataset = TextualizedLIARDataset(VALID_PATH, tokenizer, max_len=MAX_LEN)\n",
        "    test_dataset  = TextualizedLIARDataset(TEST_PATH, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    valid_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(f\"Stats: Train={len(train_dataset)}, Valid={len(valid_dataset)}, Test={len(test_dataset)}\")\n",
        "\n",
        "    print(\"Calculating class weights from Training set...\")\n",
        "    train_labels = train_dataset.df['label'].values\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(train_labels),\n",
        "        y=train_labels\n",
        "    )\n",
        "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
        "    print(f\"Class Weights: {class_weights}\")\n",
        "    # LoRA\n",
        "    print(\"Loading base model with LoRA...\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        r=best_params.get('lora_r', 8),\n",
        "        lora_alpha=best_params.get('lora_alpha', 32),\n",
        "        lora_dropout=best_params.get('lora_dropout', 0.01),\n",
        "        target_modules=[\"query\", \"value\"],\n",
        "        bias=\"none\",\n",
        "        inference_mode=False\n",
        "    )\n",
        "\n",
        "\n",
        "    print(\"Final LoRA Configuration\")\n",
        "    print(f\"  LoRA Rank (r):        {lora_config.r}\")\n",
        "    print(f\"  LoRA Alpha:           {lora_config.lora_alpha}\")\n",
        "    print(f\"  LoRA Dropout:         {lora_config.lora_dropout}\")\n",
        "    print(f\"  Target Modules:       {lora_config.target_modules}\")\n",
        "    print(f\"  Bias:                 {lora_config.bias}\")\n",
        "    print(f\"  Inference Mode:       {lora_config.inference_mode}\")\n",
        "    print(f\"  Task Type:            {lora_config.task_type}\")\n",
        "\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=best_params.get('learning_rate', 2e-5),\n",
        "        weight_decay=best_params.get('weight_decay', 0.01)\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Optimizer Configuration\")\n",
        "    print(f\"  Learning Rate:        {optimizer.get('learning_rate', 2e-5)}\")\n",
        "    print(f\"  Weight Decay:         {optimizer.get('weight_decay', 0.01)}\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "    total_train_time = 0\n",
        "    epoch_times = []\n",
        "    epoch_memories = []\n",
        "\n",
        "\n",
        "    total_steps = len(train_loader) * EPOCHS\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)\n",
        "\n",
        "    best_val_f1 = 0\n",
        "\n",
        "    print(\"\\nStarting Training on Official Split...\")\n",
        "\n",
        "    profiler_data = {\n",
        "    'cpu_time': [],\n",
        "    'cuda_time': [],\n",
        "    'memory': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "      if epoch == 2:\n",
        "        print(\"Profiling enabled for epoch 2 ...\")\n",
        "\n",
        "        with profile(\n",
        "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "            record_shapes=True,\n",
        "            profile_memory=True,\n",
        "            with_stack=False\n",
        "        ) as prof:\n",
        "            train_loss, train_time, train_memory = train_epoch_with_profiler(\n",
        "                model, train_loader, optimizer, scheduler, criterion, DEVICE, prof\n",
        "            )\n",
        "\n",
        "\n",
        "        print(\"PROFILER SUMMARY (Epoch 2)\")\n",
        "\n",
        "\n",
        "        print(\"\\nTop Operations by CPU Time:\")\n",
        "        print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=15))\n",
        "\n",
        "        print(\"\\nTop Operations by CUDA Time:\")\n",
        "        try:\n",
        "            print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=15))\n",
        "        except:\n",
        "            print(prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=15))\n",
        "\n",
        "        print(\"\\nTop Operations by Memory:\")\n",
        "        print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=15))\n",
        "\n",
        "        trace_file = \"profiler_trace_lora_epoch1.json\"\n",
        "        prof.export_chrome_trace(trace_file)\n",
        "        print(f\"\\n Chrome trace saved to: {trace_file}\")\n",
        "        print(\"  Download and open in chrome://tracing\")\n",
        "      else:\n",
        "        train_loss, train_time, train_memory = train_epoch(\n",
        "          model, train_loader, optimizer, scheduler, criterion, DEVICE\n",
        "        )\n",
        "\n",
        "      val_loss, val_acc, val_labels, val_preds, val_time, val_memory = evaluate(\n",
        "          model, valid_loader, criterion, DEVICE\n",
        "      )\n",
        "\n",
        "      total_train_time += train_time\n",
        "      epoch_times.append(train_time)\n",
        "      epoch_memories.append(train_memory)\n",
        "\n",
        "      report_dict = classification_report(val_labels, val_preds, output_dict=True)\n",
        "      macro_f1 = report_dict['macro avg']['f1-score']\n",
        "      fake_recall = report_dict['0']['recall']\n",
        "\n",
        "      print(f\"Epoch {epoch}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Macro-F1: {macro_f1:.4f} | Fake Recall: {fake_recall:.4f}|  T_Time: {train_time:.1f}s | T_Mem: {train_memory:.0f}MB | V_Time: {val_time:.2f}s | V_Memory: {val_memory:.1f} MB\")\n",
        "\n",
        "      if macro_f1 > best_val_f1:\n",
        "          best_val_f1 = macro_f1\n",
        "          torch.save(model.state_dict(), 'best_lora_model.pth')\n",
        "          print(\" -> Best model updated!\")\n",
        "\n",
        "    print(f\"Total Training Time: {total_train_time:.1f}s\")\n",
        "    print(f\"Avg Epoch Time: {sum(epoch_times)/len(epoch_times):.1f}s\")\n",
        "    print(f\"Avg Peak Memory: {sum(epoch_memories)/len(epoch_memories):.0f}MB\")\n",
        "\n",
        "    # automatically run best model on Official Test Set after training\n",
        "    print(\"\\n========== FINAL TEST RESULT (Official Test Set) ==========\")\n",
        "    model.load_state_dict(torch.load('best_lora_model.pth'))\n",
        "    test_loss, test_acc, test_labels, test_preds, test_time, test_memory = evaluate(model, test_loader, criterion, DEVICE)\n",
        "    print(classification_report(test_labels, test_preds, target_names=['Fake (0)', 'True (1)']))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    run_official_split_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GUlXK5x5M-f8",
        "outputId": "9c296c69-1242-46c4-cdcb-25d702b1a50e",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda | Model: bert-base-uncased\n",
            "Loading Official Datasets (Train / Valid / Test)...\n",
            "Stats: Train=10240, Valid=1284, Test=1267\n",
            "Calculating class weights from Training set...\n",
            "Class Weights: [1.14081996 0.89012517]\n",
            "Loading base model with LoRA...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final LoRA Configuration\n",
            "  LoRA Rank (r):        8\n",
            "  LoRA Alpha:           32\n",
            "  LoRA Dropout:         0.0\n",
            "  Target Modules:       {'query', 'value'}\n",
            "  Bias:                 none\n",
            "  Inference Mode:       False\n",
            "  Task Type:            TaskType.SEQ_CLS\n",
            "trainable params: 296,450 || all params: 109,780,228 || trainable%: 0.2700\n",
            "Optimizer Configuration\n",
            "  Learning Rate:        3e-05\n",
            "  Weight Decay:         0.01\n",
            "\n",
            "Starting Training on Official Split...\n",
            "Epoch 1/10 | Train Loss: 0.6981 | Val Loss: 0.6901 | Acc: 0.5467 | Macro-F1: 0.5430 | Fake Recall: 0.4756|  T_Time: 13.3s | T_Mem: 13532MB | V_Time: 4.71s | V_Memory: 4909.0 MB\n",
            " -> Best model updated!\n",
            "Profiling enabled for epoch 2 ...\n",
            "PROFILER SUMMARY (Epoch 2)\n",
            "\n",
            "Top Operations by CPU Time:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                             aten::item         0.15%      16.499ms        31.17%        3.471s     425.268us       0.000us         0.00%     340.584us       0.042us           0 B           0 B           0 B           0 B          8162  \n",
            "                              aten::_local_scalar_dense         0.10%      10.879ms        31.03%        3.455s     423.247us     340.584us         0.00%     340.584us       0.042us           0 B           0 B           0 B           0 B          8162  \n",
            "                                  cudaStreamSynchronize        30.87%        3.437s        30.88%        3.438s      21.487ms       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B           160  \n",
            "                                           aten::linear         2.04%     226.816ms        27.52%        3.064s     156.988us       0.000us         0.00%        4.741s     242.899us           0 B           0 B    1096.38 GB    -180.38 GB         19520  \n",
            "                                               aten::to         0.94%     104.574ms        13.31%        1.482s      36.403us       0.000us         0.00%        1.516s      37.231us           0 B           0 B     895.55 GB           0 B         40720  \n",
            "                                         aten::_to_copy         2.84%     316.590ms        12.37%        1.378s      40.051us       0.000us         0.00%        1.516s      44.072us           0 B           0 B     895.55 GB     -48.00 MB         34400  \n",
            "                                       cudaLaunchKernel         8.49%     945.700ms         8.64%     962.547ms      11.172us       0.000us         0.00%     769.052us       0.009us           0 B           0 B           0 B           0 B         86160  \n",
            "                                               aten::mm         5.89%     656.088ms         8.16%     908.361ms      53.058us        2.320s        19.70%        2.320s     135.511us           0 B           0 B     568.20 GB     568.20 GB         17120  \n",
            "                                            aten::copy_         3.09%     343.941ms         6.59%     733.246ms      21.168us        1.520s        12.91%        1.520s      43.876us           0 B           0 B           0 B           0 B         34640  \n",
            "autograd::engine::evaluate_function: ToCopyBackward0...         1.23%     137.233ms         6.37%     709.480ms      74.525us       0.000us         0.00%        1.666s     175.022us           0 B           0 B    -251.19 GB    -753.81 GB          9520  \n",
            "       autograd::engine::evaluate_function: MmBackward0         0.72%      79.766ms         5.99%     666.855ms     173.660us       0.000us         0.00%     342.531ms      89.201us           0 B           0 B     -98.44 GB    -181.92 GB          3840  \n",
            "                                            MmBackward0         0.58%      64.537ms         5.27%     587.090ms     152.888us       0.000us         0.00%     342.531ms      89.201us           0 B           0 B      83.48 GB           0 B          3840  \n",
            "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...         4.66%     518.807ms         4.69%     521.744ms       6.441ms       0.000us         0.00%       0.000us       0.000us           0 B     -80.00 KB           0 B           0 B            81  \n",
            "    autograd::engine::evaluate_function: AddmmBackward0         0.72%      80.430ms         4.52%     503.021ms      88.560us       0.000us         0.00%        1.793s     315.704us           0 B           0 B     -14.43 GB    -408.21 GB          5680  \n",
            "                                        ToCopyBackward0         0.32%      35.415ms         4.20%     467.937ms      49.153us       0.000us         0.00%     738.844ms      77.610us           0 B           0 B     502.62 GB           0 B          9520  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 11.135s\n",
            "Self CUDA time total: 11.774s\n",
            "\n",
            "\n",
            "Top Operations by CUDA Time:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::linear         2.04%     226.816ms        27.52%        3.064s     156.988us       0.000us         0.00%        4.741s     242.899us           0 B           0 B    1096.38 GB    -180.38 GB         19520  \n",
            "                                               aten::mm         5.89%     656.088ms         8.16%     908.361ms      53.058us        2.320s        19.70%        2.320s     135.511us           0 B           0 B     568.20 GB     568.20 GB         17120  \n",
            "autograd::engine::evaluate_function: ScaledDotProduc...         0.31%      34.322ms         2.19%     243.527ms     253.674us       0.000us         0.00%        1.989s       2.071ms     -15.00 KB     -15.00 KB    -110.41 GB    -241.66 GB           960  \n",
            "            ScaledDotProductEfficientAttentionBackward0         0.14%      15.717ms         1.88%     209.205ms     217.922us       0.000us         0.00%        1.989s       2.071ms           0 B           0 B     131.25 GB      -3.75 GB           960  \n",
            "aten::_scaled_dot_product_efficient_attention_backwa...         0.28%      30.795ms         1.74%     193.487ms     201.549us       0.000us         0.00%        1.989s       2.071ms           0 B           0 B     135.00 GB           0 B           960  \n",
            "                    aten::_efficient_attention_backward         0.34%      37.454ms         1.09%     121.063ms     126.107us        1.939s        16.47%        1.989s       2.071ms           0 B           0 B     135.00 GB     -92.37 GB           960  \n",
            "fmha_cutlassB_bf16_aligned_64x64_k64_dropout_sm80(Py...         0.00%       0.000us         0.00%       0.000us       0.000us        1.939s        16.47%        1.939s       2.020ms           0 B           0 B           0 B           0 B           960  \n",
            "                                            aten::addmm         2.83%     314.717ms         3.62%     403.323ms      68.129us        1.900s        16.14%        1.901s     321.152us           0 B           0 B     405.01 GB     405.01 GB          5920  \n",
            "    autograd::engine::evaluate_function: AddmmBackward0         0.72%      80.430ms         4.52%     503.021ms      88.560us       0.000us         0.00%        1.793s     315.704us           0 B           0 B     -14.43 GB    -408.21 GB          5680  \n",
            "                                         AddmmBackward0         0.45%      49.561ms         3.76%     418.274ms      73.640us       0.000us         0.00%        1.793s     315.592us           0 B           0 B     393.78 GB           0 B          5680  \n",
            "autograd::engine::evaluate_function: ToCopyBackward0...         1.23%     137.233ms         6.37%     709.480ms      74.525us       0.000us         0.00%        1.666s     175.022us           0 B           0 B    -251.19 GB    -753.81 GB          9520  \n",
            "                                            aten::copy_         3.09%     343.941ms         6.59%     733.246ms      21.168us        1.520s        12.91%        1.520s      43.876us           0 B           0 B           0 B           0 B         34640  \n",
            "                                               aten::to         0.94%     104.574ms        13.31%        1.482s      36.403us       0.000us         0.00%        1.516s      37.231us           0 B           0 B     895.55 GB           0 B         40720  \n",
            "                                         aten::_to_copy         2.84%     316.590ms        12.37%        1.378s      40.051us       0.000us         0.00%        1.516s      44.072us           0 B           0 B     895.55 GB     -48.00 MB         34400  \n",
            "                     aten::scaled_dot_product_attention         0.23%      25.523ms         3.23%     359.412ms     187.194us       0.000us         0.00%        1.507s     784.933us      30.00 KB           0 B     108.31 GB           0 B          1920  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 11.135s\n",
            "Self CUDA time total: 11.774s\n",
            "\n",
            "\n",
            "Top Operations by Memory:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                    aten::empty_strided         3.64%     404.791ms         3.70%     412.225ms       9.668us       0.000us         0.00%       0.000us       0.000us           0 B           0 B    1040.04 GB    1040.04 GB         42640  \n",
            "                                            aten::empty         1.31%     146.036ms         1.31%     146.036ms       8.410us       0.000us         0.00%       0.000us       0.000us     160.64 KB     160.64 KB     742.05 GB     742.05 GB         17364  \n",
            "                                               aten::mm         5.89%     656.088ms         8.16%     908.361ms      53.058us        2.320s        19.70%        2.320s     135.511us           0 B           0 B     568.20 GB     568.20 GB         17120  \n",
            "                                            aten::addmm         2.83%     314.717ms         3.62%     403.323ms      68.129us        1.900s        16.14%        1.901s     321.152us           0 B           0 B     405.01 GB     405.01 GB          5920  \n",
            "                                              aten::add         0.76%      84.274ms         1.13%     126.180ms      31.545us     540.470ms         4.59%     540.470ms     135.117us           0 B           0 B     277.58 GB     277.58 GB          4000  \n",
            "                                              aten::mul         0.85%      94.637ms         1.28%     142.356ms      36.315us     229.484ms         1.95%     229.484ms      58.542us           0 B           0 B     180.00 GB     180.00 GB          3920  \n",
            "                                             aten::gelu         0.21%      23.716ms         0.31%      35.028ms      36.487us     222.879ms         1.89%     222.879ms     232.165us           0 B           0 B     180.00 GB     180.00 GB           960  \n",
            "                                    aten::gelu_backward         0.17%      19.247ms         0.28%      30.810ms      32.093us     352.697ms         3.00%     352.697ms     367.393us           0 B           0 B     180.00 GB     180.00 GB           960  \n",
            "                                          aten::resize_         0.02%       2.498ms         0.02%       2.498ms       7.710us       0.000us         0.00%       0.000us       0.000us           0 B           0 B      15.06 GB      15.06 GB           324  \n",
            "                                              aten::sub         0.02%       2.372ms         0.03%       3.541ms      44.264us       3.162ms         0.03%       3.162ms      39.520us           0 B           0 B       2.50 GB       2.50 GB            80  \n",
            "                                           Buffer Flush         0.04%       4.904ms         0.05%       5.215ms     325.916us     637.211us         0.01%     637.211us      39.826us           0 B           0 B     484.38 MB     436.01 MB            16  \n",
            "                               cudaPointerGetAttributes         0.01%     915.714us         0.01%     917.224us       3.822us       0.000us         0.00%       0.000us       0.000us           0 B           0 B      48.00 MB      48.00 MB           240  \n",
            "                                             aten::tanh         0.02%       2.249ms         0.03%       3.233ms      40.408us     247.666us         0.00%     247.666us       3.096us           0 B           0 B      15.00 MB      15.00 MB            80  \n",
            "                                    aten::tanh_backward         0.01%       1.658ms         0.02%       2.678ms      33.480us     184.213us         0.00%     184.213us       2.303us           0 B           0 B      15.00 MB      15.00 MB            80  \n",
            "                                               aten::eq         0.03%       3.221ms         0.04%       4.458ms      55.720us     241.149us         0.00%     241.149us       3.014us           0 B           0 B       2.50 MB       2.50 MB            80  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 11.135s\n",
            "Self CUDA time total: 11.774s\n",
            "\n",
            "\n",
            " Chrome trace saved to: profiler_trace_lora_epoch1.json\n",
            "  Download and open in chrome://tracing\n",
            "Epoch 2/10 | Train Loss: 0.6940 | Val Loss: 0.6899 | Acc: 0.5802 | Macro-F1: 0.5438 | Fake Recall: 0.3101|  T_Time: 13.5s | T_Mem: 13532MB | V_Time: 5.13s | V_Memory: 2379.5 MB\n",
            " -> Best model updated!\n",
            "Epoch 3/10 | Train Loss: 0.6890 | Val Loss: 0.6849 | Acc: 0.5732 | Macro-F1: 0.5668 | Fake Recall: 0.4708|  T_Time: 13.6s | T_Mem: 10984MB | V_Time: 4.97s | V_Memory: 2379.5 MB\n",
            " -> Best model updated!\n",
            "Epoch 4/10 | Train Loss: 0.6835 | Val Loss: 0.6873 | Acc: 0.5872 | Macro-F1: 0.5650 | Fake Recall: 0.3766|  T_Time: 13.6s | T_Mem: 10984MB | V_Time: 5.00s | V_Memory: 2379.5 MB\n",
            "Epoch 5/10 | Train Loss: 0.6770 | Val Loss: 0.6793 | Acc: 0.5826 | Macro-F1: 0.5789 | Fake Recall: 0.5097|  T_Time: 13.5s | T_Mem: 10984MB | V_Time: 4.94s | V_Memory: 2379.5 MB\n",
            " -> Best model updated!\n",
            "Epoch 6/10 | Train Loss: 0.6715 | Val Loss: 0.6793 | Acc: 0.5927 | Macro-F1: 0.5871 | Fake Recall: 0.4968|  T_Time: 13.5s | T_Mem: 10984MB | V_Time: 5.00s | V_Memory: 2379.5 MB\n",
            " -> Best model updated!\n",
            "Epoch 7/10 | Train Loss: 0.6673 | Val Loss: 0.6741 | Acc: 0.5966 | Macro-F1: 0.5936 | Fake Recall: 0.5325|  T_Time: 13.6s | T_Mem: 10984MB | V_Time: 5.01s | V_Memory: 2379.5 MB\n",
            " -> Best model updated!\n",
            "Epoch 8/10 | Train Loss: 0.6669 | Val Loss: 0.6743 | Acc: 0.6005 | Macro-F1: 0.5966 | Fake Recall: 0.5244|  T_Time: 13.5s | T_Mem: 10984MB | V_Time: 5.02s | V_Memory: 2379.5 MB\n",
            " -> Best model updated!\n",
            "Epoch 9/10 | Train Loss: 0.6635 | Val Loss: 0.6723 | Acc: 0.5989 | Macro-F1: 0.5963 | Fake Recall: 0.5406|  T_Time: 13.6s | T_Mem: 10984MB | V_Time: 4.93s | V_Memory: 2379.5 MB\n",
            "Epoch 10/10 | Train Loss: 0.6640 | Val Loss: 0.6718 | Acc: 0.5958 | Macro-F1: 0.5934 | Fake Recall: 0.5406|  T_Time: 13.6s | T_Mem: 10984MB | V_Time: 5.00s | V_Memory: 2379.5 MB\n",
            "Total Training Time: 135.3s\n",
            "Avg Epoch Time: 13.5s\n",
            "Avg Peak Memory: 11494MB\n",
            "\n",
            "========== FINAL TEST RESULT (Official Test Set) ==========\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Fake (0)       0.56      0.52      0.54       553\n",
            "    True (1)       0.65      0.68      0.67       714\n",
            "\n",
            "    accuracy                           0.61      1267\n",
            "   macro avg       0.60      0.60      0.60      1267\n",
            "weighted avg       0.61      0.61      0.61      1267\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title rebuild for lora + amp\n",
        "\n",
        "# 1. Re-prepare environment and data\n",
        "\n",
        "# Redefine configuration\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "BATCH_SIZE = 128\n",
        "MAX_LEN = 256\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "VALID_PATH = '/kaggle/input/liar-dataset/valid.tsv'\n",
        "TEST_PATH =  '/kaggle/input/liar-dataset/test.tsv'\n",
        "\n",
        "print(\"Re-loading Tokenizer and Dataloaders...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Re-instantiate datasets (ensure TextualizedLIARDataset class has already been run above)\n",
        "valid_dataset = TextualizedLIARDataset(VALID_PATH, tokenizer, max_len=MAX_LEN)\n",
        "test_dataset  = TextualizedLIARDataset(TEST_PATH,  tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "# Re-instantiate DataLoader\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# 2. Define function for searching best threshold\n",
        "\n",
        "def find_optimal_threshold(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(\"Running inference on Validation Set...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            # Softmax to obtain probabilities\n",
        "            probs = F.softmax(outputs.logits, dim=1)\n",
        "            # Extract probability of label 1 (True)\n",
        "            true_probs = probs[:, 1].cpu().numpy()\n",
        "\n",
        "            all_probs.extend(true_probs)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_probs = np.array(all_probs)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Iterate to find best F1 score\n",
        "    best_threshold = 0.5\n",
        "    best_f1 = 0\n",
        "\n",
        "    thresholds = np.arange(0.1, 0.95, 0.05)\n",
        "\n",
        "    print(f\"\\n{'Threshold':<10} | {'Macro F1':<10} | {'Fake Recall':<12} | {'True Recall':<12}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for thresh in thresholds:\n",
        "        preds = (all_probs > thresh).astype(int)\n",
        "\n",
        "        report = classification_report(all_labels, preds, output_dict=True)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        fake_recall = report['0']['recall']\n",
        "        true_recall = report['1']['recall']\n",
        "\n",
        "        print(f\"{thresh:.2f}       | {macro_f1:.4f}     | {fake_recall:.4f}       | {true_recall:.4f}\")\n",
        "\n",
        "        if macro_f1 > best_f1:\n",
        "            best_f1 = macro_f1\n",
        "            best_threshold = thresh\n",
        "\n",
        "    print(f\"\\nBest Threshold found: {best_threshold:.2f}\")\n",
        "    return best_threshold\n",
        "\n",
        "# 3. Run optimization\n",
        "\n",
        "# Load model\n",
        "print(\"\\nLoading model weights from 'best_lora_model.pth'...\")\n",
        "# Must reinitialize model structure before loading weights\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "best_params = grid_search_output\n",
        "lora_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        r=best_params.get('lora_r', 8),\n",
        "        lora_alpha=best_params.get('lora_alpha', 32),\n",
        "        lora_dropout=best_params.get('lora_dropout', 0.01),\n",
        "        target_modules=[\"query\", \"value\"],\n",
        "        bias=\"none\",\n",
        "        inference_mode=False\n",
        "    )\n",
        "\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "# Load fine-tuned weights\n",
        "model.load_state_dict(torch.load('best_lora_model.pth'))\n",
        "\n",
        "model.to(DEVICE)\n",
        "\n",
        "best_thresh = find_optimal_threshold(model, valid_loader, DEVICE)\n",
        "\n",
        "# 2. Apply to Test Set\n",
        "print(f\"\\nApplying Threshold {best_thresh:.2f} to Test Set...\")\n",
        "model.eval()\n",
        "test_probs = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(DEVICE)\n",
        "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "        labels = batch['label'].to(DEVICE)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        probs = F.softmax(outputs.logits, dim=1)\n",
        "        test_probs.extend(probs[:, 1].cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_probs = np.array(test_probs)\n",
        "final_preds = (test_probs > best_thresh).astype(int)\n",
        "\n",
        "print(\"\\n========== OPTIMIZED TEST RESULT ==========\")\n",
        "print(classification_report(test_labels, final_preds, target_names=['Fake (0)', 'True (1)']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLE8tR9IAS4_",
        "outputId": "504e0c4c-2e54-4004-94c0-303d949ec0f4",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Re-loading Tokenizer and Dataloaders...\n",
            "\n",
            "Loading model weights from 'best_lora_model.pth'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running inference on Validation Set...\n",
            "\n",
            "Threshold  | Macro F1   | Fake Recall  | True Recall \n",
            "------------------------------------------------------------\n",
            "0.10       | 0.3422     | 0.0000       | 1.0000\n",
            "0.15       | 0.3422     | 0.0000       | 1.0000\n",
            "0.20       | 0.3422     | 0.0000       | 1.0000\n",
            "0.25       | 0.3422     | 0.0000       | 1.0000\n",
            "0.30       | 0.3458     | 0.0032       | 1.0000\n",
            "0.35       | 0.4148     | 0.0731       | 0.9880\n",
            "0.40       | 0.5186     | 0.2289       | 0.9072\n",
            "0.45       | 0.5752     | 0.3847       | 0.7934\n",
            "0.50       | 0.5966     | 0.5244       | 0.6707\n",
            "0.55       | 0.5832     | 0.6266       | 0.5434\n",
            "0.60       | 0.5725     | 0.7451       | 0.4281\n",
            "0.65       | 0.5535     | 0.8506       | 0.3278\n",
            "0.70       | 0.4703     | 0.9221       | 0.1781\n",
            "0.75       | 0.3718     | 0.9919       | 0.0464\n",
            "0.80       | 0.3259     | 1.0000       | 0.0015\n",
            "0.85       | 0.3242     | 1.0000       | 0.0000\n",
            "0.90       | 0.3242     | 1.0000       | 0.0000\n",
            "\n",
            "Best Threshold found: 0.50\n",
            "\n",
            "Applying Threshold 0.50 to Test Set...\n",
            "\n",
            "========== OPTIMIZED TEST RESULT ==========\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Fake (0)       0.56      0.52      0.54       553\n",
            "    True (1)       0.65      0.68      0.67       714\n",
            "\n",
            "    accuracy                           0.61      1267\n",
            "   macro avg       0.60      0.60      0.60      1267\n",
            "weighted avg       0.61      0.61      0.61      1267\n",
            "\n"
          ]
        }
      ]
    }
  ]
}