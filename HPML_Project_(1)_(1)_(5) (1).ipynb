{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def get_gpu_memory_mb():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.memory_allocated() / 1024**2\n",
        "    return 0\n",
        "\n",
        "def get_peak_gpu_memory_mb():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.max_memory_allocated() / 1024**2\n",
        "    return 0\n",
        "\n",
        "def reset_peak_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.reset_peak_memory_stats()"
      ],
      "metadata": {
        "id": "3KpkE94sTxmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "\n",
        "# Set cache directory (just the directory, not including specific files)\n",
        "# Don't include the filename or 'versions/1/train.tsv' in the path\n",
        "os.environ['KAGGLE_HUB_CACHE'] = '/root/.cache/kagglehub'\n",
        "\n",
        "# Download the dataset\n",
        "path = kagglehub.dataset_download(\"doanquanvietnamca/liar-dataset\")\n",
        "\n",
        "# CORRECT way to join paths - don't start with '/'\n",
        "train_path = os.path.join(path, 'train.tsv')\n",
        "valid_path = os.path.join(path, 'valid.tsv')\n",
        "test_path = os.path.join(path, 'test.tsv')\n",
        "\n",
        "print(\"Path to dataset directory:\", path)\n",
        "print(\"Path to train.tsv file:\", train_path)\n",
        "print(\"Path to valid.tsv file:\", valid_path)\n",
        "print(\"Path to test.tsv file:\", test_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgowySh4JPQU",
        "outputId": "cbdd3f99-c26d-4ee6-f0cc-3ff4f04b2de9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'liar-dataset' dataset.\n",
            "Path to dataset directory: /kaggle/input/liar-dataset\n",
            "Path to train.tsv file: /kaggle/input/liar-dataset/train.tsv\n",
            "Path to valid.tsv file: /kaggle/input/liar-dataset/valid.tsv\n",
            "Path to test.tsv file: /kaggle/input/liar-dataset/test.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#tsv_path = '/kaggle/input/liar-dataset/train.tsv'\n",
        "df = pd.read_csv(train_path, sep='\\t', header=None, names=[\n",
        "            \"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job\",\n",
        "            \"state\", \"party\", \"barely_true_counts\", \"false_counts\",\n",
        "            \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\",\n",
        "            \"context\"\n",
        "        ])"
      ],
      "metadata": {
        "id": "R4iSPyXDKMxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "VfoM9u2HK8lj",
        "outputId": "366c94ef-08c1-4c0a-928d-2684f6152d2e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           id        label                                          statement  \\\n",
              "0   2635.json        false  Says the Annies List political group supports ...   \n",
              "1  10540.json    half-true  When did the decline of coal start? It started...   \n",
              "2    324.json  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
              "3   1123.json        false  Health care reform legislation is likely to ma...   \n",
              "4   9028.json    half-true  The economic turnaround started at the end of ...   \n",
              "\n",
              "                              subject         speaker           speaker_job  \\\n",
              "0                            abortion    dwayne-bohac  State representative   \n",
              "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
              "2                      foreign-policy    barack-obama             President   \n",
              "3                         health-care    blog-posting                   NaN   \n",
              "4                        economy,jobs   charlie-crist                   NaN   \n",
              "\n",
              "      state       party  barely_true_counts  false_counts  half_true_counts  \\\n",
              "0     Texas  republican                 0.0           1.0               0.0   \n",
              "1  Virginia    democrat                 0.0           0.0               1.0   \n",
              "2  Illinois    democrat                70.0          71.0             160.0   \n",
              "3       NaN        none                 7.0          19.0               3.0   \n",
              "4   Florida    democrat                15.0           9.0              20.0   \n",
              "\n",
              "   mostly_true_counts  pants_on_fire_counts              context  \n",
              "0                 0.0                   0.0             a mailer  \n",
              "1                 1.0                   0.0      a floor speech.  \n",
              "2               163.0                   9.0               Denver  \n",
              "3                 5.0                  44.0       a news release  \n",
              "4                19.0                   2.0  an interview on CNN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-630b3826-9a54-48bd-95e4-3a2b793563e6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>statement</th>\n",
              "      <th>subject</th>\n",
              "      <th>speaker</th>\n",
              "      <th>speaker_job</th>\n",
              "      <th>state</th>\n",
              "      <th>party</th>\n",
              "      <th>barely_true_counts</th>\n",
              "      <th>false_counts</th>\n",
              "      <th>half_true_counts</th>\n",
              "      <th>mostly_true_counts</th>\n",
              "      <th>pants_on_fire_counts</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2635.json</td>\n",
              "      <td>false</td>\n",
              "      <td>Says the Annies List political group supports ...</td>\n",
              "      <td>abortion</td>\n",
              "      <td>dwayne-bohac</td>\n",
              "      <td>State representative</td>\n",
              "      <td>Texas</td>\n",
              "      <td>republican</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>a mailer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10540.json</td>\n",
              "      <td>half-true</td>\n",
              "      <td>When did the decline of coal start? It started...</td>\n",
              "      <td>energy,history,job-accomplishments</td>\n",
              "      <td>scott-surovell</td>\n",
              "      <td>State delegate</td>\n",
              "      <td>Virginia</td>\n",
              "      <td>democrat</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>a floor speech.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>324.json</td>\n",
              "      <td>mostly-true</td>\n",
              "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
              "      <td>foreign-policy</td>\n",
              "      <td>barack-obama</td>\n",
              "      <td>President</td>\n",
              "      <td>Illinois</td>\n",
              "      <td>democrat</td>\n",
              "      <td>70.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>Denver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1123.json</td>\n",
              "      <td>false</td>\n",
              "      <td>Health care reform legislation is likely to ma...</td>\n",
              "      <td>health-care</td>\n",
              "      <td>blog-posting</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>none</td>\n",
              "      <td>7.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>a news release</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9028.json</td>\n",
              "      <td>half-true</td>\n",
              "      <td>The economic turnaround started at the end of ...</td>\n",
              "      <td>economy,jobs</td>\n",
              "      <td>charlie-crist</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Florida</td>\n",
              "      <td>democrat</td>\n",
              "      <td>15.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>an interview on CNN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-630b3826-9a54-48bd-95e4-3a2b793563e6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-630b3826-9a54-48bd-95e4-3a2b793563e6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-630b3826-9a54-48bd-95e4-3a2b793563e6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1560e905-1d33-434a-913c-51bf5b64f910\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1560e905-1d33-434a-913c-51bf5b64f910')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1560e905-1d33-434a-913c-51bf5b64f910 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10240,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10240,\n        \"samples\": [\n          \"10626.json\",\n          \"1520.json\",\n          \"1326.json\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"false\",\n          \"half-true\",\n          \"pants-fire\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"statement\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10223,\n        \"samples\": [\n          \"Countries bombed: Obama 7, Bush 4\",\n          \"Says she couldn't take stimulus money because it required \\\"universal building codes.\\\"\",\n          \"In the past decade, K-12 funding has grown six times faster than the rate of enrollment.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subject\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3827,\n        \"samples\": [\n          \"military,new-hampshire-2012\",\n          \"city-budget,city-government,county-budget,county-government,taxes\",\n          \"children,families,poverty,welfare\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"speaker\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2910,\n        \"samples\": [\n          \"ken-plum\",\n          \"mike-adams\",\n          \"tom-harkin\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"speaker_job\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1183,\n        \"samples\": [\n          \"Candidate, Multnomah County commission\",\n          \"policy analyst, The Tax Foundation\",\n          \"campaign committee\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"state\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 83,\n        \"samples\": [\n          \"Illinois \",\n          \"Texas\",\n          \"Wyoming\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"party\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 23,\n        \"samples\": [\n          \"tea-party-member\",\n          \"newsmaker\",\n          \"republican\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"barely_true_counts\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18.9737643185268,\n        \"min\": 0.0,\n        \"max\": 70.0,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          8.0,\n          13.0,\n          26.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"false_counts\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 24.11293628542088,\n        \"min\": 0.0,\n        \"max\": 114.0,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          18.0,\n          43.0,\n          11.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"half_true_counts\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 35.846510979322154,\n        \"min\": 0.0,\n        \"max\": 160.0,\n        \"num_unique_values\": 29,\n        \"samples\": [\n          12.0,\n          7.0,\n          38.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mostly_true_counts\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 36.15168793284596,\n        \"min\": 0.0,\n        \"max\": 163.0,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          4.0,\n          6.0,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pants_on_fire_counts\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16.12892735191374,\n        \"min\": 0.0,\n        \"max\": 105.0,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.0,\n          6.0,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4345,\n        \"samples\": [\n          \"a broadcast of \\\"The Kelly File\\\"\",\n          \"a Spanish-language radio ad\",\n          \"an appearance on ABC's \\\"This Week.\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Profiler function\n",
        "def train_epoch_with_profiler(model, dataloader, optimizer, scheduler, criterion, device, prof):\n",
        "    \"\"\"带Profiler的训练epoch (只profile前几个batch)\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Profile前5个batch\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        if batch_idx < 5:\n",
        "            with record_function(\"data_loading\"):\n",
        "                input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "                attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
        "                labels = batch['label'].to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with record_function(\"forward_pass\"):\n",
        "                with torch.amp.autocast(device_type='cuda', dtype=dtype):\n",
        "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                    loss = criterion(outputs.logits, labels)\n",
        "\n",
        "            with record_function(\"backward_pass\"):\n",
        "                loss.backward()\n",
        "\n",
        "            with record_function(\"optimizer_step\"):\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            prof.step()\n",
        "        else:\n",
        "            # 剩余batch正常训练（不profile）\n",
        "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
        "            labels = batch['label'].to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.amp.autocast(device_type='cuda', dtype=dtype):\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                loss = criterion(outputs.logits, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_time = end_time - start_time\n",
        "    peak_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
        "\n",
        "    return total_loss / len(dataloader), epoch_time, peak_memory"
      ],
      "metadata": {
        "id": "JTopLXFMupc2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title baseline+Amp\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup, BertConfig\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ========== 配置 ==========\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 128    # A100 显存大\n",
        "LR = 2e-5\n",
        "EPOCHS = 10\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "NUM_WORKERS = 8\n",
        "\n",
        "# ✅ 修改点 1: 定义官方数据集路径\n",
        "TRAIN_PATH = '/kaggle/input/liar-dataset/train.tsv'\n",
        "VALID_PATH = '/kaggle/input/liar-dataset/valid.tsv'\n",
        "TEST_PATH  = '/kaggle/input/liar-dataset/test.tsv'\n",
        "\n",
        "# ========== 1. 数据集类 (保持不变) ==========\n",
        "class TextualizedLIARDataset(Dataset):\n",
        "    def __init__(self, tsv_path, tokenizer, max_len=128):\n",
        "        self.df = pd.read_csv(tsv_path, sep='\\t', header=None, names=[\n",
        "            \"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job\",\n",
        "            \"state\", \"party\", \"barely_true_counts\", \"false_counts\",\n",
        "            \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\",\n",
        "            \"context\"\n",
        "        ])\n",
        "\n",
        "        self.df.dropna(subset=['statement'], inplace=True)\n",
        "\n",
        "        # 标签逻辑：False/Pants-fire/Barely-true = 0 (Fake)\n",
        "        self.label_map = {\n",
        "            \"pants-fire\": 0, \"false\": 0, \"barely-true\": 0,\n",
        "            \"half-true\": 1, \"mostly-true\": 1, \"true\": 1\n",
        "        }\n",
        "\n",
        "        self.df['label'] = self.df['label'].map(self.label_map)\n",
        "        self.df.dropna(subset=['label'], inplace=True)\n",
        "        self.df['label'] = self.df['label'].astype(int)\n",
        "\n",
        "        text_cols = ['statement', 'subject', 'speaker', 'party', 'state', 'speaker_job', 'context']\n",
        "        for col in text_cols:\n",
        "            self.df[col] = self.df[col].fillna(\"Unknown\")\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        metadata_str = (\n",
        "            f\"Speaker: {row['speaker']} | \"\n",
        "            f\"Job: {row['speaker_job']} | \"\n",
        "            f\"Party: {row['party']} | \"\n",
        "            f\"State: {row['state']} | \"\n",
        "            f\"Context: {row['context']} | \"\n",
        "            f\"Subject: {row['subject']}\"\n",
        "        )\n",
        "        final_text = f\"{metadata_str} [SEP] Statement: {row['statement']}\"\n",
        "\n",
        "        encoded = self.tokenizer(\n",
        "            final_text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(row['label'], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# ========== 2. 训练与评估函数 (保持不变) ==========\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "    reset_peak_memory()\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
        "        labels = batch['label'].to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', dtype=dtype):\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_time = end_time - start_time\n",
        "\n",
        "    peak_memory = get_peak_gpu_memory_mb()\n",
        "\n",
        "    return total_loss / len(dataloader), epoch_time, peak_memory\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    reset_peak_memory()\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    eval_time = time.time() - start_time\n",
        "    peak_memory = get_peak_gpu_memory_mb()\n",
        "\n",
        "    return total_loss / len(dataloader), accuracy_score(all_labels, all_preds), all_labels, all_preds, eval_time, peak_memory\n",
        "\n",
        "# ========== 3. 主程序  ==========\n",
        "def run_official_split_training():\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    print(f\"Using device: {DEVICE} | Model: {MODEL_NAME}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    print(\"Loading Official Datasets (Train / Valid / Test)...\")\n",
        "    if not os.path.exists(TRAIN_PATH):\n",
        "        print(f\"Error: Path {TRAIN_PATH} not found.\")\n",
        "        return\n",
        "\n",
        "    train_dataset = TextualizedLIARDataset(TRAIN_PATH, tokenizer, max_len=MAX_LEN)\n",
        "    valid_dataset = TextualizedLIARDataset(VALID_PATH, tokenizer, max_len=MAX_LEN)\n",
        "    test_dataset  = TextualizedLIARDataset(TEST_PATH, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "    # 创建 DataLoader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "    test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "    print(f\"Stats: Train={len(train_dataset)}, Valid={len(valid_dataset)}, Test={len(test_dataset)}\")\n",
        "\n",
        "    # 计算 Class Weights (直接从 train_dataset 获取)\n",
        "    print(\"Calculating class weights from Training set...\")\n",
        "    train_labels = train_dataset.df['label'].values\n",
        "\n",
        "    # 自动计算权重\n",
        "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
        "\n",
        "    print(f\"Class Weights: {class_weights} (Index 0 is Fake, Index 1 is True)\")\n",
        "\n",
        "    # 定义加权 Loss\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "    # 模型加载与配置 (Dropout 0.3)\n",
        "    print(\"Loading model with increased dropout...\")\n",
        "    config = BertConfig.from_pretrained(MODEL_NAME)\n",
        "    config.hidden_dropout_prob = 0.3\n",
        "    config.attention_probs_dropout_prob = 0.3\n",
        "    config.num_labels = 2\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
        "\n",
        "    # 计算参数数量\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"trainable params: {trainable:,} || all params: {total:,} || trainable%: {100*trainable/total:.4f}\")\n",
        "\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
        "\n",
        "    total_train_time = 0\n",
        "    epoch_times = []\n",
        "    epoch_memories = []\n",
        "\n",
        "\n",
        "    total_steps = len(train_loader) * EPOCHS\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)\n",
        "\n",
        "    best_val_f1 = 0\n",
        "\n",
        "    print(\"\\nStarting Training on Official Split...\")\n",
        "\n",
        "    profiler_data = {\n",
        "    'cpu_time': [],\n",
        "    'cuda_time': [],\n",
        "    'memory': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "      if epoch == 1:\n",
        "        print(\"Profiling enabled for epoch 1 ...\")\n",
        "\n",
        "        with profile(\n",
        "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "            record_shapes=True,\n",
        "            profile_memory=True,\n",
        "            with_stack=False\n",
        "        ) as prof:\n",
        "            train_loss, train_time, train_memory = train_epoch_with_profiler(\n",
        "                model, train_loader, optimizer, scheduler, criterion, DEVICE, prof\n",
        "            )\n",
        "\n",
        "        # ========== 使用内置表格，最稳定 ==========\n",
        "\n",
        "        print(\"PROFILER SUMMARY (Epoch 1)\")\n",
        "\n",
        "\n",
        "        print(\"\\nTop Operations by CPU Time:\")\n",
        "        print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=15))\n",
        "\n",
        "        print(\"\\nTop Operations by CUDA Time:\")\n",
        "        try:\n",
        "            print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=15))\n",
        "        except:\n",
        "            print(prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=15))\n",
        "\n",
        "        print(\"\\nTop Operations by Memory:\")\n",
        "        print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=15))\n",
        "\n",
        "        # 导出chrome trace\n",
        "        trace_file = \"profiler_trace_epoch1.json\"\n",
        "        prof.export_chrome_trace(trace_file)\n",
        "        print(f\"\\ Chrome trace saved to: {trace_file}\")\n",
        "        print(\"  Download and open in chrome://tracing\")\n",
        "      else:\n",
        "        train_loss, train_time, train_memory = train_epoch(\n",
        "          model, train_loader, optimizer, scheduler, criterion, DEVICE\n",
        "        )\n",
        "\n",
        "      val_loss, val_acc, val_labels, val_preds, val_time, val_memory = evaluate(\n",
        "          model, valid_loader, criterion, DEVICE\n",
        "      )\n",
        "\n",
        "      total_train_time += train_time\n",
        "      epoch_times.append(train_time)\n",
        "      epoch_memories.append(train_memory)\n",
        "\n",
        "      report_dict = classification_report(val_labels, val_preds, output_dict=True)\n",
        "      macro_f1 = report_dict['macro avg']['f1-score']\n",
        "      fake_recall = report_dict['0']['recall']\n",
        "\n",
        "      print(f\"Epoch {epoch}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Macro-F1: {macro_f1:.4f} | Fake Recall: {fake_recall:.4f}|  T_Time: {train_time:.1f}s | T_Mem: {train_memory:.0f}MB | V_Time: {val_time:.2f}s | V_Memory: {val_memory:.1f} MB\")\n",
        "\n",
        "      if macro_f1 > best_val_f1:\n",
        "          best_val_f1 = macro_f1\n",
        "          torch.save(model.state_dict(), 'best_baselineamp_model.pth')\n",
        "          print(\" -> Best baselineamp model updated!\")\n",
        "\n",
        "\n",
        "    print(f\"Total Training Time: {total_train_time:.1f}s\")\n",
        "    print(f\"Avg Epoch Time: {sum(epoch_times)/len(epoch_times):.1f}s\")\n",
        "    print(f\"Avg Peak Memory: {sum(epoch_memories)/len(epoch_memories):.0f}MB\")\n",
        "\n",
        "    # 训练结束后，自动在官方 Test Set 上跑一次\n",
        "    print(\"\\n========== FINAL TEST RESULT (Official Test Set) ==========\")\n",
        "    # 注意：这里我们直接用最后一个epoch的模型跑test，如果你想用 best model，需要上面取消注释 torch.save 并在这里 load\n",
        "    model.load_state_dict(torch.load('best_baselineamp_model.pth'))\n",
        "    test_loss, test_acc, test_labels, test_preds, test_time, test_memory = evaluate(model, test_loader, criterion, DEVICE)\n",
        "    print(classification_report(test_labels, test_preds, target_names=['Fake (0)', 'True (1)']))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_official_split_training()"
      ],
      "metadata": {
        "id": "CT1LYjFO2YxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e0856c2-6e73-4c43-f3d9-4e3125fdde5f",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda | Model: bert-base-uncased\n",
            "Loading Official Datasets (Train / Valid / Test)...\n",
            "Stats: Train=10240, Valid=1284, Test=1267\n",
            "Calculating class weights from Training set...\n",
            "Class Weights: [1.14081996 0.89012517] (Index 0 is Fake, Index 1 is True)\n",
            "Loading model with increased dropout...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 109,483,778 || all params: 109,483,778 || trainable%: 100.0000\n",
            "\n",
            "Starting Training on Official Split...\n",
            "Profiling enabled for epoch 1 ...\n",
            "PROFILER SUMMARY (Epoch 1)\n",
            "\n",
            "Top Operations by CPU Time:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                             aten::item         0.53%      70.165ms        38.90%        5.186s     160.446us       0.000us         0.00%     343.577us       0.011us           0 B           0 B           0 B           0 B         32322  \n",
            "                              aten::_local_scalar_dense         0.17%      22.832ms        38.37%        5.116s     158.275us     343.577us         0.00%     343.577us       0.011us           0 B           0 B           0 B           0 B         32322  \n",
            "                                  cudaStreamSynchronize        38.16%        5.087s        38.17%        5.088s      31.800ms       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B           160  \n",
            "                                           aten::linear         1.10%     146.691ms        14.54%        1.938s     163.685us       0.000us         0.00%        4.197s     354.457us           0 B           0 B    1004.61 GB      -1.12 MB         11840  \n",
            "                                               aten::to         0.81%     107.994ms        10.66%        1.422s      27.532us       0.000us         0.00%        1.170s      22.659us           0 B           0 B     688.66 GB           0 B         51641  \n",
            "    autograd::engine::evaluate_function: AddmmBackward0         1.30%     173.247ms        10.01%        1.335s     225.527us       0.000us         0.00%        3.874s     654.362us           0 B           0 B    -361.77 GB    -779.59 GB          5920  \n",
            "                                         aten::_to_copy         2.43%     323.550ms         9.85%        1.314s      37.579us       0.000us         0.00%        1.170s      33.470us           0 B           0 B     688.66 GB           0 B         34960  \n",
            "autograd::engine::evaluate_function: ToCopyBackward0...         1.34%     178.997ms         6.97%     928.957ms      58.646us       0.000us         0.00%        1.253s      79.092us           0 B           0 B    -166.92 GB    -552.81 GB         15840  \n",
            "                                       cudaLaunchKernel         6.78%     904.369ms         6.87%     916.127ms       9.746us       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B         94002  \n",
            "                                         AddmmBackward0         0.82%     108.980ms         6.56%     874.673ms     147.749us       0.000us         0.00%        3.443s     581.670us           0 B           0 B     417.81 GB           0 B          5920  \n",
            "                              Optimizer.step#AdamW.step         2.15%     286.949ms         6.33%     844.041ms      10.551ms       0.000us         0.00%     456.123ms       5.702ms         804 B        -320 B     838.80 MB     -33.04 GB            80  \n",
            "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...         5.67%     756.172ms         5.70%     759.384ms       9.375ms       0.000us         0.00%       0.000us       0.000us           0 B     -80.00 KB           0 B           0 B            81  \n",
            "                                            aten::copy_         2.30%     306.680ms         5.10%     680.435ms      19.287us        1.174s         8.75%        1.174s      33.284us           0 B           0 B           0 B           0 B         35280  \n",
            "                                        ToCopyBackward0         0.41%      54.945ms         5.07%     675.869ms      42.668us       0.000us         0.00%     588.545ms      37.156us           0 B           0 B     385.89 GB           0 B         15840  \n",
            "                                               aten::mm         3.01%     401.681ms         4.39%     585.209ms      49.426us        3.443s        25.65%        3.443s     290.835us           0 B           0 B     417.81 GB     417.81 GB         11840  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 13.331s\n",
            "Self CUDA time total: 13.427s\n",
            "\n",
            "\n",
            "Top Operations by CUDA Time:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::linear         1.10%     146.691ms        14.54%        1.938s     163.685us       0.000us         0.00%        4.197s     354.457us           0 B           0 B    1004.61 GB      -1.12 MB         11840  \n",
            "    autograd::engine::evaluate_function: AddmmBackward0         1.30%     173.247ms        10.01%        1.335s     225.527us       0.000us         0.00%        3.874s     654.362us           0 B           0 B    -361.77 GB    -779.59 GB          5920  \n",
            "                                         AddmmBackward0         0.82%     108.980ms         6.56%     874.673ms     147.749us       0.000us         0.00%        3.443s     581.670us           0 B           0 B     417.81 GB           0 B          5920  \n",
            "                                               aten::mm         3.01%     401.681ms         4.39%     585.209ms      49.426us        3.443s        25.65%        3.443s     290.835us           0 B           0 B     417.81 GB     417.81 GB         11840  \n",
            "autograd::engine::evaluate_function: ScaledDotProduc...         0.24%      31.440ms         1.71%     227.819ms     237.312us       0.000us         0.00%        1.995s       2.078ms     -15.00 KB     -15.00 KB    -106.60 GB    -241.60 GB           960  \n",
            "            ScaledDotProductEfficientAttentionBackward0         0.11%      14.487ms         1.47%     196.379ms     204.562us       0.000us         0.00%        1.995s       2.078ms           0 B           0 B     135.00 GB           0 B           960  \n",
            "aten::_scaled_dot_product_efficient_attention_backwa...         0.20%      26.794ms         1.36%     181.892ms     189.471us       0.000us         0.00%        1.995s       2.078ms           0 B           0 B     135.00 GB           0 B           960  \n",
            "                    aten::_efficient_attention_backward         0.27%      36.154ms         0.86%     114.150ms     118.906us        1.946s        14.50%        1.995s       2.078ms           0 B           0 B     135.00 GB     -91.61 GB           960  \n",
            "fmha_cutlassB_bf16_aligned_64x64_k64_dropout_sm80(Py...         0.00%       0.000us         0.00%       0.000us       0.000us        1.946s        14.50%        1.946s       2.027ms           0 B           0 B           0 B           0 B           960  \n",
            "                                            aten::addmm         2.19%     291.822ms         2.82%     375.909ms      63.498us        1.904s        14.18%        1.904s     321.624us           0 B           0 B     405.01 GB     405.01 GB          5920  \n",
            "                     aten::scaled_dot_product_attention         0.17%      22.579ms         2.59%     345.326ms     179.857us       0.000us         0.00%        1.509s     786.050us      30.00 KB           0 B     108.21 GB           0 B          1920  \n",
            "ampere_bf16_s16816gemm_bf16_128x128_ldg8_relu_f2f_st...         0.00%       0.000us         0.00%       0.000us       0.000us        1.287s         9.59%        1.287s     268.171us           0 B           0 B           0 B           0 B          4800  \n",
            "autograd::engine::evaluate_function: ToCopyBackward0...         1.34%     178.997ms         6.97%     928.957ms      58.646us       0.000us         0.00%        1.253s      79.092us           0 B           0 B    -166.92 GB    -552.81 GB         15840  \n",
            "ampere_bf16_s16816gemm_bf16_128x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us        1.240s         9.24%        1.240s     258.359us           0 B           0 B           0 B           0 B          4800  \n",
            "                                            aten::copy_         2.30%     306.680ms         5.10%     680.435ms      19.287us        1.174s         8.75%        1.174s      33.284us           0 B           0 B           0 B           0 B         35280  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 13.331s\n",
            "Self CUDA time total: 13.427s\n",
            "\n",
            "\n",
            "Top Operations by Memory:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                    aten::empty_strided         3.38%     451.053ms         3.50%     466.197ms       8.372us       0.000us         0.00%       0.000us       0.000us           0 B           0 B     866.91 GB     866.91 GB         55682  \n",
            "                                            aten::empty         1.36%     181.898ms         1.41%     187.584ms       7.933us       0.000us         0.00%       0.000us       0.000us     161.43 KB     161.43 KB     772.06 GB     772.06 GB         23645  \n",
            "                                               aten::mm         3.01%     401.681ms         4.39%     585.209ms      49.426us        3.443s        25.65%        3.443s     290.835us           0 B           0 B     417.81 GB     417.81 GB         11840  \n",
            "                                            aten::addmm         2.19%     291.822ms         2.82%     375.909ms      63.498us        1.904s        14.18%        1.904s     321.624us           0 B           0 B     405.01 GB     405.01 GB          5920  \n",
            "                                              aten::add         0.35%      46.459ms         0.53%      71.084ms      34.175us     365.986ms         2.73%     365.986ms     175.955us           0 B           0 B     187.50 GB     187.50 GB          2080  \n",
            "                                             aten::gelu         0.16%      21.489ms         0.26%      35.144ms      36.609us     223.218ms         1.66%     223.218ms     232.519us           0 B           0 B     180.00 GB     180.00 GB           960  \n",
            "                                    aten::gelu_backward         0.12%      16.034ms         0.19%      25.700ms      26.771us     347.922ms         2.59%     347.922ms     362.419us           0 B           0 B     180.00 GB     180.00 GB           960  \n",
            "                                          aten::resize_         0.02%       3.083ms         0.02%       3.083ms       6.371us       0.000us         0.00%       0.000us       0.000us           0 B           0 B      15.10 GB      15.10 GB           484  \n",
            "                                              aten::sub         0.02%       2.141ms         0.02%       3.141ms      39.258us       3.205ms         0.02%       3.205ms      40.061us           0 B           0 B       2.50 GB       2.50 GB            80  \n",
            "                               cudaPointerGetAttributes         0.01%     821.894us         0.01%     821.894us       3.425us       0.000us         0.00%       0.000us       0.000us           0 B           0 B      94.63 MB      94.63 MB           240  \n",
            "                                              aten::sum         1.28%     170.818ms         1.99%     265.146ms      44.191us     435.061ms         3.24%     435.061ms      72.510us           0 B           0 B      72.81 MB      72.81 MB          6000  \n",
            "                                           Buffer Flush         0.01%       1.783ms         0.01%       1.862ms     103.424us     602.394us         0.00%     602.394us      33.466us           0 B           0 B      59.54 MB      32.76 MB            18  \n",
            "                                             aten::tanh         0.02%       2.362ms         0.03%       3.336ms      41.705us     248.958us         0.00%     248.958us       3.112us           0 B           0 B      15.00 MB      15.00 MB            80  \n",
            "                                    aten::tanh_backward         0.01%       1.543ms         0.02%       2.444ms      30.556us     184.732us         0.00%     184.732us       2.309us           0 B           0 B      15.00 MB      15.00 MB            80  \n",
            "                                               aten::eq         0.02%       2.966ms         0.03%       4.045ms      50.558us     246.240us         0.00%     246.240us       3.078us           0 B           0 B       2.50 MB       2.50 MB            80  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 13.331s\n",
            "Self CUDA time total: 13.427s\n",
            "\n",
            "\\ Chrome trace saved to: profiler_trace_epoch1.json\n",
            "  Download and open in chrome://tracing\n",
            "Epoch 1/10 | Train Loss: 0.6994 | Val Loss: 0.6917 | Acc: 0.5576 | Macro-F1: 0.4643 | Fake Recall: 0.1461|  T_Time: 16.3s | T_Mem: 14360MB | V_Time: 6.01s | V_Memory: 2785.8 MB\n",
            " -> Best baselineamp model updated!\n",
            "Epoch 2/10 | Train Loss: 0.6747 | Val Loss: 0.6803 | Acc: 0.6184 | Macro-F1: 0.6112 | Fake Recall: 0.5032|  T_Time: 16.2s | T_Mem: 14360MB | V_Time: 5.80s | V_Memory: 2785.8 MB\n",
            " -> Best baselineamp model updated!\n",
            "Epoch 3/10 | Train Loss: 0.6583 | Val Loss: 0.7090 | Acc: 0.6106 | Macro-F1: 0.5644 | Fake Recall: 0.2971|  T_Time: 16.2s | T_Mem: 14360MB | V_Time: 5.78s | V_Memory: 2785.8 MB\n",
            "Epoch 4/10 | Train Loss: 0.6399 | Val Loss: 0.6824 | Acc: 0.6355 | Macro-F1: 0.6070 | Fake Recall: 0.3815|  T_Time: 16.2s | T_Mem: 14360MB | V_Time: 5.88s | V_Memory: 2785.8 MB\n",
            "Epoch 5/10 | Train Loss: 0.6238 | Val Loss: 0.7072 | Acc: 0.6386 | Macro-F1: 0.6047 | Fake Recall: 0.3604|  T_Time: 16.2s | T_Mem: 14360MB | V_Time: 5.86s | V_Memory: 2785.8 MB\n",
            "Epoch 6/10 | Train Loss: 0.6103 | Val Loss: 0.6792 | Acc: 0.6511 | Macro-F1: 0.6261 | Fake Recall: 0.4091|  T_Time: 16.1s | T_Mem: 14360MB | V_Time: 5.77s | V_Memory: 2785.8 MB\n",
            " -> Best baselineamp model updated!\n",
            "Epoch 7/10 | Train Loss: 0.5947 | Val Loss: 0.7221 | Acc: 0.6480 | Macro-F1: 0.6122 | Fake Recall: 0.3588|  T_Time: 16.2s | T_Mem: 14360MB | V_Time: 5.84s | V_Memory: 2785.8 MB\n",
            "Epoch 8/10 | Train Loss: 0.5834 | Val Loss: 0.7157 | Acc: 0.6519 | Macro-F1: 0.6213 | Fake Recall: 0.3831|  T_Time: 16.2s | T_Mem: 14360MB | V_Time: 5.82s | V_Memory: 2785.8 MB\n",
            "Epoch 9/10 | Train Loss: 0.5759 | Val Loss: 0.7066 | Acc: 0.6565 | Macro-F1: 0.6333 | Fake Recall: 0.4221|  T_Time: 16.2s | T_Mem: 14360MB | V_Time: 5.83s | V_Memory: 2785.8 MB\n",
            " -> Best baselineamp model updated!\n",
            "Epoch 10/10 | Train Loss: 0.5697 | Val Loss: 0.7086 | Acc: 0.6488 | Macro-F1: 0.6244 | Fake Recall: 0.4107|  T_Time: 16.2s | T_Mem: 14360MB | V_Time: 5.85s | V_Memory: 2785.8 MB\n",
            "Total Training Time: 161.9s\n",
            "Avg Epoch Time: 16.2s\n",
            "Avg Peak Memory: 14360MB\n",
            "\n",
            "========== FINAL TEST RESULT (Official Test Set) ==========\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Fake (0)       0.68      0.41      0.51       553\n",
            "    True (1)       0.65      0.85      0.74       714\n",
            "\n",
            "    accuracy                           0.66      1267\n",
            "   macro avg       0.66      0.63      0.62      1267\n",
            "weighted avg       0.66      0.66      0.64      1267\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title rebuild for baseline + amp\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 1. 重新准备环境和数据\n",
        "# ==========================================\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# 重新定义配置\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "BATCH_SIZE = 128\n",
        "MAX_LEN = 256\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "VALID_PATH = '/kaggle/input/liar-dataset/valid.tsv'\n",
        "TEST_PATH =  '/kaggle/input/liar-dataset/test.tsv'\n",
        "\n",
        "print(\"Re-loading Tokenizer and Dataloaders...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# 重新实例化数据集 (确保 TextualizedLIARDataset 类已经在前面的格子里运行过)\n",
        "valid_dataset = TextualizedLIARDataset(VALID_PATH, tokenizer, max_len=MAX_LEN)\n",
        "test_dataset  = TextualizedLIARDataset(TEST_PATH, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "# 重新实例化 DataLoader\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# ==========================================\n",
        "# 2. 定义寻找最佳阈值的函数\n",
        "# ==========================================\n",
        "def find_optimal_threshold(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(\"Running inference on Validation Set...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            # Softmax 拿到概率\n",
        "            probs = F.softmax(outputs.logits, dim=1)\n",
        "            # 取出 Label 1 (True) 的概率\n",
        "            true_probs = probs[:, 1].cpu().numpy()\n",
        "\n",
        "            all_probs.extend(true_probs)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_probs = np.array(all_probs)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # 遍历寻找最佳 F1\n",
        "    best_threshold = 0.5\n",
        "    best_f1 = 0\n",
        "\n",
        "    thresholds = np.arange(0.1, 0.95, 0.05)\n",
        "\n",
        "    print(f\"\\n{'Threshold':<10} | {'Macro F1':<10} | {'Fake Recall':<12} | {'True Recall':<12}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for thresh in thresholds:\n",
        "        preds = (all_probs > thresh).astype(int)\n",
        "\n",
        "        report = classification_report(all_labels, preds, output_dict=True)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        fake_recall = report['0']['recall']\n",
        "        true_recall = report['1']['recall']\n",
        "\n",
        "        print(f\"{thresh:.2f}       | {macro_f1:.4f}     | {fake_recall:.4f}       | {true_recall:.4f}\")\n",
        "\n",
        "        if macro_f1 > best_f1:\n",
        "            best_f1 = macro_f1\n",
        "            best_threshold = thresh\n",
        "\n",
        "    print(f\"\\nBest Threshold found: {best_threshold:.2f}\")\n",
        "    return best_threshold\n",
        "\n",
        "# ==========================================\n",
        "# 3. 执行优化\n",
        "# ==========================================\n",
        "# 加载模型\n",
        "print(\"\\nLoading model weights from 'best_baselineamp_model.pth'...\")\n",
        "# 必须重新初始化模型结构才能加载权重\n",
        "config = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).config\n",
        "config.num_labels = 2\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
        "model.load_state_dict(torch.load('best_baselineamp_model.pth'))\n",
        "model.to(DEVICE)\n",
        "\n",
        "# 1. 在 Validation Set 上找最佳门槛\n",
        "best_thresh = find_optimal_threshold(model, valid_loader, DEVICE)\n",
        "\n",
        "# 2. 应用到 Test Set\n",
        "print(f\"\\nApplying Threshold {best_thresh:.2f} to Test Set...\")\n",
        "model.eval()\n",
        "test_probs = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(DEVICE)\n",
        "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "        labels = batch['label'].to(DEVICE)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        probs = F.softmax(outputs.logits, dim=1)\n",
        "        test_probs.extend(probs[:, 1].cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_probs = np.array(test_probs)\n",
        "final_preds = (test_probs > best_thresh).astype(int)\n",
        "\n",
        "print(\"\\n========== OPTIMIZED TEST RESULT ==========\")\n",
        "print(classification_report(test_labels, final_preds, target_names=['Fake (0)', 'True (1)']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oW2a7YqQykK",
        "outputId": "1d7d860a-bbf5-44ef-be5c-fbde4af5db7f",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Re-loading Tokenizer and Dataloaders...\n",
            "\n",
            "Loading model weights from 'best_baselineamp_model.pth'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running inference on Validation Set...\n",
            "\n",
            "Threshold  | Macro F1   | Fake Recall  | True Recall \n",
            "------------------------------------------------------------\n",
            "0.10       | 0.3582     | 0.0146       | 1.0000\n",
            "0.15       | 0.3845     | 0.0406       | 0.9955\n",
            "0.20       | 0.4392     | 0.1006       | 0.9820\n",
            "0.25       | 0.4865     | 0.1575       | 0.9731\n",
            "0.30       | 0.5153     | 0.2013       | 0.9536\n",
            "0.35       | 0.5534     | 0.2549       | 0.9461\n",
            "0.40       | 0.5832     | 0.3052       | 0.9311\n",
            "0.45       | 0.6060     | 0.3523       | 0.9102\n",
            "0.50       | 0.6333     | 0.4221       | 0.8728\n",
            "0.55       | 0.6436     | 0.4805       | 0.8204\n",
            "0.60       | 0.6546     | 0.5373       | 0.7769\n",
            "0.65       | 0.6605     | 0.6055       | 0.7156\n",
            "0.70       | 0.6464     | 0.6623       | 0.6317\n",
            "0.75       | 0.6331     | 0.7597       | 0.5225\n",
            "0.80       | 0.6043     | 0.8409       | 0.4132\n",
            "0.85       | 0.5463     | 0.9058       | 0.2859\n",
            "0.90       | 0.4568     | 0.9643       | 0.1452\n",
            "\n",
            "Best Threshold found: 0.65\n",
            "\n",
            "Applying Threshold 0.65 to Test Set...\n",
            "\n",
            "========== OPTIMIZED TEST RESULT ==========\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Fake (0)       0.60      0.59      0.60       553\n",
            "    True (1)       0.69      0.70      0.69       714\n",
            "\n",
            "    accuracy                           0.65      1267\n",
            "   macro avg       0.65      0.64      0.64      1267\n",
            "weighted avg       0.65      0.65      0.65      1267\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title LORA\n",
        "!pip install peft --break-system-packages"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18hoIU4HL5BB",
        "outputId": "eaa0bf7c-bc33-4da7-b216-a7c46c2617be",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.12.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.7.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup, BertConfig\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import warnings\n",
        "import os\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "hmKuPFJgOtsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title grid search outside funciton\n",
        "\n",
        "import itertools\n",
        "from copy import deepcopy\n",
        "import time\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "\n",
        "def grid_search_lora_bert_model(\n",
        "    model_name,\n",
        "    train_loader,\n",
        "    valid_loader,\n",
        "    param_grid,\n",
        "    tokenizer,\n",
        "    device,\n",
        "    class_weights_tensor,\n",
        "    epochs_per_config=2,\n",
        "    verbose=True\n",
        "):\n",
        "    \"\"\"\n",
        "    对BERT+LoRA模型进行Grid Search\n",
        "    \"\"\"\n",
        "\n",
        "    # 生成所有参数组合\n",
        "    keys = param_grid.keys()\n",
        "    values = param_grid.values()\n",
        "    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"GRID SEARCH START\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Total configurations to search: {len(param_combinations)}\")\n",
        "    print(f\"Epochs per configuration: {epochs_per_config}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    results = []\n",
        "    best_f1 = 0\n",
        "    best_config = None\n",
        "    best_model_state = None\n",
        "    last_r = None\n",
        "\n",
        "    for idx, params in enumerate(param_combinations, 1):\n",
        "        # 打印当前配置（简洁版）\n",
        "        param_str = \" | \".join([f\"{k}={v}\" for k, v in params.items()])\n",
        "        print(f\"[{idx}/{len(param_combinations)}] {param_str}\")\n",
        "\n",
        "        # 根据参数重新创建数据加载器(如果batch_size在参数中)\n",
        "        if 'batch_size' in params:\n",
        "            current_train_loader = DataLoader(\n",
        "                train_loader.dataset,\n",
        "                batch_size=params['batch_size'],\n",
        "                shuffle=True,\n",
        "                num_workers=train_loader.num_workers,\n",
        "                pin_memory=True\n",
        "            )\n",
        "            current_valid_loader = DataLoader(\n",
        "                valid_loader.dataset,\n",
        "                batch_size=params['batch_size'],\n",
        "                shuffle=False,\n",
        "                num_workers=valid_loader.num_workers,\n",
        "                pin_memory=True\n",
        "            )\n",
        "        else:\n",
        "            current_train_loader = train_loader\n",
        "            current_valid_loader = valid_loader\n",
        "\n",
        "        # 创建基础模型\n",
        "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=2\n",
        "        )\n",
        "\n",
        "        # 配置LoRA\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            r=params.get('lora_r', 8),\n",
        "            lora_alpha=params.get('lora_alpha', 32),\n",
        "            lora_dropout=params.get('lora_dropout', 0.01),\n",
        "            target_modules=[\"query\", \"value\"],\n",
        "            bias=\"none\",\n",
        "            inference_mode=False\n",
        "        )\n",
        "\n",
        "        # 应用LoRA\n",
        "        model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "        if verbose:\n",
        "            current_r = params.get('lora_r', 8)\n",
        "            if current_r != last_r:\n",
        "                model.print_trainable_parameters()\n",
        "                last_r = current_r\n",
        "\n",
        "        model.to(device)\n",
        "\n",
        "        # 创建优化器\n",
        "        optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=params.get('learning_rate', 2e-5),\n",
        "            weight_decay=params.get('weight_decay', 0.01)\n",
        "        )\n",
        "\n",
        "        # 学习率调度器\n",
        "        total_steps = len(current_train_loader) * epochs_per_config\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=int(0.1 * total_steps),\n",
        "            num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "        # 损失函数\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "        # 训练（不打印每个epoch的详情）\n",
        "        best_epoch_f1 = 0\n",
        "        best_epoch_acc = 0\n",
        "        best_epoch_loss = float('inf')\n",
        "\n",
        "        for epoch in range(1, epochs_per_config + 1):\n",
        "            train_loss, train_time, train_memory = train_epoch(\n",
        "                model, current_train_loader, optimizer, scheduler, criterion, device\n",
        "            )\n",
        "            val_loss, val_acc, val_labels, val_preds, val_time, val_memory = evaluate(\n",
        "                model, current_valid_loader, criterion, device\n",
        "            )\n",
        "\n",
        "            report_dict = classification_report(\n",
        "                val_labels, val_preds, output_dict=True, zero_division=0\n",
        "            )\n",
        "            macro_f1 = report_dict['macro avg']['f1-score']\n",
        "\n",
        "            if macro_f1 > best_epoch_f1:\n",
        "                best_epoch_f1 = macro_f1\n",
        "                best_epoch_acc = val_acc\n",
        "                best_epoch_loss = val_loss\n",
        "\n",
        "        # 打印该配置的最终结果（一行）\n",
        "        print(f\" Val Loss: {best_epoch_loss:.4f} | Val Acc: {best_epoch_acc:.4f} | Val F1: {best_epoch_f1:.4f}\", end=\"\")\n",
        "\n",
        "        # 保存结果\n",
        "        result = {\n",
        "            'params': params.copy(),\n",
        "            'best_val_f1': best_epoch_f1,\n",
        "            'best_val_acc': best_epoch_acc,\n",
        "            'best_val_loss': best_epoch_loss,\n",
        "            'final_train_time': train_time,\n",
        "            'final_memory': train_memory\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        # 更新全局最佳\n",
        "        if best_epoch_f1 > best_f1:\n",
        "            best_f1 = best_epoch_f1\n",
        "            best_config = params.copy()\n",
        "            best_model_state = deepcopy(model.state_dict())\n",
        "            print(\"NEW BEST!\")\n",
        "        else:\n",
        "            print()  # 换行\n",
        "\n",
        "        # 清理内存\n",
        "        del model, base_model, optimizer, scheduler\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # 打印最终结果\n",
        "    print(f\"\\n\")\n",
        "    print(\"GRID SEARCH COMPLETE\")\n",
        "    print(\"\\nTop 5 Configurations (sorted by F1 score):\")\n",
        "\n",
        "    results_sorted = sorted(results, key=lambda x: x['best_val_f1'], reverse=True)\n",
        "\n",
        "    for i, r in enumerate(results_sorted[:5], 1):\n",
        "        param_str = \" | \".join([f\"{k}={v}\" for k, v in r['params'].items()])\n",
        "        print(f\"{i}. F1: {r['best_val_f1']:.4f} | Acc: {r['best_val_acc']:.4f} | Loss: {r['best_val_loss']:.4f}\")\n",
        "        print(f\"   {param_str}\")\n",
        "\n",
        "    print(f\"\\n\")\n",
        "    print(\"BEST CONFIGURATION:\")\n",
        "    print(f\"Best Validation F1: {best_f1:.4f}\")\n",
        "    param_str = \" | \".join([f\"{k}={v}\" for k, v in best_config.items()])\n",
        "    print(f\"Parameters: {param_str}\")\n",
        "    print(f\"\\n\")\n",
        "\n",
        "    return {\n",
        "        'all_results': results,\n",
        "        'best_config': best_config,\n",
        "        'best_f1': best_f1,\n",
        "        'best_model_state': best_model_state\n",
        "    }"
      ],
      "metadata": {
        "id": "HX6o2Xc8dKmz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Lora grid search before final training\n",
        "\n",
        "import torch\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "def run_lora_training_with_grid_search():\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    print(f\"Using device: {DEVICE} | Model: {MODEL_NAME}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    print(\"Loading Official Datasets (Train / Valid / Test)...\")\n",
        "    if not os.path.exists(TRAIN_PATH):\n",
        "        print(f\"Error: Path {TRAIN_PATH} not found.\")\n",
        "        return\n",
        "\n",
        "    train_dataset = TextualizedLIARDataset(TRAIN_PATH, tokenizer, max_len=MAX_LEN)\n",
        "    valid_dataset = TextualizedLIARDataset(VALID_PATH, tokenizer, max_len=MAX_LEN)\n",
        "    test_dataset  = TextualizedLIARDataset(TEST_PATH, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    valid_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(f\"Stats: Train={len(train_dataset)}, Valid={len(valid_dataset)}, Test={len(test_dataset)}\")\n",
        "\n",
        "    # 计算类别权重\n",
        "    print(\"Calculating class weights from Training set...\")\n",
        "    train_labels = train_dataset.df['label'].values\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(train_labels),\n",
        "        y=train_labels\n",
        "    )\n",
        "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
        "    print(f\"Class Weights: {class_weights}\")\n",
        "\n",
        "    # ========== 定义LoRA参数网格 ==========\n",
        "    param_grid = {\n",
        "        'learning_rate': [1e-5, 2e-5, 3e-5],\n",
        "        'lora_r': [4, 8, 16],\n",
        "        'lora_alpha': [16, 32],\n",
        "        'lora_dropout': [0.0, 0.1],\n",
        "        'weight_decay': [0.01]\n",
        "        # '': [64, 128]  # 如果想搜索batch_size\n",
        "    }\n",
        "\n",
        "    # ========== 执行Grid Search ==========\n",
        "    print(\"Starting Grid Search for LoRA Model...\")\n",
        "\n",
        "    search_results = grid_search_lora_bert_model(\n",
        "        model_name=MODEL_NAME,\n",
        "        train_loader=train_loader,\n",
        "        valid_loader=valid_loader,\n",
        "        param_grid=param_grid,\n",
        "        tokenizer=tokenizer,\n",
        "        device=DEVICE,\n",
        "        class_weights_tensor=class_weights_tensor,\n",
        "        epochs_per_config=2,  # 每个配置训练2轮\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # ========== 使用最佳配置重新训练完整模型 ==========\n",
        "    print(\"Training final model with best LoRA configuration...\")\n",
        "\n",
        "    best_params = search_results['best_config']\n",
        "\n",
        "    print(\"\\nBest Configuration Selected:\")\n",
        "    print(f\"  Learning Rate:  {best_params.get('learning_rate', 2e-5)}\")\n",
        "    print(f\"  LoRA Rank (r):  {best_params.get('lora_r', 8)}\")\n",
        "    print(f\"  LoRA Alpha:     {best_params.get('lora_alpha', 32)}\")\n",
        "    print(f\"  LoRA Dropout:   {best_params.get('lora_dropout', 0.01)}\")\n",
        "    print(f\"  Weight Decay:   {best_params.get('weight_decay', 0.01)}\")\n",
        "    print(f\"  Best Val F1:    {search_results['best_f1']:.4f}\")\n",
        "\n",
        "    return best_params"
      ],
      "metadata": {
        "cellView": "form",
        "id": "j7tcoxm3vyv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title get best params\n",
        "\n",
        "grid_search_output = run_lora_training_with_grid_search()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mz90eGUkZnmX",
        "outputId": "32abf0ee-e4c3-4f63-edc3-e7c71f5508ed",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda | Model: bert-base-uncased\n",
            "Loading Official Datasets (Train / Valid / Test)...\n",
            "Stats: Train=10240, Valid=1284, Test=1267\n",
            "Calculating class weights from Training set...\n",
            "Class Weights: [1.14081996 0.89012517]\n",
            "Starting Grid Search for LoRA Model...\n",
            "\n",
            "======================================================================\n",
            "GRID SEARCH START\n",
            "======================================================================\n",
            "Total configurations to search: 36\n",
            "Epochs per configuration: 2\n",
            "======================================================================\n",
            "\n",
            "[1/36] learning_rate=1e-05 | lora_r=4 | lora_alpha=16 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 148,994 || all params: 109,632,772 || trainable%: 0.1359\n",
            " Val Loss: 0.6903 | Val Acc: 0.5350 | Val F1: 0.5329NEW BEST!\n",
            "[2/36] learning_rate=1e-05 | lora_r=4 | lora_alpha=16 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6973 | Val Acc: 0.4930 | Val F1: 0.4795\n",
            "[3/36] learning_rate=1e-05 | lora_r=4 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6946 | Val Acc: 0.4782 | Val F1: 0.4202\n",
            "[4/36] learning_rate=1e-05 | lora_r=4 | lora_alpha=32 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6947 | Val Acc: 0.4961 | Val F1: 0.4959\n",
            "[5/36] learning_rate=1e-05 | lora_r=8 | lora_alpha=16 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 296,450 || all params: 109,780,228 || trainable%: 0.2700\n",
            " Val Loss: 0.7017 | Val Acc: 0.4540 | Val F1: 0.4524\n",
            "[6/36] learning_rate=1e-05 | lora_r=8 | lora_alpha=16 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6926 | Val Acc: 0.5101 | Val F1: 0.5092\n",
            "[7/36] learning_rate=1e-05 | lora_r=8 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6939 | Val Acc: 0.5000 | Val F1: 0.4743\n",
            "[8/36] learning_rate=1e-05 | lora_r=8 | lora_alpha=32 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6922 | Val Acc: 0.5421 | Val F1: 0.5418NEW BEST!\n",
            "[9/36] learning_rate=1e-05 | lora_r=16 | lora_alpha=16 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 591,362 || all params: 110,075,140 || trainable%: 0.5372\n",
            " Val Loss: 0.6901 | Val Acc: 0.5343 | Val F1: 0.5058\n",
            "[10/36] learning_rate=1e-05 | lora_r=16 | lora_alpha=16 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6946 | Val Acc: 0.5467 | Val F1: 0.5419NEW BEST!\n",
            "[11/36] learning_rate=1e-05 | lora_r=16 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6966 | Val Acc: 0.4922 | Val F1: 0.4780\n",
            "[12/36] learning_rate=1e-05 | lora_r=16 | lora_alpha=32 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6920 | Val Acc: 0.5522 | Val F1: 0.5425NEW BEST!\n",
            "[13/36] learning_rate=2e-05 | lora_r=4 | lora_alpha=16 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 148,994 || all params: 109,632,772 || trainable%: 0.1359\n",
            " Val Loss: 0.6919 | Val Acc: 0.5600 | Val F1: 0.5463NEW BEST!\n",
            "[14/36] learning_rate=2e-05 | lora_r=4 | lora_alpha=16 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6898 | Val Acc: 0.5421 | Val F1: 0.5404\n",
            "[15/36] learning_rate=2e-05 | lora_r=4 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6907 | Val Acc: 0.5584 | Val F1: 0.5524NEW BEST!\n",
            "[16/36] learning_rate=2e-05 | lora_r=4 | lora_alpha=32 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6926 | Val Acc: 0.5483 | Val F1: 0.5264\n",
            "[17/36] learning_rate=2e-05 | lora_r=8 | lora_alpha=16 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 296,450 || all params: 109,780,228 || trainable%: 0.2700\n",
            " Val Loss: 0.6922 | Val Acc: 0.5296 | Val F1: 0.4980\n",
            "[18/36] learning_rate=2e-05 | lora_r=8 | lora_alpha=16 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6948 | Val Acc: 0.4626 | Val F1: 0.4189\n",
            "[19/36] learning_rate=2e-05 | lora_r=8 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6966 | Val Acc: 0.5374 | Val F1: 0.5277\n",
            "[20/36] learning_rate=2e-05 | lora_r=8 | lora_alpha=32 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6940 | Val Acc: 0.5117 | Val F1: 0.4903\n",
            "[21/36] learning_rate=2e-05 | lora_r=16 | lora_alpha=16 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 591,362 || all params: 110,075,140 || trainable%: 0.5372\n",
            " Val Loss: 0.6955 | Val Acc: 0.4938 | Val F1: 0.4936\n",
            "[22/36] learning_rate=2e-05 | lora_r=16 | lora_alpha=16 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6938 | Val Acc: 0.5101 | Val F1: 0.5059\n",
            "[23/36] learning_rate=2e-05 | lora_r=16 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6943 | Val Acc: 0.4782 | Val F1: 0.4751\n",
            "[24/36] learning_rate=2e-05 | lora_r=16 | lora_alpha=32 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6935 | Val Acc: 0.5140 | Val F1: 0.4792\n",
            "[25/36] learning_rate=3e-05 | lora_r=4 | lora_alpha=16 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 148,994 || all params: 109,632,772 || trainable%: 0.1359\n",
            " Val Loss: 0.6915 | Val Acc: 0.5132 | Val F1: 0.5108\n",
            "[26/36] learning_rate=3e-05 | lora_r=4 | lora_alpha=16 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6909 | Val Acc: 0.5055 | Val F1: 0.4858\n",
            "[27/36] learning_rate=3e-05 | lora_r=4 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6919 | Val Acc: 0.5413 | Val F1: 0.4960\n",
            "[28/36] learning_rate=3e-05 | lora_r=4 | lora_alpha=32 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6958 | Val Acc: 0.5187 | Val F1: 0.5167\n",
            "[29/36] learning_rate=3e-05 | lora_r=8 | lora_alpha=16 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 296,450 || all params: 109,780,228 || trainable%: 0.2700\n",
            " Val Loss: 0.6941 | Val Acc: 0.5055 | Val F1: 0.5032\n",
            "[30/36] learning_rate=3e-05 | lora_r=8 | lora_alpha=16 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6904 | Val Acc: 0.5600 | Val F1: 0.5562NEW BEST!\n",
            "[31/36] learning_rate=3e-05 | lora_r=8 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6904 | Val Acc: 0.5600 | Val F1: 0.5597NEW BEST!\n",
            "[32/36] learning_rate=3e-05 | lora_r=8 | lora_alpha=32 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6930 | Val Acc: 0.5179 | Val F1: 0.5100\n",
            "[33/36] learning_rate=3e-05 | lora_r=16 | lora_alpha=16 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 591,362 || all params: 110,075,140 || trainable%: 0.5372\n",
            " Val Loss: 0.6956 | Val Acc: 0.4540 | Val F1: 0.4512\n",
            "[34/36] learning_rate=3e-05 | lora_r=16 | lora_alpha=16 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6922 | Val Acc: 0.5358 | Val F1: 0.5313\n",
            "[35/36] learning_rate=3e-05 | lora_r=16 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6922 | Val Acc: 0.5148 | Val F1: 0.5115\n",
            "[36/36] learning_rate=3e-05 | lora_r=16 | lora_alpha=32 | lora_dropout=0.1 | weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Val Loss: 0.6923 | Val Acc: 0.5506 | Val F1: 0.5391\n",
            "\n",
            "\n",
            "GRID SEARCH COMPLETE\n",
            "\n",
            "Top 5 Configurations (sorted by F1 score):\n",
            "1. F1: 0.5597 | Acc: 0.5600 | Loss: 0.6904\n",
            "   learning_rate=3e-05 | lora_r=8 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n",
            "2. F1: 0.5562 | Acc: 0.5600 | Loss: 0.6904\n",
            "   learning_rate=3e-05 | lora_r=8 | lora_alpha=16 | lora_dropout=0.1 | weight_decay=0.01\n",
            "3. F1: 0.5524 | Acc: 0.5584 | Loss: 0.6907\n",
            "   learning_rate=2e-05 | lora_r=4 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n",
            "4. F1: 0.5463 | Acc: 0.5600 | Loss: 0.6919\n",
            "   learning_rate=2e-05 | lora_r=4 | lora_alpha=16 | lora_dropout=0.0 | weight_decay=0.01\n",
            "5. F1: 0.5425 | Acc: 0.5522 | Loss: 0.6920\n",
            "   learning_rate=1e-05 | lora_r=16 | lora_alpha=32 | lora_dropout=0.1 | weight_decay=0.01\n",
            "\n",
            "\n",
            "BEST CONFIGURATION:\n",
            "Best Validation F1: 0.5597\n",
            "Parameters: learning_rate=3e-05 | lora_r=8 | lora_alpha=32 | lora_dropout=0.0 | weight_decay=0.01\n",
            "\n",
            "\n",
            "Training final model with best LoRA configuration...\n",
            "\n",
            "Best Configuration Selected:\n",
            "  Learning Rate:  3e-05\n",
            "  LoRA Rank (r):  8\n",
            "  LoRA Alpha:     32\n",
            "  LoRA Dropout:   0.0\n",
            "  Weight Decay:   0.01\n",
            "  Best Val F1:    0.5597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title model with lora\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup, BertConfig\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ========== 配置 ==========\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 128    # A100 显存大\n",
        "LR = 2e-5\n",
        "EPOCHS = 10\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "NUM_WORKERS = 8\n",
        "\n",
        "# ✅ 修改点 1: 定义官方数据集路径\n",
        "TRAIN_PATH = '/kaggle/input/liar-dataset/train.tsv'\n",
        "VALID_PATH = '/kaggle/input/liar-dataset/valid.tsv'\n",
        "TEST_PATH  = '/kaggle/input/liar-dataset/test.tsv'\n",
        "\n",
        "# ========== 1. 数据集类 (保持不变) ==========\n",
        "class TextualizedLIARDataset(Dataset):\n",
        "    def __init__(self, tsv_path, tokenizer, max_len=128):\n",
        "        self.df = pd.read_csv(tsv_path, sep='\\t', header=None, names=[\n",
        "            \"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job\",\n",
        "            \"state\", \"party\", \"barely_true_counts\", \"false_counts\",\n",
        "            \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\",\n",
        "            \"context\"\n",
        "        ])\n",
        "\n",
        "        self.df.dropna(subset=['statement'], inplace=True)\n",
        "\n",
        "        # 标签逻辑：False/Pants-fire/Barely-true = 0 (Fake)\n",
        "        self.label_map = {\n",
        "            \"pants-fire\": 0, \"false\": 0, \"barely-true\": 0,\n",
        "            \"half-true\": 1, \"mostly-true\": 1, \"true\": 1\n",
        "        }\n",
        "\n",
        "        self.df['label'] = self.df['label'].map(self.label_map)\n",
        "        self.df.dropna(subset=['label'], inplace=True)\n",
        "        self.df['label'] = self.df['label'].astype(int)\n",
        "\n",
        "        text_cols = ['statement', 'subject', 'speaker', 'party', 'state', 'speaker_job', 'context']\n",
        "        for col in text_cols:\n",
        "            self.df[col] = self.df[col].fillna(\"Unknown\")\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        metadata_str = (\n",
        "            f\"Speaker: {row['speaker']} | \"\n",
        "            f\"Job: {row['speaker_job']} | \"\n",
        "            f\"Party: {row['party']} | \"\n",
        "            f\"State: {row['state']} | \"\n",
        "            f\"Context: {row['context']} | \"\n",
        "            f\"Subject: {row['subject']}\"\n",
        "        )\n",
        "        final_text = f\"{metadata_str} [SEP] Statement: {row['statement']}\"\n",
        "\n",
        "        encoded = self.tokenizer(\n",
        "            final_text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(row['label'], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# ========== 2. 训练与评估函数 (保持不变) ==========\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "    reset_peak_memory()\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
        "        labels = batch['label'].to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', dtype=dtype):\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_time = end_time - start_time\n",
        "\n",
        "    peak_memory = get_peak_gpu_memory_mb()\n",
        "\n",
        "    return total_loss / len(dataloader), epoch_time, peak_memory\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    reset_peak_memory()\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    eval_time = time.time() - start_time\n",
        "    peak_memory = get_peak_gpu_memory_mb()\n",
        "\n",
        "    return total_loss / len(dataloader), accuracy_score(all_labels, all_preds), all_labels, all_preds, eval_time, peak_memory\n",
        "\n",
        "# ========== 3. 主程序  ==========\n",
        "def run_official_split_training():\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    print(f\"Using device: {DEVICE} | Model: {MODEL_NAME}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    print(\"Loading Official Datasets (Train / Valid / Test)...\")\n",
        "    if not os.path.exists(TRAIN_PATH):\n",
        "        print(f\"Error: Path {TRAIN_PATH} not found.\")\n",
        "        return\n",
        "\n",
        "    best_params = grid_search_output\n",
        "\n",
        "    # ========== 直接使用Grid Search中的数据加载器 ==========\n",
        "    train_dataset = TextualizedLIARDataset(TRAIN_PATH, tokenizer, max_len=MAX_LEN)\n",
        "    valid_dataset = TextualizedLIARDataset(VALID_PATH, tokenizer, max_len=MAX_LEN)\n",
        "    test_dataset  = TextualizedLIARDataset(TEST_PATH, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    valid_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(f\"Stats: Train={len(train_dataset)}, Valid={len(valid_dataset)}, Test={len(test_dataset)}\")\n",
        "\n",
        "    # 计算类别权重\n",
        "    print(\"Calculating class weights from Training set...\")\n",
        "    train_labels = train_dataset.df['label'].values\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(train_labels),\n",
        "        y=train_labels\n",
        "    )\n",
        "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
        "    print(f\"Class Weights: {class_weights}\")\n",
        "    # LoRA\n",
        "    print(\"Loading base model with LoRA...\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        r=best_params.get('lora_r', 8),\n",
        "        lora_alpha=best_params.get('lora_alpha', 32),\n",
        "        lora_dropout=best_params.get('lora_dropout', 0.01),\n",
        "        target_modules=[\"query\", \"value\"],\n",
        "        bias=\"none\",\n",
        "        inference_mode=False\n",
        "    )\n",
        "\n",
        "    print(\"Final LoRA Configuration\")\n",
        "    print(f\"  LoRA Rank (r):        {lora_config.r}\")\n",
        "    print(f\"  LoRA Alpha:           {lora_config.lora_alpha}\")\n",
        "    print(f\"  LoRA Dropout:         {lora_config.lora_dropout}\")\n",
        "    print(f\"  Target Modules:       {lora_config.target_modules}\")\n",
        "    print(f\"  Bias:                 {lora_config.bias}\")\n",
        "    print(f\"  Inference Mode:       {lora_config.inference_mode}\")\n",
        "    print(f\"  Task Type:            {lora_config.task_type}\")\n",
        "\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    # 优化器\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=best_params.get('learning_rate', 2e-5),\n",
        "        weight_decay=best_params.get('weight_decay', 0.01)\n",
        "    )\n",
        "\n",
        "    print(\"Optimizer Configuration\")\n",
        "    print(f\"  Learning Rate:        {best_params.get('learning_rate', 2e-5)}\")\n",
        "    print(f\"  Weight Decay:         {best_params.get('weight_decay', 0.01)}\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "    total_train_time = 0\n",
        "    epoch_times = []\n",
        "    epoch_memories = []\n",
        "\n",
        "\n",
        "    total_steps = len(train_loader) * EPOCHS\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)\n",
        "\n",
        "    best_val_f1 = 0\n",
        "\n",
        "    print(\"\\nStarting Training on Official Split...\")\n",
        "\n",
        "    profiler_data = {\n",
        "    'cpu_time': [],\n",
        "    'cuda_time': [],\n",
        "    'memory': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "      if epoch == 1:\n",
        "        print(\"Profiling enabled for epoch 1 ...\")\n",
        "\n",
        "        with profile(\n",
        "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "            record_shapes=True,\n",
        "            profile_memory=True,\n",
        "            with_stack=False\n",
        "        ) as prof:\n",
        "            train_loss, train_time, train_memory = train_epoch_with_profiler(\n",
        "                model, train_loader, optimizer, scheduler, criterion, DEVICE, prof\n",
        "            )\n",
        "\n",
        "        # ========== 使用内置表格，最稳定 ==========\n",
        "\n",
        "        print(\"PROFILER SUMMARY (Epoch 1)\")\n",
        "\n",
        "\n",
        "        print(\"\\nTop Operations by CPU Time:\")\n",
        "        print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=15))\n",
        "\n",
        "        print(\"\\nTop Operations by CUDA Time:\")\n",
        "        try:\n",
        "            print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=15))\n",
        "        except:\n",
        "            print(prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=15))\n",
        "\n",
        "        print(\"\\nTop Operations by Memory:\")\n",
        "        print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=15))\n",
        "\n",
        "        # 导出chrome trace\n",
        "        trace_file = \"profiler_trace_lora_epoch1.json\"\n",
        "        prof.export_chrome_trace(trace_file)\n",
        "        print(f\"\\n Chrome trace saved to: {trace_file}\")\n",
        "        print(\"  Download and open in chrome://tracing\")\n",
        "      else:\n",
        "        train_loss, train_time, train_memory = train_epoch(\n",
        "          model, train_loader, optimizer, scheduler, criterion, DEVICE\n",
        "        )\n",
        "\n",
        "      val_loss, val_acc, val_labels, val_preds, val_time, val_memory = evaluate(\n",
        "          model, valid_loader, criterion, DEVICE\n",
        "      )\n",
        "\n",
        "      total_train_time += train_time\n",
        "      epoch_times.append(train_time)\n",
        "      epoch_memories.append(train_memory)\n",
        "\n",
        "      report_dict = classification_report(val_labels, val_preds, output_dict=True)\n",
        "      macro_f1 = report_dict['macro avg']['f1-score']\n",
        "      fake_recall = report_dict['0']['recall']\n",
        "\n",
        "      print(f\"Epoch {epoch}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Macro-F1: {macro_f1:.4f} | Fake Recall: {fake_recall:.4f}|  T_Time: {train_time:.1f}s | T_Mem: {train_memory:.0f}MB | V_Time: {val_time:.2f}s | V_Memory: {val_memory:.1f} MB\")\n",
        "\n",
        "      if macro_f1 > best_val_f1:\n",
        "          best_val_f1 = macro_f1\n",
        "          torch.save(model.state_dict(), 'best_lora_model.pth')\n",
        "          print(\" -> Best model updated!\")\n",
        "\n",
        "    print(f\"Total Training Time: {total_train_time:.1f}s\")\n",
        "    print(f\"Avg Epoch Time: {sum(epoch_times)/len(epoch_times):.1f}s\")\n",
        "    print(f\"Avg Peak Memory: {sum(epoch_memories)/len(epoch_memories):.0f}MB\")\n",
        "\n",
        "    # 训练结束后，自动在官方 Test Set 上跑一次\n",
        "    print(\"\\n========== FINAL TEST RESULT (Official Test Set) ==========\")\n",
        "    # 注意：这里我们直接用最后一个epoch的模型跑test，如果你想用 best model，需要上面取消注释 torch.save 并在这里 load\n",
        "    model.load_state_dict(torch.load('best_lora_model.pth'))\n",
        "    test_loss, test_acc, test_labels, test_preds, test_time, test_memory = evaluate(model, test_loader, criterion, DEVICE)\n",
        "    print(classification_report(test_labels, test_preds, target_names=['Fake (0)', 'True (1)']))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    run_official_split_training()\n",
        "    #search_results = run_lora_training_with_grid_search()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GUlXK5x5M-f8",
        "outputId": "80cc91c4-f353-4ace-ad00-251837d8afaa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda | Model: bert-base-uncased\n",
            "Loading Official Datasets (Train / Valid / Test)...\n",
            "Stats: Train=10240, Valid=1284, Test=1267\n",
            "Calculating class weights from Training set...\n",
            "Class Weights: [1.14081996 0.89012517]\n",
            "Loading base model with LoRA...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final LoRA Configuration\n",
            "  LoRA Rank (r):        8\n",
            "  LoRA Alpha:           32\n",
            "  LoRA Dropout:         0.0\n",
            "  Target Modules:       {'query', 'value'}\n",
            "  Bias:                 none\n",
            "  Inference Mode:       False\n",
            "  Task Type:            TaskType.SEQ_CLS\n",
            "trainable params: 296,450 || all params: 109,780,228 || trainable%: 0.2700\n",
            "Optimizer Configuration\n",
            "  Learning Rate:        3e-05\n",
            "  Weight Decay:         0.01\n",
            "\n",
            "Starting Training on Official Split...\n",
            "Profiling enabled for epoch 1 ...\n",
            "PROFILER SUMMARY (Epoch 1)\n",
            "\n",
            "Top Operations by CPU Time:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                             aten::item         0.12%      14.366ms        30.82%        3.547s     434.610us       0.000us         0.00%     336.980us       0.041us           0 B           0 B           0 B           0 B          8162  \n",
            "                              aten::_local_scalar_dense         0.07%       8.253ms        30.70%        3.533s     432.850us     336.980us         0.00%     336.980us       0.041us           0 B           0 B           0 B           0 B          8162  \n",
            "                                  cudaStreamSynchronize        30.58%        3.519s        30.58%        3.520s      21.999ms       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B           160  \n",
            "                                           aten::linear         2.02%     232.143ms        28.48%        3.278s     167.907us       0.000us         0.00%        4.754s     243.553us           0 B           0 B    1096.75 GB    -180.43 GB         19520  \n",
            "                                               aten::to         0.87%      99.720ms        13.12%        1.510s      37.025us       0.000us         0.00%        1.516s      37.177us           0 B           0 B     896.01 GB           0 B         40770  \n",
            "                                         aten::_to_copy         2.94%     337.944ms        12.25%        1.410s      40.983us       0.000us         0.00%        1.516s      44.061us           0 B           0 B     896.01 GB      -1.50 MB         34400  \n",
            "                                       cudaLaunchKernel         7.75%     892.291ms         7.91%     910.894ms      10.560us       0.000us         0.00%     629.698us       0.007us           0 B           0 B           0 B           0 B         86260  \n",
            "                                               aten::mm         5.63%     647.762ms         7.69%     885.058ms      51.697us        2.326s        19.73%        2.326s     135.865us           0 B           0 B     568.20 GB     568.20 GB         17120  \n",
            "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...         6.51%     748.807ms         6.55%     753.873ms       9.307ms       0.000us         0.00%       0.000us       0.000us           0 B     -80.00 KB           0 B           0 B            81  \n",
            "                                            aten::copy_         2.89%     333.073ms         6.20%     713.543ms      20.599us        1.519s        12.89%        1.520s      43.866us           0 B           0 B           0 B           0 B         34640  \n",
            "autograd::engine::evaluate_function: ToCopyBackward0...         1.14%     131.071ms         5.92%     680.991ms      71.533us       0.000us         0.00%        1.666s     175.013us           0 B           0 B    -251.19 GB    -753.81 GB          9520  \n",
            "       autograd::engine::evaluate_function: MmBackward0         0.68%      78.583ms         5.52%     634.963ms     165.355us       0.000us         0.00%     343.182ms      89.370us           0 B           0 B     -98.44 GB    -181.92 GB          3840  \n",
            "                                            MmBackward0         0.54%      61.852ms         4.83%     556.380ms     144.891us       0.000us         0.00%     343.182ms      89.370us           0 B           0 B      83.48 GB           0 B          3840  \n",
            "    autograd::engine::evaluate_function: AddmmBackward0         0.68%      78.779ms         4.25%     488.826ms      86.061us       0.000us         0.00%        1.797s     316.330us           0 B           0 B     -14.80 GB    -408.58 GB          5680  \n",
            "                                        ToCopyBackward0         0.30%      34.309ms         3.93%     452.695ms      47.552us       0.000us         0.00%     738.147ms      77.536us           0 B           0 B     502.62 GB           0 B          9520  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 11.510s\n",
            "Self CUDA time total: 11.787s\n",
            "\n",
            "\n",
            "Top Operations by CUDA Time:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::linear         2.02%     232.143ms        28.48%        3.278s     167.907us       0.000us         0.00%        4.754s     243.553us           0 B           0 B    1096.75 GB    -180.43 GB         19520  \n",
            "                                               aten::mm         5.63%     647.762ms         7.69%     885.058ms      51.697us        2.326s        19.73%        2.326s     135.865us           0 B           0 B     568.20 GB     568.20 GB         17120  \n",
            "autograd::engine::evaluate_function: ScaledDotProduc...         0.28%      32.546ms         2.07%     237.820ms     247.729us       0.000us         0.00%        1.989s       2.072ms     -15.00 KB     -15.00 KB    -110.43 GB    -241.68 GB           960  \n",
            "            ScaledDotProductEfficientAttentionBackward0         0.13%      14.579ms         1.78%     205.275ms     213.828us       0.000us         0.00%        1.989s       2.072ms           0 B           0 B     131.25 GB      -3.75 GB           960  \n",
            "aten::_scaled_dot_product_efficient_attention_backwa...         0.25%      28.594ms         1.66%     190.696ms     198.641us       0.000us         0.00%        1.989s       2.072ms           0 B           0 B     135.00 GB           0 B           960  \n",
            "                    aten::_efficient_attention_backward         0.34%      38.972ms         1.06%     121.754ms     126.827us        1.939s        16.45%        1.989s       2.072ms           0 B           0 B     135.00 GB     -91.85 GB           960  \n",
            "fmha_cutlassB_bf16_aligned_64x64_k64_dropout_sm80(Py...         0.00%       0.000us         0.00%       0.000us       0.000us        1.939s        16.45%        1.939s       2.020ms           0 B           0 B           0 B           0 B           960  \n",
            "                                            aten::addmm         2.87%     330.711ms         3.63%     417.814ms      70.577us        1.904s        16.15%        1.906s     321.908us           0 B           0 B     405.01 GB     405.01 GB          5920  \n",
            "    autograd::engine::evaluate_function: AddmmBackward0         0.68%      78.779ms         4.25%     488.826ms      86.061us       0.000us         0.00%        1.797s     316.330us           0 B           0 B     -14.80 GB    -408.58 GB          5680  \n",
            "                                         AddmmBackward0         0.42%      48.092ms         3.53%     406.040ms      71.486us       0.000us         0.00%        1.796s     316.216us           0 B           0 B     393.78 GB           0 B          5680  \n",
            "autograd::engine::evaluate_function: ToCopyBackward0...         1.14%     131.071ms         5.92%     680.991ms      71.533us       0.000us         0.00%        1.666s     175.013us           0 B           0 B    -251.19 GB    -753.81 GB          9520  \n",
            "                                            aten::copy_         2.89%     333.073ms         6.20%     713.543ms      20.599us        1.519s        12.89%        1.520s      43.866us           0 B           0 B           0 B           0 B         34640  \n",
            "                                               aten::to         0.87%      99.720ms        13.12%        1.510s      37.025us       0.000us         0.00%        1.516s      37.177us           0 B           0 B     896.01 GB           0 B         40770  \n",
            "                                         aten::_to_copy         2.94%     337.944ms        12.25%        1.410s      40.983us       0.000us         0.00%        1.516s      44.061us           0 B           0 B     896.01 GB      -1.50 MB         34400  \n",
            "                     aten::scaled_dot_product_attention         0.20%      23.524ms         3.15%     362.227ms     188.660us       0.000us         0.00%        1.508s     785.485us      30.00 KB           0 B     108.36 GB           0 B          1920  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 11.510s\n",
            "Self CUDA time total: 11.787s\n",
            "\n",
            "\n",
            "Top Operations by Memory:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                    aten::empty_strided         3.75%     432.163ms         3.82%     439.895ms      10.292us       0.000us         0.00%       0.000us       0.000us           0 B           0 B    1040.49 GB    1040.49 GB         42740  \n",
            "                                            aten::empty         1.36%     156.273ms         1.36%     156.584ms       8.992us       0.000us         0.00%       0.000us       0.000us     160.84 KB     160.84 KB     741.55 GB     741.55 GB         17414  \n",
            "                                               aten::mm         5.63%     647.762ms         7.69%     885.058ms      51.697us        2.326s        19.73%        2.326s     135.865us           0 B           0 B     568.20 GB     568.20 GB         17120  \n",
            "                                            aten::addmm         2.87%     330.711ms         3.63%     417.814ms      70.577us        1.904s        16.15%        1.906s     321.908us           0 B           0 B     405.01 GB     405.01 GB          5920  \n",
            "                                              aten::add         0.79%      91.381ms         1.13%     130.235ms      32.559us     540.031ms         4.58%     540.031ms     135.008us           0 B           0 B     277.50 GB     277.50 GB          4000  \n",
            "                                              aten::mul         0.83%      95.908ms         1.21%     139.490ms      35.584us     230.213ms         1.95%     230.213ms      58.728us           0 B           0 B     180.00 GB     180.00 GB          3920  \n",
            "                                             aten::gelu         0.21%      24.428ms         0.31%      35.333ms      36.805us     223.270ms         1.89%     223.270ms     232.573us           0 B           0 B     180.00 GB     180.00 GB           960  \n",
            "                                    aten::gelu_backward         0.15%      17.384ms         0.24%      27.773ms      28.930us     353.062ms         3.00%     353.062ms     367.773us           0 B           0 B     180.00 GB     180.00 GB           960  \n",
            "                                          aten::resize_         0.02%       2.132ms         0.02%       2.132ms       6.580us       0.000us         0.00%       0.000us       0.000us           0 B           0 B      15.06 GB      15.06 GB           324  \n",
            "                                              aten::sub         0.02%       2.160ms         0.03%       3.169ms      39.611us       3.180ms         0.03%       3.180ms      39.751us           0 B           0 B       2.50 GB       2.50 GB            80  \n",
            "                                           Buffer Flush         0.01%       1.639ms         0.01%       1.668ms     104.239us     720.748us         0.01%     720.748us      45.047us           0 B           0 B     389.43 MB     389.43 MB            16  \n",
            "                                             aten::tanh         0.02%       2.344ms         0.03%       3.318ms      41.472us     247.259us         0.00%     247.259us       3.091us           0 B           0 B      15.00 MB      15.00 MB            80  \n",
            "                                    aten::tanh_backward         0.01%       1.550ms         0.02%       2.480ms      31.001us     184.016us         0.00%     184.016us       2.300us           0 B           0 B      15.00 MB      15.00 MB            80  \n",
            "                                               aten::eq         0.03%       2.892ms         0.03%       4.007ms      50.093us     243.163us         0.00%     243.163us       3.040us           0 B           0 B       2.50 MB       2.50 MB            80  \n",
            "                                    aten::_foreach_norm         0.16%      18.917ms         0.23%      26.188ms     327.346us     651.781us         0.01%     765.920us       9.574us           0 B           0 B       1.95 MB       1.91 MB            80  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 11.510s\n",
            "Self CUDA time total: 11.787s\n",
            "\n",
            "\n",
            " Chrome trace saved to: profiler_trace_lora_epoch1.json\n",
            "  Download and open in chrome://tracing\n",
            "Epoch 1/10 | Train Loss: 0.6981 | Val Loss: 0.6901 | Acc: 0.5467 | Macro-F1: 0.5430 | Fake Recall: 0.4756|  T_Time: 14.8s | T_Mem: 11001MB | V_Time: 6.00s | V_Memory: 1950.9 MB\n",
            " -> Best model updated!\n",
            "Epoch 2/10 | Train Loss: 0.6940 | Val Loss: 0.6899 | Acc: 0.5802 | Macro-F1: 0.5438 | Fake Recall: 0.3101|  T_Time: 14.5s | T_Mem: 10570MB | V_Time: 5.99s | V_Memory: 1950.9 MB\n",
            " -> Best model updated!\n",
            "Epoch 3/10 | Train Loss: 0.6890 | Val Loss: 0.6849 | Acc: 0.5732 | Macro-F1: 0.5668 | Fake Recall: 0.4708|  T_Time: 14.5s | T_Mem: 10570MB | V_Time: 5.97s | V_Memory: 1950.9 MB\n",
            " -> Best model updated!\n",
            "Epoch 4/10 | Train Loss: 0.6835 | Val Loss: 0.6873 | Acc: 0.5872 | Macro-F1: 0.5650 | Fake Recall: 0.3766|  T_Time: 14.5s | T_Mem: 10570MB | V_Time: 6.01s | V_Memory: 1950.9 MB\n",
            "Epoch 5/10 | Train Loss: 0.6770 | Val Loss: 0.6793 | Acc: 0.5826 | Macro-F1: 0.5789 | Fake Recall: 0.5097|  T_Time: 14.5s | T_Mem: 10570MB | V_Time: 5.97s | V_Memory: 1950.9 MB\n",
            " -> Best model updated!\n",
            "Epoch 6/10 | Train Loss: 0.6715 | Val Loss: 0.6793 | Acc: 0.5927 | Macro-F1: 0.5871 | Fake Recall: 0.4968|  T_Time: 14.4s | T_Mem: 10570MB | V_Time: 6.01s | V_Memory: 1950.9 MB\n",
            " -> Best model updated!\n",
            "Epoch 7/10 | Train Loss: 0.6673 | Val Loss: 0.6741 | Acc: 0.5966 | Macro-F1: 0.5936 | Fake Recall: 0.5325|  T_Time: 14.6s | T_Mem: 10570MB | V_Time: 6.01s | V_Memory: 1950.9 MB\n",
            " -> Best model updated!\n",
            "Epoch 8/10 | Train Loss: 0.6669 | Val Loss: 0.6743 | Acc: 0.6005 | Macro-F1: 0.5966 | Fake Recall: 0.5244|  T_Time: 14.5s | T_Mem: 10570MB | V_Time: 6.04s | V_Memory: 1950.9 MB\n",
            " -> Best model updated!\n",
            "Epoch 9/10 | Train Loss: 0.6635 | Val Loss: 0.6723 | Acc: 0.5989 | Macro-F1: 0.5963 | Fake Recall: 0.5406|  T_Time: 14.5s | T_Mem: 10570MB | V_Time: 6.02s | V_Memory: 1950.9 MB\n",
            "Epoch 10/10 | Train Loss: 0.6640 | Val Loss: 0.6718 | Acc: 0.5958 | Macro-F1: 0.5934 | Fake Recall: 0.5406|  T_Time: 14.5s | T_Mem: 10570MB | V_Time: 5.96s | V_Memory: 1950.9 MB\n",
            "Total Training Time: 145.4s\n",
            "Avg Epoch Time: 14.5s\n",
            "Avg Peak Memory: 10613MB\n",
            "\n",
            "========== FINAL TEST RESULT (Official Test Set) ==========\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Fake (0)       0.56      0.52      0.54       553\n",
            "    True (1)       0.65      0.68      0.67       714\n",
            "\n",
            "    accuracy                           0.61      1267\n",
            "   macro avg       0.60      0.60      0.60      1267\n",
            "weighted avg       0.61      0.61      0.61      1267\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title rebuild for lora + amp\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 1. 重新准备环境和数据\n",
        "# ==========================================\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# 重新定义配置\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "BATCH_SIZE = 128\n",
        "MAX_LEN = 256\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "VALID_PATH = '/kaggle/input/liar-dataset/valid.tsv'\n",
        "TEST_PATH =  '/kaggle/input/liar-dataset/test.tsv'\n",
        "\n",
        "print(\"Re-loading Tokenizer and Dataloaders...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# 重新实例化数据集 (确保 TextualizedLIARDataset 类已经在前面的格子里运行过)\n",
        "valid_dataset = TextualizedLIARDataset(VALID_PATH, tokenizer, max_len=MAX_LEN)\n",
        "test_dataset  = TextualizedLIARDataset(TEST_PATH, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "# 重新实例化 DataLoader\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# ==========================================\n",
        "# 2. 定义寻找最佳阈值的函数\n",
        "# ==========================================\n",
        "def find_optimal_threshold(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(\"Running inference on Validation Set...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            # Softmax 拿到概率\n",
        "            probs = F.softmax(outputs.logits, dim=1)\n",
        "            # 取出 Label 1 (True) 的概率\n",
        "            true_probs = probs[:, 1].cpu().numpy()\n",
        "\n",
        "            all_probs.extend(true_probs)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_probs = np.array(all_probs)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # 遍历寻找最佳 F1\n",
        "    best_threshold = 0.5\n",
        "    best_f1 = 0\n",
        "\n",
        "    thresholds = np.arange(0.1, 0.95, 0.05)\n",
        "\n",
        "    print(f\"\\n{'Threshold':<10} | {'Macro F1':<10} | {'Fake Recall':<12} | {'True Recall':<12}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for thresh in thresholds:\n",
        "        preds = (all_probs > thresh).astype(int)\n",
        "\n",
        "        report = classification_report(all_labels, preds, output_dict=True)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        fake_recall = report['0']['recall']\n",
        "        true_recall = report['1']['recall']\n",
        "\n",
        "        print(f\"{thresh:.2f}       | {macro_f1:.4f}     | {fake_recall:.4f}       | {true_recall:.4f}\")\n",
        "\n",
        "        if macro_f1 > best_f1:\n",
        "            best_f1 = macro_f1\n",
        "            best_threshold = thresh\n",
        "\n",
        "    print(f\"\\nBest Threshold found: {best_threshold:.2f}\")\n",
        "    return best_threshold\n",
        "\n",
        "# ==========================================\n",
        "# 3. 执行优化\n",
        "# ==========================================\n",
        "# 加载模型\n",
        "print(\"\\nLoading model weights from 'best_lora_model.pth'...\")\n",
        "# 必须重新初始化模型结构才能加载权重\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=2\n",
        ")\n",
        "best_params = grid_search_output\n",
        "lora_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        r=best_params.get('lora_r', 8),\n",
        "        lora_alpha=best_params.get('lora_alpha', 32),\n",
        "        lora_dropout=best_params.get('lora_dropout', 0.01),\n",
        "        target_modules=[\"query\", \"value\"],\n",
        "        bias=\"none\",\n",
        "        inference_mode=False\n",
        "    )\n",
        "\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "# 加载训练好的权重\n",
        "model.load_state_dict(torch.load('best_lora_model.pth'))\n",
        "\n",
        "model.to(DEVICE)\n",
        "\n",
        "best_thresh = find_optimal_threshold(model, valid_loader, DEVICE)\n",
        "\n",
        "# 2. 应用到 Test Set\n",
        "print(f\"\\nApplying Threshold {best_thresh:.2f} to Test Set...\")\n",
        "model.eval()\n",
        "test_probs = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(DEVICE)\n",
        "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "        labels = batch['label'].to(DEVICE)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        probs = F.softmax(outputs.logits, dim=1)\n",
        "        test_probs.extend(probs[:, 1].cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_probs = np.array(test_probs)\n",
        "final_preds = (test_probs > best_thresh).astype(int)\n",
        "\n",
        "print(\"\\n========== OPTIMIZED TEST RESULT ==========\")\n",
        "print(classification_report(test_labels, final_preds, target_names=['Fake (0)', 'True (1)']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLE8tR9IAS4_",
        "outputId": "8a6c0830-7992-4d44-d617-c3b3765f88e2",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Re-loading Tokenizer and Dataloaders...\n",
            "\n",
            "Loading model weights from 'best_lora_model.pth'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running inference on Validation Set...\n",
            "\n",
            "Threshold  | Macro F1   | Fake Recall  | True Recall \n",
            "------------------------------------------------------------\n",
            "0.10       | 0.3422     | 0.0000       | 1.0000\n",
            "0.15       | 0.3422     | 0.0000       | 1.0000\n",
            "0.20       | 0.3422     | 0.0000       | 1.0000\n",
            "0.25       | 0.3422     | 0.0000       | 1.0000\n",
            "0.30       | 0.3458     | 0.0032       | 1.0000\n",
            "0.35       | 0.4148     | 0.0731       | 0.9880\n",
            "0.40       | 0.5186     | 0.2289       | 0.9072\n",
            "0.45       | 0.5752     | 0.3847       | 0.7934\n",
            "0.50       | 0.5966     | 0.5244       | 0.6707\n",
            "0.55       | 0.5832     | 0.6266       | 0.5434\n",
            "0.60       | 0.5725     | 0.7451       | 0.4281\n",
            "0.65       | 0.5535     | 0.8506       | 0.3278\n",
            "0.70       | 0.4703     | 0.9221       | 0.1781\n",
            "0.75       | 0.3718     | 0.9919       | 0.0464\n",
            "0.80       | 0.3259     | 1.0000       | 0.0015\n",
            "0.85       | 0.3242     | 1.0000       | 0.0000\n",
            "0.90       | 0.3242     | 1.0000       | 0.0000\n",
            "\n",
            "Best Threshold found: 0.50\n",
            "\n",
            "Applying Threshold 0.50 to Test Set...\n",
            "\n",
            "========== OPTIMIZED TEST RESULT ==========\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Fake (0)       0.56      0.52      0.54       553\n",
            "    True (1)       0.65      0.68      0.67       714\n",
            "\n",
            "    accuracy                           0.61      1267\n",
            "   macro avg       0.60      0.60      0.60      1267\n",
            "weighted avg       0.61      0.61      0.61      1267\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Only baseline\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup, BertConfig\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ========== 配置 ==========\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 128    # A100 显存大\n",
        "LR = 2e-5\n",
        "EPOCHS = 10\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "NUM_WORKERS = 8\n",
        "\n",
        "# ✅ 修改点 1: 定义官方数据集路径\n",
        "TRAIN_PATH = '/kaggle/input/liar-dataset/train.tsv'\n",
        "VALID_PATH = '/kaggle/input/liar-dataset/valid.tsv'\n",
        "TEST_PATH  = '/kaggle/input/liar-dataset/test.tsv'\n",
        "\n",
        "# ========== 1. 数据集类 (保持不变) ==========\n",
        "class TextualizedLIARDataset(Dataset):\n",
        "    def __init__(self, tsv_path, tokenizer, max_len=128):\n",
        "        self.df = pd.read_csv(tsv_path, sep='\\t', header=None, names=[\n",
        "            \"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job\",\n",
        "            \"state\", \"party\", \"barely_true_counts\", \"false_counts\",\n",
        "            \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\",\n",
        "            \"context\"\n",
        "        ])\n",
        "\n",
        "        self.df.dropna(subset=['statement'], inplace=True)\n",
        "\n",
        "        # 标签逻辑：False/Pants-fire/Barely-true = 0 (Fake)\n",
        "        self.label_map = {\n",
        "            \"pants-fire\": 0, \"false\": 0, \"barely-true\": 0,\n",
        "            \"half-true\": 1, \"mostly-true\": 1, \"true\": 1\n",
        "        }\n",
        "\n",
        "        self.df['label'] = self.df['label'].map(self.label_map)\n",
        "        self.df.dropna(subset=['label'], inplace=True)\n",
        "        self.df['label'] = self.df['label'].astype(int)\n",
        "\n",
        "        text_cols = ['statement', 'subject', 'speaker', 'party', 'state', 'speaker_job', 'context']\n",
        "        for col in text_cols:\n",
        "            self.df[col] = self.df[col].fillna(\"Unknown\")\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        metadata_str = (\n",
        "            f\"Speaker: {row['speaker']} | \"\n",
        "            f\"Job: {row['speaker_job']} | \"\n",
        "            f\"Party: {row['party']} | \"\n",
        "            f\"State: {row['state']} | \"\n",
        "            f\"Context: {row['context']} | \"\n",
        "            f\"Subject: {row['subject']}\"\n",
        "        )\n",
        "        final_text = f\"{metadata_str} [SEP] Statement: {row['statement']}\"\n",
        "\n",
        "        encoded = self.tokenizer(\n",
        "            final_text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(row['label'], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# ========== 2. 训练与评估函数 (保持不变) ==========\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    #dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "    reset_peak_memory()\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
        "        labels = batch['label'].to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_time = end_time - start_time\n",
        "\n",
        "    peak_memory = get_peak_gpu_memory_mb()\n",
        "\n",
        "    return total_loss / len(dataloader), epoch_time, peak_memory\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    reset_peak_memory()\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    eval_time = time.time() - start_time\n",
        "    peak_memory = get_peak_gpu_memory_mb()\n",
        "\n",
        "    return total_loss / len(dataloader), accuracy_score(all_labels, all_preds), all_labels, all_preds, eval_time, peak_memory\n",
        "\n",
        "# ========== 3. 主程序  ==========\n",
        "def run_official_split_training():\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    print(f\"Using device: {DEVICE} | Model: {MODEL_NAME}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    print(\"Loading Official Datasets (Train / Valid / Test)...\")\n",
        "    if not os.path.exists(TRAIN_PATH):\n",
        "        print(f\"Error: Path {TRAIN_PATH} not found.\")\n",
        "        return\n",
        "\n",
        "    train_dataset = TextualizedLIARDataset(TRAIN_PATH, tokenizer, max_len=MAX_LEN)\n",
        "    valid_dataset = TextualizedLIARDataset(VALID_PATH, tokenizer, max_len=MAX_LEN)\n",
        "    test_dataset  = TextualizedLIARDataset(TEST_PATH, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "    # 创建 DataLoader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "    test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "    print(f\"Stats: Train={len(train_dataset)}, Valid={len(valid_dataset)}, Test={len(test_dataset)}\")\n",
        "\n",
        "    # 计算 Class Weights (直接从 train_dataset 获取)\n",
        "    print(\"Calculating class weights from Training set...\")\n",
        "    train_labels = train_dataset.df['label'].values\n",
        "\n",
        "    # 自动计算权重\n",
        "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
        "\n",
        "    print(f\"Class Weights: {class_weights} (Index 0 is Fake, Index 1 is True)\")\n",
        "\n",
        "    # 定义加权 Loss\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "    # 模型加载与配置 (Dropout 0.3)\n",
        "    print(\"Loading model with increased dropout...\")\n",
        "    config = BertConfig.from_pretrained(MODEL_NAME)\n",
        "    config.hidden_dropout_prob = 0.3\n",
        "    config.attention_probs_dropout_prob = 0.3\n",
        "    config.num_labels = 2\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
        "\n",
        "    # 计算参数数量\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"trainable params: {trainable:,} || all params: {total:,} || trainable%: {100*trainable/total:.4f}\")\n",
        "\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
        "\n",
        "    total_train_time = 0\n",
        "    epoch_times = []\n",
        "    epoch_memories = []\n",
        "\n",
        "\n",
        "    total_steps = len(train_loader) * EPOCHS\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)\n",
        "\n",
        "    best_val_f1 = 0\n",
        "\n",
        "    print(\"\\nStarting Training on Official Split...\")\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "      if epoch == 1:\n",
        "        print(\"Profiling enabled for epoch 1 ...\")\n",
        "\n",
        "        with profile(\n",
        "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "            record_shapes=True,\n",
        "            profile_memory=True,\n",
        "            with_stack=False\n",
        "        ) as prof:\n",
        "            train_loss, train_time, train_memory = train_epoch_with_profiler(\n",
        "                model, train_loader, optimizer, scheduler, criterion, DEVICE, prof\n",
        "            )\n",
        "\n",
        "        # ========== 使用内置表格，最稳定 ==========\n",
        "\n",
        "        print(\"PROFILER SUMMARY (Epoch 1)\")\n",
        "\n",
        "\n",
        "        print(\"\\nTop Operations by CPU Time:\")\n",
        "        print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=15))\n",
        "\n",
        "        print(\"\\nTop Operations by CUDA Time:\")\n",
        "        try:\n",
        "            print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=15))\n",
        "        except:\n",
        "            print(prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=15))\n",
        "\n",
        "        print(\"\\nTop Operations by Memory:\")\n",
        "        print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=15))\n",
        "\n",
        "        # 导出chrome trace\n",
        "        trace_file = \"profiler_trace_baselineonly_epoch1.json\"\n",
        "        prof.export_chrome_trace(trace_file)\n",
        "        print(f\"\\n Chrome trace saved to: {trace_file}\")\n",
        "        print(\"  Download and open in chrome://tracing\")\n",
        "      else:\n",
        "        train_loss, train_time, train_memory = train_epoch(\n",
        "          model, train_loader, optimizer, scheduler, criterion, DEVICE\n",
        "        )\n",
        "\n",
        "      val_loss, val_acc, val_labels, val_preds, val_time, val_memory = evaluate(\n",
        "          model, valid_loader, criterion, DEVICE\n",
        "      )\n",
        "\n",
        "      total_train_time += train_time\n",
        "      epoch_times.append(train_time)\n",
        "      epoch_memories.append(train_memory)\n",
        "\n",
        "      report_dict = classification_report(val_labels, val_preds, output_dict=True)\n",
        "      macro_f1 = report_dict['macro avg']['f1-score']\n",
        "      fake_recall = report_dict['0']['recall']\n",
        "\n",
        "      print(f\"Epoch {epoch}/{EPOCHS} | Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Macro-F1: {macro_f1:.4f} | Fake Recall: {fake_recall:.4f}|  T_Time: {train_time:.1f}s | T_Mem: {train_memory:.0f}MB | V_Time: {val_time:.2f}s | V_Memory: {val_memory:.1f} MB\")\n",
        "\n",
        "      if macro_f1 > best_val_f1:\n",
        "          best_val_f1 = macro_f1\n",
        "          torch.save(model.state_dict(), 'best_baselineonly_model.pth')\n",
        "          print(\" -> Best baselineamp model updated!\")\n",
        "\n",
        "    print(f\"Total Training Time: {total_train_time:.1f}s\")\n",
        "    print(f\"Avg Epoch Time: {sum(epoch_times)/len(epoch_times):.1f}s\")\n",
        "    print(f\"Avg Peak Memory: {sum(epoch_memories)/len(epoch_memories):.0f}MB\")\n",
        "\n",
        "    # 训练结束后，自动在官方 Test Set 上跑一次\n",
        "    print(\"\\n========== FINAL TEST RESULT (Official Test Set) ==========\")\n",
        "    # 注意：这里我们直接用最后一个epoch的模型跑test，如果你想用 best model，需要上面取消注释 torch.save 并在这里 load\n",
        "    model.load_state_dict(torch.load('best_baselineonly_model.pth'))\n",
        "    test_loss, test_acc, test_labels, test_preds, test_time, test_memory = evaluate(model, test_loader, criterion, DEVICE)\n",
        "    print(classification_report(test_labels, test_preds, target_names=['Fake (0)', 'True (1)']))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_official_split_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bTFQCjeNdIHy",
        "outputId": "623cbb25-11c9-40d6-f5df-6f9523f2b985",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda | Model: bert-base-uncased\n",
            "Loading Official Datasets (Train / Valid / Test)...\n",
            "Stats: Train=10240, Valid=1284, Test=1267\n",
            "Calculating class weights from Training set...\n",
            "Class Weights: [1.14081996 0.89012517] (Index 0 is Fake, Index 1 is True)\n",
            "Loading model with increased dropout...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 109,483,778 || all params: 109,483,778 || trainable%: 100.0000\n",
            "\n",
            "Starting Training on Official Split...\n",
            "Profiling enabled for epoch 1 ...\n",
            "PROFILER SUMMARY (Epoch 1)\n",
            "\n",
            "Top Operations by CPU Time:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                             aten::item         0.48%      63.317ms        39.39%        5.217s     161.408us       0.000us         0.00%     338.305us       0.010us           0 B           0 B           0 B           0 B         32322  \n",
            "                              aten::_local_scalar_dense         0.18%      23.519ms        38.91%        5.154s     159.449us     338.305us         0.00%     338.305us       0.010us           0 B           0 B           0 B           0 B         32322  \n",
            "                                  cudaStreamSynchronize        38.69%        5.124s        38.69%        5.125s      32.033ms       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B           160  \n",
            "                                           aten::linear         1.02%     134.715ms        13.83%        1.832s     154.707us       0.000us         0.00%        4.198s     354.591us           0 B           0 B    1004.98 GB      -1.12 MB         11840  \n",
            "                                               aten::to         0.78%     103.660ms        10.67%        1.414s      27.381us       0.000us         0.00%        1.169s      22.633us           0 B           0 B     689.04 GB           0 B         51641  \n",
            "    autograd::engine::evaluate_function: AddmmBackward0         1.28%     169.859ms        10.22%        1.353s     228.602us       0.000us         0.00%        3.878s     655.089us           0 B           0 B    -361.27 GB    -779.97 GB          5920  \n",
            "                                         aten::_to_copy         2.35%     311.518ms         9.89%        1.310s      37.480us       0.000us         0.00%        1.169s      33.432us           0 B           0 B     689.04 GB      -2.00 MB         34960  \n",
            "autograd::engine::evaluate_function: ToCopyBackward0...         1.38%     182.408ms         7.16%     947.868ms      59.840us       0.000us         0.00%        1.251s      78.976us           0 B           0 B    -167.87 GB    -553.68 GB         15840  \n",
            "                                       cudaLaunchKernel         6.86%     909.048ms         6.89%     912.910ms       9.712us       0.000us         0.00%      10.560us       0.000us           0 B           0 B           0 B           0 B         94002  \n",
            "                                         AddmmBackward0         0.82%     108.841ms         6.63%     878.171ms     148.340us       0.000us         0.00%        3.446s     582.121us           0 B           0 B     418.68 GB           0 B          5920  \n",
            "                              Optimizer.step#AdamW.step         2.64%     349.113ms         6.62%     877.304ms      10.966ms       0.000us         0.00%     454.747ms       5.684ms         804 B        -320 B     843.17 MB     -33.00 GB            80  \n",
            "                                        ToCopyBackward0         0.42%      55.143ms         5.24%     693.630ms      43.790us       0.000us         0.00%     587.019ms      37.059us           0 B           0 B     385.81 GB           0 B         15840  \n",
            "                                            aten::copy_         2.41%     319.154ms         5.20%     689.429ms      19.542us        1.173s         8.73%        1.173s      33.247us           0 B           0 B           0 B           0 B         35280  \n",
            "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...         5.08%     673.139ms         5.11%     676.209ms       8.348ms       0.000us         0.00%       0.000us       0.000us           0 B     -80.00 KB           0 B           0 B            81  \n",
            "                                               aten::mm         3.00%     396.984ms         4.42%     585.661ms      49.465us        3.446s        25.66%        3.446s     291.060us           0 B           0 B     418.68 GB     418.68 GB         11840  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 13.246s\n",
            "Self CUDA time total: 13.431s\n",
            "\n",
            "\n",
            "Top Operations by CUDA Time:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::linear         1.02%     134.715ms        13.83%        1.832s     154.707us       0.000us         0.00%        4.198s     354.591us           0 B           0 B    1004.98 GB      -1.12 MB         11840  \n",
            "    autograd::engine::evaluate_function: AddmmBackward0         1.28%     169.859ms        10.22%        1.353s     228.602us       0.000us         0.00%        3.878s     655.089us           0 B           0 B    -361.27 GB    -779.97 GB          5920  \n",
            "                                         AddmmBackward0         0.82%     108.841ms         6.63%     878.171ms     148.340us       0.000us         0.00%        3.446s     582.121us           0 B           0 B     418.68 GB           0 B          5920  \n",
            "                                               aten::mm         3.00%     396.984ms         4.42%     585.661ms      49.465us        3.446s        25.66%        3.446s     291.060us           0 B           0 B     418.68 GB     418.68 GB         11840  \n",
            "autograd::engine::evaluate_function: ScaledDotProduc...         0.23%      30.744ms         1.73%     228.511ms     238.033us       0.000us         0.00%        1.996s       2.079ms     -15.00 KB     -15.00 KB    -106.74 GB    -241.74 GB           960  \n",
            "            ScaledDotProductEfficientAttentionBackward0         0.10%      13.732ms         1.49%     197.767ms     206.008us       0.000us         0.00%        1.996s       2.079ms           0 B           0 B     135.00 GB           0 B           960  \n",
            "aten::_scaled_dot_product_efficient_attention_backwa...         0.20%      27.026ms         1.39%     184.036ms     191.704us       0.000us         0.00%        1.996s       2.079ms           0 B           0 B     135.00 GB           0 B           960  \n",
            "                    aten::_efficient_attention_backward         0.27%      36.272ms         0.87%     115.402ms     120.211us        1.947s        14.50%        1.996s       2.079ms           0 B           0 B     135.00 GB     -91.69 GB           960  \n",
            "fmha_cutlassB_bf16_aligned_64x64_k64_dropout_sm80(Py...         0.00%       0.000us         0.00%       0.000us       0.000us        1.947s        14.50%        1.947s       2.028ms           0 B           0 B           0 B           0 B           960  \n",
            "                                            aten::addmm         2.15%     285.173ms         2.69%     355.680ms      60.081us        1.905s        14.18%        1.905s     321.743us           0 B           0 B     405.01 GB     405.01 GB          5920  \n",
            "                     aten::scaled_dot_product_attention         0.16%      20.955ms         2.34%     309.870ms     161.391us       0.000us         0.00%        1.509s     785.839us      30.00 KB           0 B     108.45 GB           0 B          1920  \n",
            "ampere_bf16_s16816gemm_bf16_128x128_ldg8_relu_f2f_st...         0.00%       0.000us         0.00%       0.000us       0.000us        1.288s         9.59%        1.288s     268.364us           0 B           0 B           0 B           0 B          4800  \n",
            "autograd::engine::evaluate_function: ToCopyBackward0...         1.38%     182.408ms         7.16%     947.868ms      59.840us       0.000us         0.00%        1.251s      78.976us           0 B           0 B    -167.87 GB    -553.68 GB         15840  \n",
            "ampere_bf16_s16816gemm_bf16_128x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us        1.241s         9.24%        1.241s     258.593us           0 B           0 B           0 B           0 B          4800  \n",
            "                                            aten::copy_         2.41%     319.154ms         5.20%     689.429ms      19.542us        1.173s         8.73%        1.173s      33.247us           0 B           0 B           0 B           0 B         35280  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 13.246s\n",
            "Self CUDA time total: 13.431s\n",
            "\n",
            "\n",
            "Top Operations by Memory:\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                    aten::empty_strided         3.35%     444.343ms         3.41%     451.724ms       8.113us       0.000us         0.00%       0.000us       0.000us           0 B           0 B     867.26 GB     867.26 GB         55682  \n",
            "                                            aten::empty         1.36%     179.667ms         1.36%     179.671ms       7.599us       0.000us         0.00%       0.000us       0.000us     161.43 KB     161.43 KB     772.23 GB     772.23 GB         23645  \n",
            "                                               aten::mm         3.00%     396.984ms         4.42%     585.661ms      49.465us        3.446s        25.66%        3.446s     291.060us           0 B           0 B     418.68 GB     418.68 GB         11840  \n",
            "                                            aten::addmm         2.15%     285.173ms         2.69%     355.680ms      60.081us        1.905s        14.18%        1.905s     321.743us           0 B           0 B     405.01 GB     405.01 GB          5920  \n",
            "                                              aten::add         0.34%      45.046ms         0.52%      69.204ms      33.271us     365.827ms         2.72%     366.010ms     175.966us           0 B           0 B     187.50 GB     187.50 GB          2080  \n",
            "                                             aten::gelu         0.16%      20.736ms         0.23%      30.587ms      31.862us     223.092ms         1.66%     223.092ms     232.387us           0 B           0 B     180.00 GB     180.00 GB           960  \n",
            "                                    aten::gelu_backward         0.12%      15.956ms         0.19%      25.631ms      26.699us     347.795ms         2.59%     347.795ms     362.287us           0 B           0 B     180.00 GB     180.00 GB           960  \n",
            "                                          aten::resize_         0.02%       3.131ms         0.02%       3.131ms       6.469us       0.000us         0.00%       0.000us       0.000us           0 B           0 B      15.10 GB      15.10 GB           484  \n",
            "                                              aten::sub         0.02%       2.221ms         0.02%       3.172ms      39.651us       3.190ms         0.02%       3.190ms      39.876us           0 B           0 B       2.50 GB       2.50 GB            80  \n",
            "                                              aten::sum         1.41%     186.187ms         2.13%     281.807ms      46.968us     436.674ms         3.25%     436.674ms      72.779us           0 B           0 B      72.81 MB      72.81 MB          6000  \n",
            "                                           Buffer Flush         0.01%       1.763ms         0.01%       1.854ms     102.986us     468.028us         0.00%     468.028us      26.002us           0 B           0 B      83.92 MB      63.64 MB            18  \n",
            "                                             aten::tanh         0.02%       2.166ms         0.02%       3.108ms      38.849us     249.951us         0.00%     249.951us       3.124us           0 B           0 B      15.00 MB      15.00 MB            80  \n",
            "                                    aten::tanh_backward         0.01%       1.581ms         0.02%       2.527ms      31.588us     186.747us         0.00%     186.747us       2.334us           0 B           0 B      15.00 MB      15.00 MB            80  \n",
            "                                               aten::eq         0.02%       2.998ms         0.03%       4.279ms      53.492us     245.376us         0.00%     245.376us       3.067us           0 B           0 B       2.50 MB       2.50 MB            80  \n",
            "                               cudaPointerGetAttributes         0.01%       1.213ms         0.01%       1.216ms       5.065us       0.000us         0.00%       0.000us       0.000us           8 B           8 B     138.50 KB     138.50 KB           240  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 13.246s\n",
            "Self CUDA time total: 13.431s\n",
            "\n",
            "\n",
            " Chrome trace saved to: profiler_trace_baselineonly_epoch1.json\n",
            "  Download and open in chrome://tracing\n",
            "Epoch 1/10 | Val Loss: 0.6917 | Acc: 0.5576 | Macro-F1: 0.4643 | Fake Recall: 0.1461|  T_Time: 16.0s | T_Mem: 16056MB | V_Time: 6.13s | V_Memory: 4474.1 MB\n",
            " -> Best baselineamp model updated!\n",
            "Epoch 2/10 | Val Loss: 0.6740 | Acc: 0.6121 | Macro-F1: 0.6038 | Fake Recall: 0.4870|  T_Time: 86.6s | T_Mem: 22728MB | V_Time: 5.87s | V_Memory: 4471.8 MB\n",
            " -> Best baselineamp model updated!\n",
            "Epoch 3/10 | Val Loss: 0.7004 | Acc: 0.6199 | Macro-F1: 0.5839 | Fake Recall: 0.3393|  T_Time: 86.6s | T_Mem: 22728MB | V_Time: 5.82s | V_Memory: 4471.8 MB\n",
            "Epoch 4/10 | Val Loss: 0.7095 | Acc: 0.6153 | Macro-F1: 0.5597 | Fake Recall: 0.2711|  T_Time: 86.5s | T_Mem: 22728MB | V_Time: 5.86s | V_Memory: 4471.8 MB\n",
            "Epoch 5/10 | Val Loss: 0.6934 | Acc: 0.6215 | Macro-F1: 0.5771 | Fake Recall: 0.3101|  T_Time: 86.6s | T_Mem: 22728MB | V_Time: 5.89s | V_Memory: 4471.8 MB\n",
            "Epoch 6/10 | Val Loss: 0.6762 | Acc: 0.6565 | Macro-F1: 0.6339 | Fake Recall: 0.4253|  T_Time: 86.5s | T_Mem: 22728MB | V_Time: 5.93s | V_Memory: 4471.8 MB\n",
            " -> Best baselineamp model updated!\n",
            "Epoch 7/10 | Val Loss: 0.7212 | Acc: 0.6417 | Macro-F1: 0.6041 | Fake Recall: 0.3474|  T_Time: 86.6s | T_Mem: 22728MB | V_Time: 5.90s | V_Memory: 4471.8 MB\n",
            "Epoch 8/10 | Val Loss: 0.7265 | Acc: 0.6480 | Macro-F1: 0.6114 | Fake Recall: 0.3555|  T_Time: 86.5s | T_Mem: 22728MB | V_Time: 5.90s | V_Memory: 4471.8 MB\n",
            "Epoch 9/10 | Val Loss: 0.7094 | Acc: 0.6519 | Macro-F1: 0.6223 | Fake Recall: 0.3880|  T_Time: 86.5s | T_Mem: 22728MB | V_Time: 5.88s | V_Memory: 4471.8 MB\n",
            "Epoch 10/10 | Val Loss: 0.7231 | Acc: 0.6534 | Macro-F1: 0.6184 | Fake Recall: 0.3653|  T_Time: 86.5s | T_Mem: 22728MB | V_Time: 5.89s | V_Memory: 4471.8 MB\n",
            "Total Training Time: 794.9s\n",
            "Avg Epoch Time: 79.5s\n",
            "Avg Peak Memory: 22061MB\n",
            "\n",
            "========== FINAL TEST RESULT (Official Test Set) ==========\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Fake (0)       0.67      0.41      0.50       553\n",
            "    True (1)       0.65      0.84      0.73       714\n",
            "\n",
            "    accuracy                           0.65      1267\n",
            "   macro avg       0.66      0.62      0.62      1267\n",
            "weighted avg       0.66      0.65      0.63      1267\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title rebuild for baseline only\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 1. 重新准备环境和数据\n",
        "# ==========================================\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# 重新定义配置\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "BATCH_SIZE = 128\n",
        "MAX_LEN = 256\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "VALID_PATH = '/kaggle/input/liar-dataset/valid.tsv'\n",
        "TEST_PATH =  '/kaggle/input/liar-dataset/test.tsv'\n",
        "\n",
        "print(\"Re-loading Tokenizer and Dataloaders...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# 重新实例化数据集 (确保 TextualizedLIARDataset 类已经在前面的格子里运行过)\n",
        "valid_dataset = TextualizedLIARDataset(VALID_PATH, tokenizer, max_len=MAX_LEN)\n",
        "test_dataset  = TextualizedLIARDataset(TEST_PATH, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "# 重新实例化 DataLoader\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# ==========================================\n",
        "# 2. 定义寻找最佳阈值的函数\n",
        "# ==========================================\n",
        "def find_optimal_threshold(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(\"Running inference on Validation Set...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            # Softmax 拿到概率\n",
        "            probs = F.softmax(outputs.logits, dim=1)\n",
        "            # 取出 Label 1 (True) 的概率\n",
        "            true_probs = probs[:, 1].cpu().numpy()\n",
        "\n",
        "            all_probs.extend(true_probs)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_probs = np.array(all_probs)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # 遍历寻找最佳 F1\n",
        "    best_threshold = 0.5\n",
        "    best_f1 = 0\n",
        "\n",
        "    thresholds = np.arange(0.1, 0.95, 0.05)\n",
        "\n",
        "    print(f\"\\n{'Threshold':<10} | {'Macro F1':<10} | {'Fake Recall':<12} | {'True Recall':<12}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for thresh in thresholds:\n",
        "        preds = (all_probs > thresh).astype(int)\n",
        "\n",
        "        report = classification_report(all_labels, preds, output_dict=True)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        fake_recall = report['0']['recall']\n",
        "        true_recall = report['1']['recall']\n",
        "\n",
        "        print(f\"{thresh:.2f}       | {macro_f1:.4f}     | {fake_recall:.4f}       | {true_recall:.4f}\")\n",
        "\n",
        "        if macro_f1 > best_f1:\n",
        "            best_f1 = macro_f1\n",
        "            best_threshold = thresh\n",
        "\n",
        "    print(f\"\\nBest Threshold found: {best_threshold:.2f}\")\n",
        "    return best_threshold\n",
        "\n",
        "# ==========================================\n",
        "# 3. 执行优化\n",
        "# ==========================================\n",
        "# 加载模型\n",
        "print(\"\\nLoading model weights from 'best_baselineonly_model.pth'...\")\n",
        "# 必须重新初始化模型结构才能加载权重\n",
        "config = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).config\n",
        "config.num_labels = 2\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
        "model.load_state_dict(torch.load('best_baselineonly_model.pth'))\n",
        "model.to(DEVICE)\n",
        "\n",
        "# 1. 在 Validation Set 上找最佳门槛\n",
        "best_thresh = find_optimal_threshold(model, valid_loader, DEVICE)\n",
        "\n",
        "# 2. 应用到 Test Set\n",
        "print(f\"\\nApplying Threshold {best_thresh:.2f} to Test Set...\")\n",
        "model.eval()\n",
        "test_probs = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(DEVICE)\n",
        "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "        labels = batch['label'].to(DEVICE)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        probs = F.softmax(outputs.logits, dim=1)\n",
        "        test_probs.extend(probs[:, 1].cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_probs = np.array(test_probs)\n",
        "final_preds = (test_probs > best_thresh).astype(int)\n",
        "\n",
        "print(\"\\n========== OPTIMIZED TEST RESULT ==========\")\n",
        "print(classification_report(test_labels, final_preds, target_names=['Fake (0)', 'True (1)']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsSdS_7v_4rW",
        "outputId": "57c62295-3900-4f03-e73c-8dfdacb1b84f",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Re-loading Tokenizer and Dataloaders...\n",
            "\n",
            "Loading model weights from 'best_baselineonly_model.pth'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running inference on Validation Set...\n",
            "\n",
            "Threshold  | Macro F1   | Fake Recall  | True Recall \n",
            "------------------------------------------------------------\n",
            "0.10       | 0.3422     | 0.0000       | 1.0000\n",
            "0.15       | 0.3529     | 0.0097       | 1.0000\n",
            "0.20       | 0.3743     | 0.0308       | 0.9955\n",
            "0.25       | 0.4215     | 0.0795       | 0.9895\n",
            "0.30       | 0.4529     | 0.1218       | 0.9656\n",
            "0.35       | 0.5147     | 0.2013       | 0.9521\n",
            "0.40       | 0.5580     | 0.2695       | 0.9311\n",
            "0.45       | 0.5993     | 0.3442       | 0.9072\n",
            "0.50       | 0.6339     | 0.4253       | 0.8698\n",
            "0.55       | 0.6551     | 0.4984       | 0.8234\n",
            "0.60       | 0.6646     | 0.5682       | 0.7635\n",
            "0.65       | 0.6624     | 0.6542       | 0.6707\n",
            "0.70       | 0.6429     | 0.7321       | 0.5629\n",
            "0.75       | 0.6205     | 0.8068       | 0.4641\n",
            "0.80       | 0.5660     | 0.8669       | 0.3368\n",
            "0.85       | 0.4962     | 0.9513       | 0.1976\n",
            "0.90       | 0.3710     | 0.9886       | 0.0464\n",
            "\n",
            "Best Threshold found: 0.60\n",
            "\n",
            "Applying Threshold 0.60 to Test Set...\n",
            "\n",
            "========== OPTIMIZED TEST RESULT ==========\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Fake (0)       0.63      0.54      0.58       553\n",
            "    True (1)       0.68      0.75      0.71       714\n",
            "\n",
            "    accuracy                           0.66      1267\n",
            "   macro avg       0.65      0.65      0.65      1267\n",
            "weighted avg       0.66      0.66      0.66      1267\n",
            "\n"
          ]
        }
      ]
    }
  ]
}